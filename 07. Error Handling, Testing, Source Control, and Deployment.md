# 7 Error handling, testing, source control, and deployment

This chapter covers

* Handling T-SQL errors
* Troubleshooting code
* Performance testing
* Modern development practices

When one thinks of the role of a database developer, it is easy to focus exclusively on writing efficient T-SQL code. In reality, however, the modern database developer has to consider many other things. Error handling is the first of these areas. If a procedure throws an error in production, we will want that error to be handled gracefully to mitigate risks such as inconsistent data. Error handling can even make a piece of code retry to avoid an application support team having to step in. We will invariably also want to be notified if an error does occur so that it can be investigated.

We also need to be able to debug code errors efficiently. Code ends up with bugs. That is a fact of life. When there are bugs in our code, we need to be able to debug them in an efficient manner. There is no point in writing a piece of code in a day if it then takes a week to debug it. Therefore, understanding debugging methodology is essential when working with complex code.

A common mistake I see being made by SQL developers is a lack of testing. We will explore unit testing as part of a wider look at the (lack of) adoption of modern development practices by SQL developers. We will explore the benefits of keeping our code in source control, writing unit tests, and using an automated build-and-deploy pipeline. In chapter 4 we created a database called `MagicChoc`, and in chapter 6 we created a database called `Marketing`. We will be using both databases in this chapter.

Modern development practices require code to be kept in source control and for DevOps processes to be used for deploying code. These practices have been the norm for many years now, but SQL developers seem to be late adopters. I have seen many SQL Server teams who still store their “source code” in database backups and manually run complex, lovingly handcrafted scripts to deploy their data-tier applications. We will explore both of these mistakes in this chapter.

Development tools

So far in this book, except for when we explored SQL Server Integration Services (SSIS), I have been using SQL Server Management Studio (SSMS) to create the examples, and my assumption is that you have probably been using SSMS to follow along. It is very familiar to SQL Server professionals, and it’s very easy to use.

There are several other tools that we can use to develop T-SQL, however, such as Visual Studio Code, SQL Server Data Tools (SSDT), and Azure Data Studio. For some advanced development techniques, we need to use some of these additional development tools. For mistakes #28 to #32 in this chapter, we will use the SQL Server Database Project template in Visual Studio. This can be downloaded from the Visual Studio marketplace, or we can install SSDT.

## 7.1 #26 Writing code that doesn’t handle errors

If our code fails in production, the last thing we want is for an unhandled error to occur. If our code fails, we want it to fail gracefully and provide useful diagnostic information to the application support team, who will need to try and resolve the issue. To explore the issue, let’s first examine a stored procedure that has no error handling and take a look at some of the consequences.

The stored procedure in listing 7.1 can be created inside the `MagicChoc` database. It is a simple stored procedure that accepts the parameters required to create a new sales order. It generates the required prefix for the `SalesOrderNumber` based on the customer and then inserts a record into the `SalesOrderHeaders` and `SalesOrderDetails` tables.

Listing 7.1 Creating the `dbo.InsertSalesOrder` stored procedure

```sql
CREATE PROCEDURE dbo.InsertSalesOrder
    @SalesOrderNumber NVARCHAR(12),
    @SalesOrderDate DATE,
    @SalesPersonID INT,
    @SalesAreaID INT,
    @CustomerID INT,
    @SalesOrderDeliveryDueDate DATE,
    @SalesOrderDeliveryActualDate DATE,
    @CurrierUsedForDelivery NVARCHAR(32),
    @ProductID INT,
    @Quantity INT
AS
BEGIN
    BEGIN TRANSACTION

        DECLARE @CustomerPrefix NVARCHAR(12) ;
        SET @CustomerPrefix = (
            SELECT SUBSTRING(CustomerCompanyName,1,3)
            FROM dbo.Customers
            WHERE CustomerID = @CustomerID
        ) ;

        INSERT INTO dbo.SalesOrderHeaders
        VALUES (
              @CustomerPrefix + @SalesOrderNumber
            , @SalesOrderDate
            , @SalesPersonID
            , @SalesAreaID
            , @CustomerID
            , @SalesOrderDeliveryDueDate
            , @SalesOrderDeliveryActualDate
            , @CurrierUsedForDelivery
        ) ;

        INSERT INTO dbo.SalesOrderDetails (
              ProductID
            , Quantity
            , SalesOrderNumber
        )
        VALUES (
              @ProductID
            , @Quantity
            , @CustomerPrefix + @SalesOrderNumber
        ) ;

    COMMIT
END
```

Let’s take a look at this in practice by using our stored procedure to try to create some sales orders. The script in the following listing will execute our stored procedure, and the sales order will be successfully inserted into both the `SalesOrderHeaders` and `SalesOrderDetails` tables.

Listing 7.2 Successfully inserting a sales order

```sql
EXEC dbo.InsertSalesOrder
      '1635D-U06'
    , '2023-08-19'
    , 1
    , 1
    , 2
    , '2023-09-01'
    , NULL
    , 'Get Me There!'
    , 5
    , 1 ;
```

So far, so good! The procedure ran successfully, and a sales order was added. Let’s execute the stored procedure again using the script in listing 7.3. This time, before executing the stored procedure, we are going to change the name of the `SalesOrderDetails` table to be `SalesOrderLines`. Naturally, this will cause a failure inside the transaction.

Listing 7.3 Attempting to insert into a table that does not exist

```sql
EXEC sp_rename 'dbo.SalesOrderDetails', 'SalesOrderLines' ;
GO

EXEC dbo.InsertSalesOrder
      '1637D-U06'
    , '2023-08-19'
    , 1
    , 1
    , 2
    , '2023-09-01'
    , NULL
    , 'Get Me There!'
    , 5
    , 1 ;
```

Two errors have been thrown, as shown:

```sql
Msg 208, Level 16, State 1, Procedure dbo.InsertSalesOrder, Line 35 [Batch Start Line 2]
Invalid object name 'dbo.SalesOrderDetails'.
Msg 266, Level 16, State 2, Procedure dbo.InsertSalesOrder, Line 35 [Batch
Start Line 2]

Transaction count after EXECUTE indicates a mismatching number of BEGIN and
COMMIT statements. Previous count = 0, current count = 1.
```

As we would expect, the first error is caused by the incorrect table name. The second error is more interesting, however. It states that the number of open transactions is higher at the end of the execution of the stored procedure than it was at the start. We can test this by running `SELECT @@TRANCOUNT`. Assuming we have no other open transactions, this value will show as 1, whereas it should ideally be `0`. This is because the transaction did not automatically roll back. We need to manually run the `ROLLBACK` command to clear up this transaction, and if you are following along, you should do that now. You should then run the following code to correct the name of the table:

```sql
EXEC sp_rename 'dbo.SalesOrderLines', 'SalesOrderDetails' ;
GO
```

The code inside the stored procedure is wrapped inside a *transaction*, which is a collection of one or more T-SQL statements that update data and are committed or undone as a single logical unit. At the most basic level, a transaction must have four key properties, which are known as ACID. *ACID* is an acronym that stands for atomic, consistent, isolated, and durable.

When we say that a transaction is atomic, it means that all statements are either committed or undone (“rolled back”) as a single unit. They either all succeed or they all fail. When we say that they are consistent, we mean that the data in the tables will be left in a consistent state at the end of the transaction. For a transaction to be isolated, it cannot interact with other transactions that are happening concurrently. For a transaction to be durable, the updated data cannot be lost after a transaction has been committed, even if SQL Server were to crash.

SQL Server uses many mechanisms to ensure that transactions have ACID properties, including locking, which enforces isolation by stopping other transactions from updating other rows at the same time, and logging by means of a transaction log, which is flushed to disk before the commit of a transaction is completed to ensure durability.

It is very important to note, however, that SQL Server is a large, complex product, and there are many nuances. For example, transaction isolation can be varied by using different transaction isolation levels, and we will examine these in chapter 10.

Another nuance is durability. A feature called *delayed durability* can improve performance by reducing IO contention on busy systems but at the expense of durability. In other words, an ill-timed system crash could result in committed data being lost.

Regarding error handling, a very important nuance is around atomicity and consistency. While a transaction should ensure that all data is either committed or rolled back as a single unit, the setting of `XACT_ABORT` will define how strictly this rule is adhered to. If `XACT_ABORT` is set to `OFF`, which is the default value, then some errors, such as data truncation, will not prevent the transaction from committing other statements.

Let’s look at another example. The script in listing 7.4 executes our stored procedure again. This time, the object names are correct, but we are trying to insert a `ProductID` that does not exist. This will cause a primary key violation.

Listing 7.4 Inserting an incorrect product ID into a sales order

```sql
EXEC dbo.InsertSalesOrder
      '1655D-U06'
    , '2023-08-19'
    , 1
    , 1
    , 2
    , '2023-09-01'
    , NULL
    , 'Get Me There!'
    , 86
    , 1 ;
```

The output of this execution is

```sql
(1 row affected)
Msg 547, Level 16, State 0, Procedure dbo.InsertSalesOrder, Line 35 [Batch
Start Line 0]
The INSERT statement conflicted with the FOREIGN KEY constraint
"FK__SalesOrde__Produ__619B8048". The conflict occurred in database
"MagicChoc", table "dbo.Products", column 'ProductID'.
```

In this output, we can see that the insert into the SalesOrderHeaders table succeeded, but the insert into the `SalesOrderDetails` table failed. If we were to run `SELECT @@TRANCOUNT` at this point, we would return a value of 0. This means that the transaction has been committed. Specifically, it has committed the insert from the first statement, despite the second insert failing.

This is because if `XACT_ABORT` is turned off, which is the default behavior, then some runtime errors, such as deadlocks or incorrect object names, will cause a transaction to abort, but other errors, such as key violations, will allow the transaction to continue and only fail the statement.

This has left our data in an incomplete state. The sales order exists in the `SalesOrderHeaders` table, but it has no line items associated with it. What’s more, the error was not of high enough severity to be written to the error log, which will make it more opaque to debug.

The first takeaway from these examples is that it is good practice to turn on `XACT_ABORT`. This can be done within the stored procedure, or batch, by using `SET XACT_ABORT ON`. However, I would recommend turning it on globally for connections to the instance using the script in the following listing.

Listing 7.5 Turning `XACT_ABORT` on globally for instance connections

```sql
EXEC sp_configure
     'user options'
   , '16384' ;
GO

RECONFIGURE WITH OVERRIDE ;
GO
```

The second, larger takeaway is that it really is a good idea to properly handle errors within our code. SQL Server provides comprehensive error-handling functionality, but not taking advantage of it is a mistake I see far too often.

Therefore, let’s explore how we could have written our stored procedure to better encompass error handling. To do this, the first feature to be aware of is a construct called `TRY..CATCH`, which may be familiar to those of you with a .NET background.

TIP For .NET developers, please be aware that SQL Server does not support a `FINALLY` block, which is a third aspect of the construct in .NET languages that allows for code to be run regardless of the success or failure of the `TRY` block.

Within a `TRY..CATCH` block, SQL Server will attempt to execute the code within the `TRY` block. If the code completes successfully, then SQL Server will exit the construct. If an error occurs, however, SQL Server will jump out of the `TRY` block at the line that caused the error and move into the `CATCH` block. The code within the CATCH block will then be executed.

This means that we can attempt to execute code, but if it fails, we can put error-handling logic inside the `CATCH` block. This logic could include rolling back a transaction and raising meaningful errors.

There are two ways to throw meaningful errors in SQL Server. The first involves calling the `RAISERROR()` function. The second is a simpler error-handling feature called `THROW`. While the two pieces of functionality both exist to raise errors, there are differences, which we will explore.

TIP Did you spot the mistake in `RAISERROR()`? If not, look again. There is a spelling mistake. But the mistake is not in this book; it is in SQL Server. It is probably the most famous and definitely the most ironic spelling mistake in the industry!

Within the context of a `CATCH` block, we have access to system functions that expose information about the error that has occurred. Specifically, the functions available, each having an intuitive name, are

* `ERROR_NUMBER()`—Returns the error number of the error that caused the `CATCH` block to execute
* `ERROR_SEVERITY()`—Returns the severity level of the error
* `ERROR_STATE()`—Returns the state number of the error, which can help identify the cause of the error
* `ERROR_MESSAGE()`—Returns the complete text of the error message
* `ERROR_PROCEDURE()`—Returns the name of the stored procedure or trigger where the error occurred
* `ERROR_LINE()`—Returns the line number within the procedure or trigger where the error occurred

These functions can be used to enhance the error-handling logic. For example, we could branch the code within the `CATCH` block based on the error number. We could also push this data to an error-logging table.

Let’s see how we could enhance our stored procedure to use a `TRY..CATCH` block. After we begin our transaction, we will begin a `TRY` block, which contains our business logic. The last statement within the `TRY` block will `COMMIT` our transaction.

The `TRY` block is immediately followed by a `CATCH` block, which contains our error-handling logic. In here, we `ROLLBACK` the transaction and then throw an error using the `THROW` command.

There are two ways that we can use the `THROW` command. We can use the keyword `THROW` on its own, which will cause the original error to be thrown. Alternatively, we can specify a custom error number, error message, and state to be thrown by adding these to the `THROW` statement. What the `THROW` statement cannot do is throw an error message that is stored in the `sys.messages` table, and we cannot provide an error severity. The severity of an error raised by the `THROW` command is always level 16, which means that it is not logged.

Error severity levels

In SQL Server, there are 25 error severity levels. Severity levels 0 through 10 are not really errors. Instead, they are informational messages. Errors of severity level 10 are informational messages that are converted to severity level 0 for reasons of compatibility.

Errors with severity levels of 11 through 16 are regarded as errors that can be fixed by the user, such as incorrect object names, syntax errors, permission-denied errors, and deadlocks.

Error severity levels 17 through 19 are errors that can usually only be fixed by an administrator, such as running out of resources, exceeding limits enforced by the Database Engine, and issues within the Database Engine that have caused the statement to fail without the connection to the instance being closed.

Errors with a severity level of 20 through 24 are fatal errors, such as encountering database or media corruption. Errors with a severity level of 19 through 24 are written to the error log.

When thinking about error handling, there are a few things about severity levels that we should bear in mind. First, only errors with a severity of 11 through 19 will cause code execution to move to the `CATCH` block. If an error with severity 0 through 10 is thrown, then execution will continue because the error was just informational. If the severity is 20 or over, on the other hand, then there is no point moving to the `CATCH` block because the error was fatal and, in many cases, the connection will even be terminated.

The next thing to bear in mind is that, as mentioned earlier, errors raised by `THROW` have a fixed severity of 16, meaning that they will never be written to the error log, which, in many cases, will make the errors opaque to anyone trying to troubleshoot.

`THROW` can only raise either the original system error or an ad hoc error, meaning an error that is not stored in `sys.messages`. Therefore, the error number we throw must be 50,000 or higher.

Finally, we need to bear in mind that although each severity level has a defined meaning (for example, severity level 22 means that a table or index has been damaged by a hardware or software issue), these levels only apply to system errors. It is perfectly possible to create a custom error message and “hijack” an error severity for our own purposes, which may not correspond to the intended severity level. This, of course, would not be good practice, however, as it is likely to cause confusion for colleagues.

The script in listing 7.6 shows the updated stored procedure definition, which has been encapsulated in a `TRY..CATCH` block. If the code in the TRY block fails, then execution will jump to the `CATCH` block, where the transaction will be rolled back, and then the error thrown using the `THROW` statement.

Listing 7.6 Adding a `TRY..CATCH` block to `InsertOrders`

```sql
ALTER PROCEDURE dbo.InsertSalesOrder
    @SalesOrderNumber NVARCHAR(12),
    @SalesOrderDate DATE,
    @SalesPersonID INT,
    @SalesAreaID INT,
    @CustomerID INT,
    @SalesOrderDeliveryDueDate DATE,
    @SalesOrderDeliveryActualDate DATE,
    @CurrierUsedForDelivery NVARCHAR(32),
    @ProductID INT,
    @Quantity INT
AS
BEGIN
    BEGIN TRANSACTION
    BEGIN TRY
        DECLARE @CustomerPrefix NVARCHAR(12) ;
        SET @CustomerPrefix = (
            SELECT SUBSTRING(CustomerCompanyName,1,3)
            FROM dbo.Customers
            WHERE CustomerID = @CustomerID
        ) ;

       INSERT INTO dbo.SalesOrderHeaders
        VALUES (
              @CustomerPrefix + @SalesOrderNumber
            , @SalesOrderDate
            , @SalesPersonID
            , @SalesAreaID
            , @CustomerID
            , @SalesOrderDeliveryDueDate
            , @SalesOrderDeliveryActualDate
            , @CurrierUsedForDelivery
        ) ;

        INSERT INTO dbo.SalesOrderDetails (
              ProductID
            , Quantity
            , SalesOrderNumber
        )
        VALUES (
              @ProductID
            , @Quantity
            , @CustomerPrefix + @SalesOrderNumber
        ) ;

    COMMIT
    END TRY
    BEGIN CATCH
        ROLLBACK ;
        THROW ;
    END CATCH
END
```

So far, this does not give us a huge benefit, however. If we executed the command in listing 7.4 again, we would receive the following error:

```sql
Msg 2627, Level 14, State 1, Procedure dbo.InsertSalesOrder, Line 23 [Batch
Start Line 0]
Violation of PRIMARY KEY constraint 'PK__SalesOrd__CF6C70EEA96DBC48'.
Cannot insert duplicate key in object 'dbo.SalesOrderHeaders'. The
duplicate key value is (Coo1655D-U06).
```

Therefore, let’s enhance our `InsertSalesOrder` procedure even further. The script in listing 7.7 will update the procedure again. This time, we will branch the code within the `CATCH` block using `IF` statements, combined with the system functions that expose error information, so that the `CATCH` bock behaves differently, depending on the error thrown. If the error is a primary key violation, a custom error message will be thrown. If the error is a deadlock, then the `GOTO` command will be used to attempt to execute the code in the `TRY` block again. If any other error is raised, then the original error will be thrown.

Listing 7.7 Updating `InsertSalesOrder` procedure to branch `CATCH` block

```sql
ALTER PROCEDURE dbo.InsertSalesOrder
    @SalesOrderNumber NVARCHAR(12),
    @SalesOrderDate DATE,
    @SalesPersonID INT,
    @SalesAreaID INT,
    @CustomerID INT,
    @SalesOrderDeliveryDueDate DATE,
    @SalesOrderDeliveryActualDate DATE,
    @CurrierUsedForDelivery NVARCHAR(32),
    @ProductID INT,
    @Quantity INT
AS
BEGIN
    BEGIN TRANSACTION
    RETRY:
    BEGIN TRY
        DECLARE @CustomerPrefix NVARCHAR(12) ;
        SET @CustomerPrefix = (
            SELECT SUBSTRING(CustomerCompanyName,1,3)
            FROM dbo.Customers
            WHERE CustomerID = @CustomerID
        ) ;

       INSERT INTO dbo.SalesOrderHeaders
        VALUES (
              @CustomerPrefix + @SalesOrderNumber
            , @SalesOrderDate
            , @SalesPersonID
            , @SalesAreaID
            , @CustomerID
            , @SalesOrderDeliveryDueDate
            , @SalesOrderDeliveryActualDate
            , @CurrierUsedForDelivery
        ) ;

        INSERT INTO dbo.SalesOrderDetails (
              ProductID
            , Quantity
            , SalesOrderNumber
        )
        VALUES (
              @ProductID
            , @Quantity
            , @CustomerPrefix + @SalesOrderNumber
        ) ;

    COMMIT
    END TRY
    BEGIN CATCH
        IF ERROR_NUMBER() = 2627                    ①
        BEGIN
          ROLLBACK ;
              THROW 50001,
                'A duplicate sales order has been entered.', 1 ;
        END
        IF ERROR_NUMBER() = 1205                    ②
        BEGIN
            ROLLBACK ;
             GOTO RETRY ;
        END
        IF ERROR_NUMBER() <> 2627                   ③
            AND ERROR_NUMBER() <> 1205              ③
        BEGIN
        ROLLBACK ;
            THROW ;
        END
    END CATCH
END
```

① Error is a primary key violation.

② Error is a deadlock.

③ Any other error

If we were to execute the command in listing 7.4 again, we would now receive the following error:

```sql
Msg 50001, Level 16, State 1, Procedure dbo.InsertSalesOrder, Line 52
[Batch Start Line 0]
A duplicate sales order has been entered.
```

To make the best use of SQL Server’s error-handling functionality, we may want to mix and match `THROW` with `RAISERROR()`. For example, in our scenario, if a generic error occurs, we may want to continue to use `THROW` so that the original error message bubbles up. We may choose to use `RAISERROR()` for primary key violations, however, so that we can send the error to the error log.

If we want to take this approach, we will first need to create a custom error message, which can be raised by `RAISERROR()`. To do this, we will use the `sp_addmessage` system stored procedure, as demonstrated in the following listing.

Listing 7.8 Creating a custom error message

```sql
EXEC sp_addmessage
      @msgnum = 50001
    , @severity = 16
    , @msgtext = 'A duplicate sales order has been entered.' ;
```

The `InsertSalesOrder` procedure can then be updated to call `RAISERROR()`, as shown in listing 7.9. The `WITH LOG` syntax will cause the error to be written to the SQL Server error log, despite the severity only being 16.

Listing 7.9 Updating `InsertSalesOrder` procedure to use `RAISERROR()`

```sql
ALTER PROCEDURE dbo.InsertSalesOrder
    @SalesOrderNumber NVARCHAR(12),
    @SalesOrderDate DATE,
    @SalesPersonID INT,
    @SalesAreaID INT,
    @CustomerID INT,
    @SalesOrderDeliveryDueDate DATE,
    @SalesOrderDeliveryActualDate DATE,
    @CurrierUsedForDelivery NVARCHAR(32),
    @ProductID INT,
    @Quantity INT
AS
BEGIN
    BEGIN TRANSACTION
    RETRY:
    BEGIN TRY
        DECLARE @CustomerPrefix NVARCHAR(12) ;
        SET @CustomerPrefix = (
            SELECT SUBSTRING(CustomerCompanyName,1,3)
            FROM dbo.Customers
            WHERE CustomerID = @CustomerID
        ) ;

       INSERT INTO dbo.SalesOrderHeaders
        VALUES (
              @CustomerPrefix + @SalesOrderNumber
            , @SalesOrderDate
            , @SalesPersonID
            , @SalesAreaID
            , @CustomerID
            , @SalesOrderDeliveryDueDate
            , @SalesOrderDeliveryActualDate
            , @CurrierUsedForDelivery
        ) ;

        INSERT INTO dbo.SalesOrderDetails (
              ProductID
            , Quantity
            , SalesOrderNumber
        )
        VALUES (
              @ProductID
            , @Quantity
            , @CustomerPrefix + @SalesOrderNumber
        ) ;

    COMMIT
    END TRY
    BEGIN CATCH
        IF ERROR_NUMBER() = 2627
        BEGIN
            ROLLBACK ;
            RAISERROR(50001, 16, 1) WITH LOG ;
        END
        IF ERROR_NUMBER() = 1205
        BEGIN
         ROLLBACK ;
            GOTO RETRY ;
        END
        IF ERROR_NUMBER() <> 2627
            AND ERROR_NUMBER() <> 1205
        BEGIN
             ROLLBACK ;
             THROW ;
        END
    END CATCH
END
```

If we were to then execute the command in listing 7.4 again, not only would our custom error be returned to the client, but it will also be written to the SQL Server error log, as shown in figure 7.1.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F01_Carter.png)<br>
**Figure 7.1 Custom error in SQL Server error log**

The alternative to using `RAISERROR() WITH LOG` is to use a custom error table. For example, the script in listing 7.10 first creates an error log table. It then updates the stored procedure again, reverting it back to using `THROW`. This time, however, it updates the `CATCH` block to log the error to our newly created table.

Listing 7.10 Adding a custom logging table to `InsertSalesOrder`

```sql
CREATE TABLE dbo.ErrorLog (
    ID            INT    PRIMARY KEY IDENTITY,
    ErrorMessage  NVARCHAR(MAX),
    ErrorNumber   INT,
    ErrorSeverity INT
) ;
GO

ALTER PROCEDURE dbo.InsertSalesOrder
    @SalesOrderNumber NVARCHAR(12),
    @SalesOrderDate DATE,
    @SalesPersonID INT,
    @SalesAreaID INT,
    @CustomerID INT,
    @SalesOrderDeliveryDueDate DATE,
    @SalesOrderDeliveryActualDate DATE,
    @CurrierUsedForDelivery NVARCHAR(32),
    @ProductID INT,
    @Quantity INT
AS
BEGIN
    BEGIN TRANSACTION
    RETRY:
    BEGIN TRY
        DECLARE @CustomerPrefix NVARCHAR(12) ;
        SET @CustomerPrefix = (
            SELECT SUBSTRING(CustomerCompanyName,1,3)
            FROM dbo.Customers
            WHERE CustomerID = @CustomerID
        ) ;

        INSERT INTO dbo.SalesOrderHeaders
        VALUES (
              @CustomerPrefix + @SalesOrderNumber
            , @SalesOrderDate
            , @SalesPersonID
            , @SalesAreaID
            , @CustomerID
            , @SalesOrderDeliveryDueDate
            , @SalesOrderDeliveryActualDate
            , @CurrierUsedForDelivery
        ) ;

        INSERT INTO dbo.SalesOrderDetails (
              ProductID
            , Quantity
            , SalesOrderNumber
        )
        VALUES (
              @ProductID
            , @Quantity
            , @CustomerPrefix + @SalesOrderNumber
        ) ;

    COMMIT
    END TRY
    BEGIN CATCH
        IF ERROR_NUMBER() = 2627
        BEGIN
            ROLLBACK ;
            INSERT INTO dbo.ErrorLog (
                ErrorMessage,                                     ①
                ErrorNumber,                                      ①
                ErrorSeverity                                     ①
            )                                                     ①
            SELECT                                                ①
                  ERROR_MESSAGE()                                 ①
                , ERROR_NUMBER()                                  ①
                , ERROR_SEVERITY() ;
            THROW 50001,
                'A duplicate sales order has been entered.', 1 ;  ②
        END
        IF ERROR_NUMBER() = 1205
        BEGIN
         ROLLBACK ;
            GOTO RETRY ;
        END
        IF ERROR_NUMBER() <> 2627
            AND ERROR_NUMBER() <> 1205
        BEGIN
            ROLLBACK ;
             THROW ;
        END
    END CATCH
END
```

① Adds an INSERT statement that adds the data to our logging table

② Replaces the RAISERROR() with a THROW

WARNING If we take the custom error-logging approach, then the ordering of statements is very important. The `INSERT` statement must be after the `ROLLBACK` and before the `THROW`. If the `INSERT` was before the `ROLLBACK`, then it would be part of the transaction and would therefore be rolled back along with the code in the `TRY` block. If the `INSERT` was after the `THROW` statement, it would not be executed.

We should consider adding error handling to our code that modifies data. This allows us to roll back transactions and raise meaningful error messages. We should also consider logging error messages where appropriate. In certain circumstances, such as deadlocks, we may even wish to code retry logic into our procedures.

## 7.2 #27 Failing to alert on errors

Once our stored procedures are in production, depending on the nature of the code, there are various ways that it may be executed. It may be executed by an ETL tool, such as SSIS; it may be executed by a SQL Server Agent job, which is an orchestrated series of actions that can be scheduled to run at a specific time; or it could be called by a client application.

If a procedure is called by a client application and if the application is well written, errors should bubble up to the user and be noticed. If the procedure is called by an automated process, however, then it may be opaque to discovery. It will require an application support person to proactively check the SQL Server Agent logs, `SSISDB` logs, or custom logging tables to discover a failure.

> [!TIP]
>
> As discussed in the previous section, depending on the severity of the error and the method used to raise the error, it may not be logged within the SQL Server error log. That makes the error log an unreliable place to check for application errors.

Proactive checks are often not undertaken, and even if they are, then they are subject to human error and *opportunity cost*, which means someone is spending their time performing menial tasks rather than higher value work. All of this means that it is very easy for a failure to go without being noticed until a user reports that the data looks incorrect.

So how can we solve this issue? The answer is alerting. If we look at any layers of an IT infrastructure stack, from the network layer through to the operating system, support teams will invariably have alerting in place to notify them if an error occurs that they need to investigate or take action to correct. When it comes to data-tier applications, however, alerting is often overlooked, and this is a mistake.

Depending on the scale and priorities of our application, we may be lucky enough to have enterprise-grade observability software, such as SolarWinds or LogicMonitor. If this is the case, then we should use this software to create a custom check that will alert if a failure occurs. If we take this path, then our custom data source will usually be written to read data out of a logging table at regular intervals and raise an alert if an error has been written to the log. When custom logging and log tables within `SSISDB` are used, this will usually involve a simple `SELECT` statement. If we need to read information from the SQL Server Agent log, however, then we will need to use the `sp_help_jobsteplog` stored procedure. For example, if we had a SQL Server Agent job called `Populate_Fact_Tables`, then we could use the following command to pull the log information for that step:

```sql
EXEC msdb.dbo.sp_help_jobhistory
      @job_name = 'Populate_Fact_Tables'
    , @mode = 'FULL' ;
```

If we are not lucky enough to have an enterprise-grade monitoring tool that we can harness to generate alerts, then we can use SQL Server’s built-in functionality. In this scenario, we will use a feature called Database Mail, along with the SQL Server Agent alerting subsystem.

Database Mail

It is worth noting, at this point, that Database Mail is much maligned in the community, as it is not an enterprise-grade tool. While I broadly agree with this assessment, frankly speaking, if our organization has not invested in a third-party tool that we can use to do the job, then we need to work with what we have, and I have been in this situation several times.

One of the main complaints about the feature is that it does not scale. The functionality resides in `msdb`, and in today’s complex SQL Server environments, a failover event may result in the tool ceasing to work correctly.

This issue has been partly addressed by the introduction of contained availability groups, which contain their own copy of the `master` and `msdb` system databases. At the time of writing, however, and hopefully not at the time of reading, there was a bug in contained availability groups that prevents the Database Mail functionality from working.

It is also worth noting that, to use the functionality, we will require access to an SMTP relay server. Database Mail does not have SMTP services built in. It needs to send the mail via an existing SMTP service.

A full discussion of Database Mail is beyond the scope of this book, but further details can be found at <https://mng.bz/PND9>.

Let’s explore how we can create an alert that will fire in response to an error message. Assuming that Database Mail is already configured, there are two artifacts that we will need to create. The first is an operator, which will be configured to use a Database Mail profile, and the second is the alert itself.

It is important to note that SQL Server Agent alerts rely on the Windows application log to know when SQL Server has fired an event. Events are forwarded to the application log when they are written to the SQL Server error log. Therefore, if an event does not create a log entry, then it will not cause an alert to fire. This means that when we use this approach we must ensure that our error messages are raised with `RAISERROR()` using the `WITH LOG` option. This means that they will be logged and the alert will fire even if the severity level is below 19.

Let’s use the script in listing 7.11 to create an operator called Pete, who will be notified if an alert is fired.

> [!TIP]
>
> In a real-world scenario, we would preferably use a group email address or a distribution list.

Listing 7.11 Creating a SQL Server Agent operator

```sql
EXEC msdb.dbo.sp_add_operator
      @name= ‚Pete'
    , @enabled=1
    , @email_address= ‚pete@onehudredsqlmistakes.com' ;
```

> [!TIP]
>
> SQL Server currently supports pager notifications as well as email, provided that we have third-party pager-to-email software. This functionality is deprecated, however, and should not be used.

To create the alert, we can choose New Alert from the context of the Alerts node under SQL Server Agent in SSMS. This will cause the New Alert dialog box to be displayed. The General page of this dialog box is shown in figure 7.2. Here we have given the alert a name and specified that we want it to respond to a SQL Server event. Other options are to make the alert respond to a WMI event or a SQL Server performance condition. In the Database Name drop-down, we can leave the default of all databases or we can select a specific database that the alert must be raised in. Finally, we can choose if we want the alert to fire in response to either a specific error number or a specific severity level.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F02_Carter.png)<br>
**Figure 7.2 Database Mail New Alert dialog box—General page**

In the Response page of the dialog box, we can choose if we want to execute a SQL Server Agent job in response to the alert, which will try to remediate the issue, or if we want to send an alert to an operator, or both. For our purpose, we will tick the Notify Operators option. This will cause the Operator list to become active, and we can select Email next to the Pete operator.

On the Options page of the dialog, we can create additional notification text that will be sent with the email. We can also specify a delay between alerts being sent. This functionality can be used to reduce noise alerts when an alert fires repeatedly in quick succession.

Alternatively, instead of using the GUI, we could have created the alert using the `sp_add_alert` and `sp_add_notifcation` stored procedures, in the `msdb` database. This is demonstrated in the following listing.

Listing 7.12 Creating an alert using T-SQL

```sql
EXEC msdb.dbo.sp_add_alert
      @name= 'DuplicateSalesOrder'
    , @message_id=50001
    , @enabled=1
    , @database_name= 'MagicChoc' ;

EXEC msdb.dbo.sp_add_notification
      @alert_name= 'DuplicateSalesOrder'
    , @operator_name= 'Pete'
    , @notification_method = 1 ;
```

We should always consider creating alerts for code within our data-tier applications, which are run via an automated process. Ideally, these alerts will be created within an enterprise-grade observability tool. If this is not available, however, then we can use SQL Server Agent alerts and Database Mail to send an alert via an existing SMTP server.

## 7.3 #28 Not utilizing debugging functionality

Those of you who come from a .NET background will likely be familiar with the vast array of debugging tools available in Visual Studio. Many of these features are also available for SQL Server development, but they are rarely used by inexperienced T-SQL developers. The reason for this is often a lack of familiarity with the feature set, but not using them can only be described as a mistake.

I have seen developers pulling their hair out trying to find a bug in a stored procedure that they have written. They repeatedly execute the stored procedure, commenting out queries and inserting `PRINT` statements. They create variables to simulate the procedure’s parameters and run individual queries within the code, desperately trying to work out where the bug is. It is not unheard of for this phase of development to take as long as the creation of the procedure itself.

T-SQL developers could save a great deal of time if they used the debugging functionality of Visual Studio in combination with the SQL Server Database Project template. To follow along with examples in this section, you should create a SQL Server Database Project called `Marketing` and then import the `Marketing` database that we created in chapter 6. You can import the database using the Project | Import | Database option from the context menu of the `Marketing` project. You will need to create a connection.

Once the project is configured, let’s add a new item to the project and create a stored procedure using the definition in listing 7.13. The stored procedure accepts parameters for `CampaignID` and `Budget` and uses these to create a campaign summary. Two result sets are generated. The first is a financial summary of the advertising campaign, and the second is a distinct list of `ReferralURL` and `RenderingID`.

Listing 7.13 Creating a `CalculateCampaignSummary` stored procedure

```sql
CREATE PROCEDURE dbo.CalculateCampaignSummary
    @CampaignID INT,
    @Budget MONEY
AS
BEGIN
    DECLARE @AvgCPM DECIMAL ;
    DECLARE @CampaignCost MONEY ;
    DECLARE @NoOfImp INT ;
    DECLARE @AvgBidPrice MONEY ;
    DECLARE @CostOfBidPerc DECIMAL ;
    DECLARE @BudgetDifference MONEY ;

    SELECT
          @NoOfImp = COUNT(*)
        , @CampaignCost = (SUM(CostPerMille) / 1000) * COUNT(*)
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID
    GROUP BY CampaignID ;

    SELECT
          @avgcpm = AvgCostPerMille
        , @avgbidprice = AvgBidPrice
    FROM reporting.ImpressionAggregates
    WHERE CampaignID = @CampaignID ;

    SET @CostOfBidPerc = (@AvgBidPrice / @AvgCPM) * 100 ;
    SET @BudgetDifference = @budget - @CampaignCost ;

    SELECT
          @CampaignID CampaignID
        , @AvgCPM AvgCPM
        , @CampaignCost CampaignCost
        , @NoOfImp ImpQty
        , @CostOfBidPerc CostToBidPercentage
        , @BudgetDifference ;

    SELECT DISTINCT
          ReferralURL
        , RenderingID
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID ;
END
```

The first debugging tool that we should discuss is the Error List window, which can be found in the View menu. This is an incredibly useful window, which analyzes our code in real time and reports on build errors and IntelliSense errors. For example, if I were to add the text `Lets add an error to our code` in between the final query and the `END` statement, then the Error List window would update within moments, as shown in figure 7.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F03_Carter.png)<br>
**Figure 7.3 Errors in Error List window**

Now let’s publish our database. We can do this by selecting Publish from the context menu of the project. Select the connection (that we created earlier) to our `Marketing` database on our server from the Publish Database dialog box. This will result in our stored procedure being deployed to our SQL Server instance.

Caution If you inserted the erroneous text along with me, you will need to remove this before publishing; otherwise, the build will fail.

Now let’s execute our new stored procedure using the command in listing 7.14. Open a new query window from the context menu of the SQL Server instance in SQL Server Object Explorer and execute the procedure by using the pale-green start button on the Query window task bar.

Listing 7.14 Executing the `CalculateCampaignSummary` stored procedure

```sql
USE Marketing ;
GO

EXEC dbo.CalculateCampaignSummary
    22961,
    52 ;
```

At this point, we will see that the procedure throws a divide-by-zero error on line 26.

So let’s execute the stored procedure again; but this time let’s use the debugger. If we click the drop-down next to the pale-green start button, we will see an extra option. This is represented by a dark-green play button labeled Execute With Debugger.

When we select this option, the execution will begin, but the first statement (in our case, the `USE Marketing` statement) will be highlighted in yellow. This indicates that the execution is paused at this point. We now have several options, which are described in table 7.1.

Table 7.1 Code debugging navigation options

| Option | Shortcut | Description |
| --- | --- | --- |
| Continue | F5 | Continue execution until the next breakpoint.* |
| Step Over | F10 | Will step through the top layer of code but continue through lower levels, such as procedures or functions, without debugging them (unless a breakpoint is hit)* |
| Step Into | F11 | Will step through all layers of code. When a procedure or function is encountered, it will step into that code block and pause execution at the first statement. |
| * Discussed shortly | | |

As we need to debug our stored procedure, the most appropriate choice is Step Into. So let’s use F11 to step into our stored procedure. As we step through each query, we will notice the values of each variable (and parameters) being updated in the Locals window. After the first two queries have been run, the Locals window should look similar to figure 7.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F04_Carter.png)<br>
**Figure 7.4 Locals window**

We can now see the cause of the divide-by-zero error. The value of the @AvgCPM variable is 0, and the next statement will divide the `@AvgBidPrice` by this value. Let’s try to get to the bottom of this. Without pausing execution, we can run queries in the Immediate window. Let’s use the Immediate window to run the query in the following listing, which will ensure that we are only returning a single row, from the second query in our procedure.

Listing 7.15 Running a query in the Immediate window

```sql
SELECT COUNT(*) FROM reporting.ImpressionAggregates WHERE CampaignID =
@CampaignID
```

This returns the number 42. Well, that explains things! The second query in our procedure returns 42 rows, or to put it another way, we are trying to assign 42 different values to each of our variables.

Bonus mistake

For those of you who may be wondering why `@AvgCPM` and `@AvgBidPrice` behaved differently, have a look at the data types. `@AvgCPM` is a `DECIMAL`, whereas `@AvgBidPrice` is `MONEY`. Therefore, `@AvgBidPrice` is using the final value to be returned.

This was an unintentional mistake that I made when creating these examples, which is why it does not appear in the debugging example. I decided to leave it in, as the behavior is actually quite interesting and may not be what you would expect, but we should set `@AvgCPM` to be `MONEY` as well.

We had better update the second query within our stored procedure to get the average values, which is what we need. We can do this by publishing the procedure definition in the following listing.

Listing 7.16 Fixing the stored procedure

```sql
CREATE PROCEDURE dbo.CalculateCampaignSummary
    @CampaignID INT,
    @Budget MONEY
AS
BEGIN
    DECLARE @AvgCPM MONEY ;
    DECLARE @CampaignCost MONEY ;
    DECLARE @NoOfImp INT ;
    DECLARE @AvgBidPrice MONEY ;
    DECLARE @CostOfBidPerc DECIMAL ;
    DECLARE @BudgetDifference MONEY ;

    SELECT
          @NoOfImp = COUNT(*)
        , @CampaignCost = (SUM(CostPerMille) / 1000) * COUNT(*)
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID
    GROUP BY CampaignID ;

    SELECT
          @avgcpm = AVG(AvgCostPerMille)
        , @avgbidprice = AVG(AvgBidPrice)
    FROM reporting.ImpressionAggregates
    WHERE CampaignID = @CampaignID ;

    SET @CostOfBidPerc = (@AvgBidPrice / @AvgCPM) * 100 ;
    SET @BudgetDifference = @budget - @CampaignCost ;

    SELECT
          @CampaignID CampaignID
        , @AvgCPM AvgCPM
        , @CampaignCost CampaignCost
        , @NoOfImp ImpQty
        , @CostOfBidPerc CostToBidPercentage
        , @BudgetDifference ;

    SELECT DISTINCT
          ReferralURL
        , RenderingID
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID ;
END
```

> [!NOTE]
>
> For SQL Server Database Projects, the Immediate window does have limitations. Specifically, we cannot run anything that will require the execution environment to be loaded. In general, this means that we can return scaler values, such as counts, but not result sets. It also means that some other functionality, such as system functions like `ERROR_NUMBER()` or `ERROR_MESSAGE()`, are not usable. Items such as system variables are available, however. Therefore, we could, for example, `SELECT @@trancount`.

Now that we have fixed our stored procedure, let’s run it in debug mode again. But before we do, let’s have a quick chat about breakpoints. A *breakpoint* is a point in the code where a developer indicates that they want execution to pause. This is an incredibly useful feature when debugging code, as it allows a developer to perform actions such as checking variable values and determining execution paths within logic.

Now that we know what a breakpoint is, let’s set a breakpoint on line 26 (`SET @CostOfBidPerc = (@AvgBidPrice / @AvgCPM) * 100 ;`). We can create a simple breakpoint by simply clicking in the left margin, next to where we want the break to occur. If we now start debugging our stored procedure and use F5 instead of F11, execution will continue until the breakpoint is hit. At this point, we can check to ensure that our `@AvgCPM` and `@AvgBidPrice` variables have expected values in the Locals window before hitting F5 again to allow execution to finish.

WARNING Visual Studio supports advanced breakpoints, which allow us to add conditions to breakpoints. These conditions are not supported by T-SQL, however.

Use modern debugging techniques within the SQL Server Database project template for Visual Studio to debug code. This saves time and makes the process of debugging our code a far less frustrating experience.

## 7.4 #29 Not making use of Schema Compare

When we are making large changes to a complex programmable object, it can be hard to keep track of what we have done. A mistake that I often see T-SQL developers make is deploying large code changes into their environment without a complete grasp of what they are going to change. This is a mistake that can lead to multiple issues in production.

Additionally, poor change and deployment processes, combined with emergency fixes, can lead to code drift between different environments. For example, if we have a development environment, a test environment, a staging environment, and a production environment, it is possible to end up with deployments out of sequence.

To avoid these issues, developers can use the *SSDT Schema Compare utility*. This is a tool that can compare databases in two different environments and highlight the differences between them. We also have the option of using a Schema Compare tool to bring the databases into synchronization.

To explore this, let’s update the `CalculateCampaignSummary` stored procedure within our `Marketing` project. We are missing an alias for `@BudgetDifference` in our summary, and we also want to add a parameter that will control whether the second result set is returned. We can make the changes to the definition using the script in the next listing. Save the changes, but don’t publish them yet.

Listing 7.17 Enhancing the `CalculateCampaignSummary` definition

```sql
CREATE PROCEDURE dbo.CalculateCampaignSummary
    @CampaignID INT,
    @Budget MONEY,
    @Detailed BIT
AS
BEGIN
    DECLARE @AvgCPM MONEY ;
    DECLARE @CampaignCost MONEY ;
    DECLARE @NoOfImp INT ;
    DECLARE @AvgBidPrice MONEY ;
    DECLARE @CostOfBidPerc DECIMAL ;
    DECLARE @BudgetDifference MONEY ;

    SELECT
          @NoOfImp = COUNT(*)
        , @CampaignCost = (SUM(CostPerMille) / 1000) * COUNT(*)
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID
    GROUP BY CampaignID ;

    SELECT
          @avgcpm = AVG(AvgCostPerMille)
        , @avgbidprice = AVG(AvgBidPrice)
    FROM reporting.ImpressionAggregates
    WHERE CampaignID = @CampaignID ;

    SET @CostOfBidPerc = (@AvgBidPrice / @AvgCPM) * 100 ;
    SET @BudgetDifference = @budget - @CampaignCost ;

    SELECT
          @CampaignID CampaignID
        , @AvgCPM AvgCPM
        , @CampaignCost CampaignCost
        , @NoOfImp ImpQty
        , @CostOfBidPerc CostToBidPercentage
        , @BudgetDifference BudgetDifference$ ;

    IF @Detailed = 1
    BEGIN
        SELECT DISTINCT
              ReferralURL
            , RenderingID
        FROM marketing.Impressions
        WHERE CampaignID = @CampaignID ;
    END
END
```

Before deploying our updated procedure, we can analyze the changes we are making. To do this, select Schema Compare from the context menu of the `Marketing` project in SQL Server Object Explorer within Visual Studio. This will cause a Compare window to appear, and we can use Select Target on the top right to select the `Marketing` database in our instance. Hitting the compare button at the top of the Compare window taskbar will now cause the comparison to be executed.

The results of this comparison are displayed in figure 7.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F05_Carter.png)<br>
**Figure 7.5 Schema Compare analysis**

The pane at the top of the window shows each object that has changed, and if we select an object, the definitions of the source and target are shown in the lower pane.

You will notice that the changes are highlighted in yellow for additions and red for removals. The bar on the left of the lower pane shows where within the code the changes have happened, and we can click this bar to move to an area of interest. The box to the right of the source and target panes show which part of the code is currently visible on screen. This is also clickable, to allow us to move to different locations.

The task bar at the top of the window gives us buttons to update the target and generate an updated script. An options button allows us to set object-exclusion rules and define any differences that should be ignored. The group-objects button will toggle grouping between object type and schema, and there are also buttons that will toggle the display of unsupported actions and objects that have not changed. We can also use the up- and down-arrow buttons to move between changed objects.

When changing large, complex objects, we should use Schema Compare prior to deployment to ensure that we fully understand the changes that we are making and that we are not going to overwrite any emergency fixes that have not yet made their way back around the deployment cycle.

## 7.5 #30 Failing to write unit tests

Developers break things. It’s a fact. No matter how good we are, there is always a risk that when we revise some code to add additional functionality, or even to fix a bug in existing functionality, we will break something else. This can happen for many reasons, from simply making a mistake to not being aware of a code module’s full functionality and purpose.

In an old-fashioned SQL Server development environment, especially one that has grown organically over the years, it’s likely that no developers who still work on the data-tier application will be fully aware of every requirement being met by every code module within a large application. This is even more of a truism in large, complex projects.

The solution to this problem is to write unit tests. A *unit test* is designed to test a specific piece of functionality. This gives two main benefits. First, it prevents a developer from inadvertently breaking a piece of existing functionality when they update a code module. Second, in the spirit of self-documentation, it creates a mapping between a business requirement and a code module.

In the modern development era, not writing unit tests is a mistake—but a mistake that I see all too often in the SQL Server community. It is an even bigger mistake if the data-tier application is being delivered through *Agile project methodology*. In Agile methodologies, such as Scrum or Kanban, an application is delivered in small, iterative cycles. This has multiple benefits for a project, such as the ability to deliver a minimum viable product sooner and allowing the project to pivot to meet changing business needs more easily.

With such a project methodology, however, it is more likely that we will need to revisit more code modules later on, which increases the risk of breaking previously written functionality. This makes unit testing even more important.

Real-world example

Between 2000 and the 2010s, I was lucky enough to lead the development of some of the largest data-tier applications built in London during that period. The difference in methodology over this period was dramatic. The first and last of these projects were both multiyear projects, involving multiple companies, with more than 20 developers at their peak. They both dealt with 40 to 50 TB of data, which required complex processing.

One project involved building and analyzing click paths from disparate data sources spanning two search engines and four cookie servers. The full user journey had to be calculated, from viewing online banners or search engine searches to actually buying a product on an advertiser’s website.

The other had complex data, including a 30-hour clock and a hierarchy of 36 overlapping advertising regions, where the views of an advert had to be mapped to the correct region at the correct level (or sometimes levels) of the hierarchy. Both applications were written in the SQL Server stack, and both ended up with more than 1 million lines of code.

The first of these projects had a traditional development methodology. We wrote code on a development server. Code modules were scripted out and stored in Team Foundation Server (TFS). TFS had many features; one of them was a pessimistic form of source control. We promoted the code to a User Acceptance Testing (UAT) environment using a manually scripted approach. The business would then test new features before the code was manually promoted to preproduction.

Everything was fine until the final testing phase of the project. At this point, many bugs were discovered, where developers had added new functionality and broken the original functionality in the process. Nobody had spotted it, because when code was promoted, only the new functionality was tested. Even worse, because the project lasted more than two years, developers who had written some of the early code had left, and we then had to work out what the modules were supposed to be doing. The result was a nine-month delay in go-live. Not a situation I ever wanted to repeat!

In the last of these projects, with lessons learned, we took a more modern approach. We stored our code in GitHub so that we could work more effectively as a team (see the next section); we also used Jenkins and Octopus Deploy to create a CI/CD pipeline (see section 7.6.2). We created unit tests for every code module that we wrote. The CI/CD pipeline would execute the unit tests, and the build would fail if the tests were not successful. This took a little extra effort up front to write the tests, but it meant that at the end of the project, there was not a huge backlog of bugs, and the project went live on time.

Let’s follow good practice and create a unit test for our `CalculateCampaignSummary` stored procedure. To start this process, we will select the Create Unit Tests option from the context menu of our stored procedure in the SQL Server Object Explorer window in Visual Studio. This will cause the Create Unit Tests dialog box to be displayed. In the top half of this window, we can select the objects. In the lower half, we can choose if we want the testing project, which will be created as an additional project within our solution, to be based on C# or Visual Basic. Quite honestly, for the purpose of writing SQL Server unit tests, it doesn’t matter too much. I generally choose C#, but that is only because I am more familiar with it. If you already have a testing project created, then you will be able to select it from this drop-down list. Give the project a name and specify a name for the output class.

The Unit Test file will then be displayed in a helpful editor. If we have multiple tests, then we can switch between them in the drop-down on the top left of the window. In the drop-down to the right of this, we can select our pretest script, test script, or posttest script.

The pretest script should be used for setting up our environment for the test. This may include tasks such as populating a table before the test is executed. We will run our script against our SQL Server instance, so there is no need for us to create a pretest script.

In the posttest script, we can clean up after ourselves. For example, if we were testing a stored procedure that adds a sales order, we would use the posttest script to delete that sales order.

The test script is where we will execute our code module. Visual Studio helpfully templates the code for us, so we can simply add in the values for the parameters that we require. For our test, we will use a `CampaignID` of `27587` and a `Budget` of `10`, and we will set the `Detailed` parameter to `1`.

We then want to create four test conditions. The first three will be scaler value conditions, which we will configure to check if the following are true:

* Column 2 = 1.7876
* Column 5 = 195
* Column 6 = 6.85

The final condition will be a Row Count condition. This condition will ensure that the second result set returns 42 rows. We can delete the inconclusive condition that is automatically generated.

To delete the inconclusive condition, highlight it in the lower pane and use the red cross button to remove it. We can then add the desired conditions by selecting the required type from the drop-down list and then using the green add button. Each condition can be configured using the properties window. Figure 7.6 shows the test code windows and the test conditions.

Once we have saved the test, we can run it by navigating to the Test Explorer window and running all tests by using the double play button on the left-hand side of the task bar. Unselecting the filters by clicking the funnel button on the task bar will cause all tests to be displayed with their statuses. We can drill through the test hierarchy and click on a test to see granular information.

To see how a test looks when it has failed, go back to the test we created, change the value of one of the expected results, and run the test again.

> [!TIP]
>
> It’s important to remember that, if a test fails, it is possible that there is no error in the code. There could be an error in the test!

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F06_Carter.png)<br>
**Figure 7.6 Creating a unit test**

We should always consider writing unit tests for our T-SQL code modules. This is especially true for modules that branch code or update data. This practice reduces the risk of bugs caused by adding new functionality, and helps make our code self-documenting by mapping business requirements to code modules.

## 7.6 Modern development techniques

In the following sections, we will discuss modern development techniques that SQL Server developers should, but often do not, use to manage their code. The first of these techniques is the use of source control. The second is using a CI/CD pipeline to manage code deployments.

But why should we bother with these techniques? What is wrong with the old, familiar processes that database professionals have followed? Regarding how we store our code, in the past it was common to find that the only copy of a database schema outside of production was on a development server. Developers would update objects directly on the development server until they were released into production. In a few environments we might find that, once the objects had been created, they would be scripted out and stored as code on another platform. Sometimes this was a developer’s computer, sometimes it was SharePoint, and in really advanced environments, the scripts may even have been stored in Team Foundation Server.

As for deployments, in the past it was common for developers to write very complex deployment scripts, which they would then expect production database administrators (DBAs) to execute on live systems with little or no context. Everyone involved would cross their fingers and hope that there was not an environment-specific issue halfway through the deployment.

So why were these techniques so problematic? Well, let’s consider the following example. MagicChoc’s development team has decided to follow a traditional mechanism for storing and deploying code. Specifically, they store their code on their development instance. They consider this to be acceptable, because they take a weekly backup of the development environment, so they can restore object definitions from backup if need be.

They also use a manual deployment process. This involves the developers writing complex scripts, which will script data out to temporary tables, drop and recreate tables and programable objects, and then script the data back in.

They are ready to deploy the new version of their database to production, so they send the scripts to the DBA and ask them to run them in the upcoming maintenance window. When the maintenance window begins, the DBA kicks off the script. Unfortunately, while the script is running, Windows patching kicks in and restarts the server. This leaves the database in an inconsistent state. Some objects have been updated; others have not. Even worse, some of the data has been lost.

The maintenance window is short, so there is a sense of panic in the air, and the DBA decides to restore the database from the backup they took just before the incident. In their haste, they restore the database over the development instance instead of the production instance. Realizing their error, they restore the database to production as well.

The developers are understanding of the DBA’s mistake and ask them to restore the development database back to the development server. Unfortunately, when the DBA tries to do this, they realize that the last backup of the development server failed, so there has not been a backup in 13 days. In short, the developers have lost almost 2 weeks of work, which includes all of their effort for the release, and they will have to start again from scratch.

> [!NOTE]
>
> While this sounds like a very unlikely scenario, it is based on the experience of a friend of mine who is a SQL developer.

So how could MagicChoc have avoided these issues? Simply put, it could have used modern development practices. If it had stored its data in source control, it could have very quickly and easily retrieved its code. If it had used a CI/CD pipeline, then it could have mitigated the risk of the issue occurring in the first place. Of course, better release management could have avoided it completely, but that’s another story!

In the following sections, we will explore the GitHub workflow for SQL Server and the concepts behind CI/CD for SQL Server deployments to get a feel for how MagicChoc could have done things better. I would encourage you to read further on both topics, however.

### 7.6.1 #31 Not keeping code in source control

In our scenario, MagicChoc was unable to recover its code because it was relying on a development server with backups. To avoid this mistake, our developers could have stored their code in source control.

With the growth of *Git*, which is an open source version control system, and since the introduction of products such as *GitHub*, which is a SaaS hosting service for managing Git repositories (repos), source control has now become a far better way of managing and storing our code.

Integrating with Git allows multiple developers to work on the same parts of a project in parallel. The source control is optimistic, which means that files are not locked, so there can still be conflicts, but these conflicts can be managed. We have access to our version history, so if we were to introduce a bug, it can easily be rolled back. It also means that we have a copy of our code stored in a known, secure, reliable location. All of these benefits of source control mean that not using it in this day and age is a clear mistake, but one I see many SQL developers still making.

To avoid this mistake, within our SQL Server Database project in Visual Studio, navigate to the Git Changes window and choose Create Git Repository. This causes the Create a Git Repository dialog box to be displayed.

As shown in figure 7.7, the top half of the screen allows us to specify the initialization settings for the repo—specifically, the local path to the repo on your computer, the license template (if any) to use, and the *gitignore template*. This is used to specify the files within the repo that should not be tracked by Git. In the bottom half of the dialog, we specify the GitHub account that we want to use. Clicking here will launch the GitHub login page. We will also specify the name, optional description, and owner of the repo.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F07_Carter.png)<br>
**Figure 7.7 Creating and configuring a repo in GitHub**

Now that our GitHub configuration is in place, our first task will be to create a branch. This will be our code branch, which we can make sure we are happy with before merging it into the trunk, which is usually called `master` or `main`. We can do this by using the branch drop-down at the top of the Git Changes window and selecting New Branch. In the Create a New Branch dialog, we can then give our branch a name (call it `CalculateCampaignSummary` if you want to follow along). We then choose the branch to base it on, which will be the trunk, in our case. Be sure to check the boxes to check out the branch and track remote changes.

> [!NOTE]
>
> At the time of writing, `master` is still the default name for a trunk in Git, but thankfully Git plans to change it to a more inclusive name soon.

Now let’s see how code changes work in practice by making a small change to our `CreateCampaignSummary` stored procedure. The script in listing 7.18 adds a `BEGIN..END` around our `IF` branch.

> [!TIP]
>
> In an `IF` block, `BEGIN..END` is mandatory if there are multiple statements in the branch but optional if there is only a single statement.

Listing 7.18 Making a change to `CalculateCampaignSummary`

```sql
CREATE PROCEDURE dbo.CalculateCampaignSummary
    @CampaignID INT,
    @Budget MONEY,
    @Detailed BIT
AS
BEGIN
    DECLARE @AvgCPM MONEY ;
    DECLARE @CampaignCost MONEY ;
    DECLARE @NoOfImp INT ;
    DECLARE @AvgBidPrice MONEY ;
    DECLARE @CostOfBidPerc DECIMAL ;
    DECLARE @BudgetDifference MONEY ;

    SELECT
          @NoOfImp = COUNT(*)
        , @CampaignCost = (SUM(CostPerMille) / 1000) * COUNT(*)
    FROM marketing.Impressions
    WHERE CampaignID = @CampaignID
    GROUP BY CampaignID ;

    SELECT
          @avgcpm = AVG(AvgCostPerMille)
        , @avgbidprice = AVG(AvgBidPrice)
    FROM reporting.ImpressionAggregates
    WHERE CampaignID = @CampaignID ;

    SET @CostOfBidPerc = (@AvgBidPrice / @AvgCPM) * 100 ;
    SET @BudgetDifference = @budget - @CampaignCost ;

    SELECT
          @CampaignID CampaignID
        , @AvgCPM AvgCPM
        , @CampaignCost CampaignCost
        , @NoOfImp ImpQty
        , @CostOfBidPerc CostToBidPercentage
        , @BudgetDifference BudgetDifference$ ;

    IF @Detailed = 1
        BEGIN
            SELECT DISTINCT
                  ReferralURL
                , RenderingID
            FROM marketing.Impressions
            WHERE CampaignID = @CampaignID ;
        END
END
```

After we have saved the file, we should look again at the Git Changes window. The window has been refreshed to show our changed artifact. Clicking the plus sign next to the change will stage the change. The reverse arrow will roll the change back. Let’s hit the plus icon to stage the change.

The changes will be moved to a Staged Changes folder, and we can now add a commit comment and commit the changes using the Commit Staged button, shown in figure 7.8.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F08_Carter.png)<br>
**Figure 7.8 Committing changes**

Pressing the up-arrow button, next to the branch drop-down, will push the changes to the remote branch. This will cause a message in a yellow banner to be displayed at the top of the Git Changes window, which contains a link to create a pull request.

If we have pushed our code to the trunk, then we can simply use the branch dropdown to switch to the trunk and then use the down-arrow button to pull the trunk down to our local repo. If we pushed to a remote branch, however, then we can follow the Create Pull Request link, which will take us to the GitHub website. Here we can create a request for a repository contributor to review our changes and merge them into the trunk.

> [!TIP]
>
> Git and GitHub are worthy of a slim volume in their own right, so if you are not familiar with the products, then I would strongly suggest you dig deeper into these topics, as we have hardly been able to scratch the surface in this section. The Git and GitHub learning resources page is a good place to start; it can be found at <https://mng.bz/JN90>.

### 7.6.2 #32 Not deploying code with a CI/CD pipeline

In our scenario, the deployment issue occurred in part due to the methodology used to deploy the new release of the database into production. MagicChoc could have avoided this mistake and reduced the risks of the deployment by packaging its application and deploying it through a CI/CD pipeline.

These processes have multiple benefits, leading to a shorter time to market for incremental releases, a lower risk of bugs being deployed into production, and less complexity and time consumption for deployments. It is a process that fits well with today’s Agile project methodologies.

Creating a CI/CD pipeline is a complex process involving multiple tools, which will usually be strategically selected by the organization. A full explanation of how to configure the tooling would need to be specific to the tooling selection and would warrant a volume in its own right. The chosen pipeline elements would also be specific to the requirements of your enterprise. Therefore, instead of attempting to explain a granular configuration, I will discuss the concepts and encourage you to undertake further reading.

First, however, let’s talk about *data-tier applications* (DAC). A DAC is important in a continuous delivery environment, as it creates a package containing all the artifacts that make up a DAC and gives the package a unique version number, which can be used by the CD process. The package contains all of the database objects, such as tables, procedures, and users, and also includes the instance-level objects, such as logins, on which the database depends.

A database can be registered as a DAC in multiple ways. For example, we can register a DAC from the context menu of the database in SSMS, or, more in line with the concepts of this chapter, we can register a project as a DAC in Visual Studio.

To register a DAC in Visual Studio, simply publish the project. After the project has been built, the Deployment dialog box will be displayed. In this dialog box, there is a check box for Register As a Data-tier Application. If this option is checked, then an additional check box will become available, allowing us to block the deployment if the target database has drifted.

The `dacpac` file that is output from the build is a zip file containing XML files with the object definitions of the database. It is essentially just a zip file, so we can explore the contents by changing the file extension from `.dacpac` to `.zip`.

The main components in a CI/CD pipeline for SQL databases are

* Developer workstations
* Build services
* Deployment services
* Source control
* Environments

The developer workstations will be equipped with development tools, such as Visual Studio, SSDT, Git, Visual Studio Code, etc. Developers create artifacts locally and then commit them to a branch in their source control repository and raise a pull request. The pull request will often trigger the build services. These build services could be orchestrated through tooling such as GitHub actions, Azure DevOps pipelines, or Jenkins.

The build services will run the unit tests that we discussed in the previous section, ensuring that our code has not broken anything. It will then create the package of the DAC. This package will output into a `dacpac` file.

If the build succeeds and the code is merged into the trunk of source control, then the DAC can be deployed. Deployment services can consist of tools such as Octopus Deploy, Azure DevOps, or Redgate Deploy and will deploy the DAC to the lowest of the environments within our release stack. In some projects, this may be a further development environment; in others it may be an integration testing environment, and in a few, it may be UAT or staging. After code has been signed off in this environment, the deployment services can be used to deploy the DAC to the next environment until it finally reaches production.

> [!NOTE]
>
> While the level of automation should, by definition, be high, different environments will have different human gates. For example, you may configure your environment so that a human reviews a pull request and then the build services are triggered by the merge into the trunk.

The diagram in figure 7.9 illustrates a typical CI/CD process for a SQL Server database.

> [!TIP]
>
> The further reading that you will want to undertake likely depends on the tooling that your organization has adopted. However, Microsoft has a blog series of walk-throughs using GitHub, GitHub Actions, and Visual Studio Code that may be of interest. The first in the series can be found at <https://mng.bz/M18W>. You could also explore the Manning title *Grokking Continuous Delivery* by Christie Wilson, which can be found at [www.manning.com/books/grokking-continuous-delivery](https://www.manning.com/books/grokking-continuous-delivery).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH07_F09_Carter.png)<br>
**Figure 7.9 CI/CD process**

We should look at implementing CI/CD pipelines for our database projects. Doing so can help make us more agile, reduce time to market, and avoid the risks of complexity. Using DAC can help simplify deployments by packaging all required artifacts together and versioning them.

## Summary

* ACID is a set of basic rules for transactions, which state that they must be atomic, consistent, isolated, and durable.
* `XACT_ABORT` is used to determine if an entire transaction is terminated when low severity errors cause a statement to fail.
* We should always write error handling for our code. Use `TRY..CATCH` to trap errors.
* Use `THROW` and `RAISERROR()` to raise meaningful error messages.
* Error severity levels, ranging from 0..24, denote the severity of an error, from informational message through to critical hardware or software failures.
* If an application has unattended processes, consider alerting if errors are raised so that the application support team can deal with them.
* Use an enterprise observability tool to raise alerts if possible. Otherwise, use Database Mail and the SQL Server Agent alerting subsystem.
* Make use of Visual Studio to debug code. This can save time and make troubleshooting more straightforward.
* Visual Studio Database Projects have built-in SQL schema comparison functionality. Use this functionality to ensure that code has not drifted before it’s deployed.
* Use SSDT Schema Compare functionality to ensure a full understanding of the changes that will be made and the functionality that will be impacted.
* Always keep T-SQL code in a source control repository. This will provide a version history and a rollback mechanism, as well as help streamline the development of a project with multiple developers.
* Always write unit tests for programmable objects. Doing so will take a little more time up front but will save more time when resolving issues with bugs breaking existing functionality.
* Use modern deployment techniques. A CI/CD pipeline can provide many benefits, including improving time to market and reducing deployment complexity.
