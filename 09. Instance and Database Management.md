# 9 Instance and database management

This chapter covers

* Common maintenance mistakes and misconceptions
* Capacity planning
* Database corruption
* Administrative scripting
* Patching

In this chapter, we will discuss common maintenance and configuration mistakes made by accidental database administrators (DBAs). As part of this topic, we will discuss the impact of autoshrinking databases before looking at some misconceptions around transaction log files, which lead to issues such as degraded performance.

Next, we will discuss capacity planning. This is a task that many DBAs fail to perform, and we will look at the potential consequences. We will then explore some common mistakes around scripting and automation, including the use of cursors, and a complete lack of maintenance automation.

Finally, we will explore the failure to patch our servers. We will think about the reasons for not patching and the consequences of doing so. We will also discuss how to avoid making this mistake.

Many of the topics in this chapter focus on the day-to-day work of a DBA. Therefore, not all topics have a direct impact on the business. It is important to remember, however, that there is most certainly an indirect impact on the business. For example, if we fail to patch, then there is an increased risk of the servers being attacked by bad actors, leading to massive disruption and reputational damage for the business.

What changes is our interactions with business teams. Until now, most activities that we have discussed will have been led by the business. On the contrary, many of the topics in this chapter will be led by the DBA team to avoid or resolve operations impacts on the business. For example, an issue such as log fragmentation is not something that a business team will be familiar with or ask us to resolve. Instead, we keep on top of these kinds of issues, because if we don’t, then the business will be indirectly impacted by performance issues.

For examples in this chapter that require a database, we will use the `MarketingArchive` database. To create this database, you will need the `Marketing` database that we created in chapter 4. For the examples that use a SQL Server instance, I would suggest using the instance that hosts the `Marketing` database, although this is not essential.

## 9.1 #43 Autoshrinking databases

Most IT professionals have been in a position where a volume has been running out of disk space and they have to perform manual housekeeping to clean up old data and avoid the volume becoming full. Many database professionals have been in a position where they have asked their storage area network (SAN) administrator to extend the size of a volume, just to hear the sound of air being sucked through teeth, because the SAN is running low on capacity.

Additionally, most database professionals are aware that when a VM running SQL Server is built in a cloud environment, such as AWS or Azure, the underlying infrastructure is not owned by your organization, which means that there is a direct cost of expanding the volume.

Therefore, when a person who is new to a SQL Server administration role realizes that there is a feature that will autoshrink databases, reclaiming unused space, they can be forgiven for thinking that this sounds like a great idea. I have seen multiple occasions where an accidental DBA has enabled this feature for all of their databases, without understanding the consequences.

So let’s imagine that MagicChoc has a SQL Server instance that is hosted in Azure. The instance hosts four databases, specifically:

* `Marketing`
* `SalesManagerPlus`
* `TargetManager`
* `MarketingArchive`

Disk utilization on the data volume is hovering around 75%, which is the threshold for the disk space warning in the monitoring tool. An inexperienced DBA responds to the warning and during their investigation discovers the option to autoshrink databases and turns it on. Within an hour, however, multiple application teams start raising tickets, complaining that the performance of their applications has fallen through the floor. This was because enabling autoshrink was a mistake. To understand why, it’s important to understand how autoshrink works.

A background task in SQL Server will wake up periodically and assess if any databases are enabled for autoshrink. If there are, then it finds the first database that is configured for autoshrink and checks to see if there is free space that can be reclaimed. If there is, then it will perform a shrink operation on that database. The task will then go back to sleep. When it wakes up again, a few minutes later, it will check and shrink (if required) the next database that is configured to autoshrink. It will continue like this permanently, moving around the databases in a round-robin pattern.

When an autoshrink operation occurs for a database, it will acquire locks that can cause blocking for applications, which also need to acquire locks. This can cause performance issues or even timeouts if a lock timeout has been configured.

Shrink operations are also resource intensive. While the shrink is in progress, there will likely be a substantial increase in CPU usage, which could impact other operations, especially where other CPU-intensive features such as encryption or compression are used. The disk subsystem will also become very busy, and you will likely see a large spike in disk activity, which could cause additional performance issues for applications. This is exacerbated by the shrink operation being fully logged, which means that there will be heavy throughput to the transaction log file, as well as to the data files.

After the shrink operation has completed, it is likely that general database performance will also be negatively impacted by an issue called *index fragmentation*. This fragmentation is caused by how the shrink operation reorganizes the pages within the data files. We will not dwell on this here, however, as we will discuss it in much more detail in the next section.

While automatically shrinking databases sounds like a tempting idea, to keep disk space under control, it is a far better idea to perform capacity planning to ensure that data volumes are appropriately sized. We will discuss capacity planning later in this chapter.

The query in the following listing can be used to check if you have any databases with autoshrink turned on.

Listing 9.1 Checking if autoshrink is turned on

```sql
SELECT
      name
    , is_auto_shrink_on
FROM sys.databases ;
```

Autoshrink can be turned off using an `ALTER DATABASE` statement. The query in the following listing turns off autoshrink for the `Marketing` database.

Listing 9.2 Disabling autoshrink

```sql
ALTER DATABASE Marketing
    SET auto_shrink OFF WITH NO_WAIT ;
```

> [!TIP]
>
> In chapter 8, we discussed enforcing configuration best practices using Desired State Configuration. If you adopt a configuration management approach, then enforcing autoshrink to be off is a great contender for your automation.

## 9.2 #44 Failing to rebuild indexes after data file shrink

In the previous section, we discussed why it is not a good idea to enable autoshrink for a database. SQL Server administrators also have the ability to manually shrink a database or file, however. As a general rule, shrinking data files, even manually, should be avoided. There are some occasions when shrinking a database manually can be helpful, however.

> [!TIP]
>
> Shrinking log files has different considerations, which we will discuss later in this chapter.

Consider the SQL Server instance discussed in the previous section. It hosts four databases and resides on an Azure VM. One of the databases is called `MarketingArchive` and is used to store historical marketing data. The database has been in operation for a long time and now stores nine years of historical data. The business requirement is only to store the last three years of data. The data volume is at 75% capacity, so there are two options. Either the volume could be expanded or six years of data could be reclaimed from the `MarketingArchive` database. If the volume was expanded, there would be additional Azure storage charges, and the data being stored is no longer needed. Therefore, in this circumstance, it makes sense to purge the data that is no longer required and perform a one-time shrink operation on the database to reclaim space.

In this scenario, the DBA removes the data and shrinks the data files. They know that there will be a performance overhead while the shrink operation takes place, so they perform the activity during a maintenance window.

Unfortunately, after the operation completes, users start to complain that there is a drop in performance and that many of their queries are taking longer than usual to complete. The reason for this is that the indexes within the database have become fragmented. To understand this, let’s follow the path that the DBA took to shrink the data files and explore what happened during the operation. But let’s first remind ourselves of the core concepts of page splits and index fragmentation.

Data is stored on a series of 8 KB pages, and a series of eight continuous 8 KB pages is known as an *extent*. The data pages that make up an index are stored in a double-linked list. This means that each page stores a pointer to both the next page and the previous page. When a data page becomes full, the next time SQL Server needs to add data to that page, then a new page must be created. This is known as a *page split*. There are two types of page split, known as “good page splits” and “bad page splits.”

To understand a good page split, consider the diagram in figure 9.1. This diagram illustrates an index that has become full. SQL Server needs to insert a new row into the last page, but there is no free space. Therefore, the next page from the extent is allocated and the new row is inserted onto the new page.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F01_Carter.png)<br>
**Figure 9.1 Good page splits**

A bad page split occurs when an insert or expansive update happens when SQL Server needs to split a page that is in the middle of an index. When this occurs, then the page that SQL Server must create will be out of order. When pages are out of order, this is known as *external fragmentation*. Additionally, half of the rows from the original page will be moved to the new page to make space on the original page. This is known as *internal fragmentation* and can lead to SQL Server having to read more pages to return the same amount of data. A bad page split is illustrated in figure 9.2.

Before we explore this scenario any further, please use the script in listing 9.3 to build and populate the `MarketingArchive` database if you would like to follow along with the examples in this section.

> [!NOTE]
>
> The script in listing 9.3 relies on the `Marketing` database, which we created in chapter 6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F02_Carter.png)<br>
**Figure 9.2 Bad page split**

Listing 9.3 Creating the `MarketingArchive` database

```sql
CREATE DATABASE MarketingArchive ;
GO

USE MarketingArchive ;
GO

CREATE TABLE dbo.ImpressionsArchive (
    ImpressionID         BIGINT              NOT NULL IDENTITY PRIMARY KEY,
    ImpressionUID        UNIQUEIDENTIFIER    NOT NULL,
    ReferralURL          VARCHAR(512)        NOT NULL,
    CookieID             UNIQUEIDENTIFIER    NOT NULL,
    CampaignID           BIGINT              NOT NULL,
    RenderingID          BIGINT              NOT NULL,
    CountryCode          TINYINT             NULL,
    StateID              TINYINT             NULL,
    BrowserVersion       BIGINT              NOT NULL,
    OperatingSystemID    BIGINT              NOT NULL,
    BidPrice             MONEY               NOT NULL,
    CostPerMille         MONEY               NOT NULL,
    EventTime            DATETIME            NOT NULL,
) ;

DECLARE @Numbers TABLE (
    Number    INT
) ;

INSERT INTO @Numbers
VALUES (-1),(-2),(-3),(-4),(-5),(-6),(-7),(-8),(-9) ;

INSERT INTO MarketingArchive.dbo.ImpressionsArchive (
    ImpressionUID,
    ReferralURL,
    CookieID,
    CampaignID,
    RenderingID,
    CountryCode,
    StateID,
    BrowserVersion,
    OperatingSystemID,
    BidPrice,
    CostPerMille,
    EventTime
)
SELECT
    ImpressionUID,
    ReferralURL,
    CookieID,
    CampaignID,
    RenderingID,
    CountryCode,
    StateID,
    BrowserVersion,
    OperatingSystemID,
    BidPrice,
    CostPerMille,
    DATEADD(YEAR, n.Number, i.EventTime)
FROM Marketing.Marketing.Impressions i
CROSS JOIN @Numbers n ;

CREATE NONCLUSTERED INDEX EventTimeNCI
    ON dbo.ImpressionsArchive(EventTime) ;

CREATE NONCLUSTERED INDEX ImpressionUIDNCI
    ON dbo.ImpressionsArchive(ImpressionUID) ;
```

Let’s first use the `sys.dm_db_index_physical_stats` *dynamic management function (DMF)*. This object returns details of index fragmentation and accepts the following parameters, in the defined order:

* Database ID
* Object ID of the table
* Index ID of the index within the table
* Partition number
* Mode

When `NULL` values are passed to the `object_id`, `index_id`, and `partition_number`, then results for all objects, indexes, and partitions will be returned, respectively. The `mode` parameter specifies the level of accuracy of the results, with a tradeoff against execution time. `LIMITED` is the fastest but least accurate mode. This mode will generate the statistics by looking at nonleaf levels of the B-tree only. `SAMPLED` mode will generate statistics based on 1% of the pages in the index, and `DETAILED` mode will scan all pages of the B-tree to generate the statistics.

The DMF returns one row for every level of the B-tree of every index in scope. This is true for both clustered and nonclustered indexes.

We can take a baseline of the amount of external fragmentation by executing the query in listing 9.4. This query uses the `sys.dm_db_index_physical_stats` DMF to return the fragmentation statistics of each index. It also joins to the `sys.indexes` system view to capture the name of the index. The `index_level_size_MB` column is generated by dividing the number of pages in the index level by 128. This is because 1 MB can store 128 8 KB pages. The `avg_fragmentation_in_percent` column returns the amount of external fragmentation. We also filter out levels greater than `0`. This means that we will only return the leaf level of each index.

> [!TIP]
>
> There is a column in the DMF called `avg_page_space_used_in_percent` that can tell us the amount of internal fragmentation. This is not of interest within this section, but we will explore internal fragmentation in chapter 11.

Listing 9.4 Taking a baseline of fragmentation

```sql
SELECT
      OBJECT_NAME(ips.object_id) AS table_name
    , i.name AS index_name
    , ips.index_type_desc
    , ips.index_level
    , ips.page_count
    , ips.page_count /128 AS index_level_size_MB
    , ips.avg_fragmentation_in_percent
FROM sys.dm_db_index_physical_stats(DB_ID('MarketingArchive'), NULL, NULL,
NULL, 'Detailed') ips
INNER JOIN sys.indexes i
    ON i.object_id = ips.object_id
    AND i.index_id = ips.index_id
WHERE index_level = 0 ;
```

You will notice that the value for `avg_fragmentation_in_percent` is very low for every index. In my results, the clustered index and the `EventTimeNCI` index were both 0.01%, and the `ImplessionUIDNCI` index was 0.12%. Your mileage may vary, however, depending on the number of cores available to the SQL Server instance, as well as other factors, such as if you have any other transactions running within the database.

Now that we are all prepared, let’s follow the same steps as the MagicChoc DBA would have taken. First, we will delete the nine years’ worth of data that we do not need; then we will perform a shrink operation on the data files. A query to purge the old data can be found in the following listing.

Listing 9.5 Purging old data

```sql
DELETE
FROM dbo.ImpressionsArchive
WHERE EventTime < '20200101' ;
```

Now let’s use the query in the following listing to see how much free space we can clean up.

Listing 9.6 Checking free space to clean up

```sql
USE MarketingArchive ;
GO

SELECT
    name
    , AvailableSpaceMB
    , CurrentSizeMB
    , (CurrentSizeMB - AvailableSpaceMB) * 1.2 AS TargetSizeMB
FROM (
    SELECT
          name
        , size / 128 - CAST(FILEPROPERTY(name, 'SpaceUsed') AS INT) / 128
AS AvailableSpaceMB
        , size / 128 AS CurrentSizeMB
    FROM sys.database_files
    WHERE type = 0
) df ;
```

While your mileage may vary a little, when I run this, I see that my target space, which includes 20% for natural growth, is 838.8 MB. Armed with this information, we will now shrink the data file of the database using the `DBCC SHRINKFILE` command. In listing 9.7, we pass this command the logical name of the data file and the target size. We also use the `WAIT_AT_LOW_PRIORITY` option. This will cause the shrink operation to time out after 1 minute, if a long-running query is preventing the command from acquiring a schema lock. This option is new in SQL Server 2022 and helps mitigate the risk of adverse impact on the application.

Listing 9.7 Shrinking the data file

```sql
DBCC SHRINKFILE ('MarketingArchive' , 838) WITH WAIT_AT_LOW_PRIORITY ;
GO
```

If we were to now rerun the query in listing 9.6, we would see that our database is approximately the target size. So everything appears to have worked as expected, but the mistake is not understanding the impact that this will have on the fragmentation of our indexes.

If we were to rerun the query in listing 9.4, we would see that some or all of our indexes are at close to 100% fragmentation, meaning that the pages of the index are completely out of order, which will cause performance problems for queries that make sequential reads, such as index scans.

> [!TIP]
>
> A common misconception is that fragmented indexes cause performance issues for all queries. This is not true. A seek operation that returns a single row will not be impacted. The performance degradation will be seen on queries that perform index scan operations or index lookup operations across multiple pages.

To understand why this is, let’s discuss the diagram in figure 9.3. This diagram illustrates the shrink operation that occurred. The process started at the back of the file, and every time it encountered a page, it moved it to the first available space within the file. For many objects, this involved completely reversing the page order, leading to massive fragmentation.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F03_Carter.png)<br>
**Figure 9.3 File shrink operation**

Therefore, in a situation where you must shrink a data file, you should always rebuild all indexes after the shrink operation completes. The simplest way to achieve this can be seen in listing 9.8, although we will be looking at an alternative approach in chapter 11. The command executes a built-in stored procedure, which loops around every table within the database. It passes a statement to rebuild all indexes on the table using `?` as a wildcard value for the table name.

Listing 9.8 Rebuilding all indexes in a database

```sql
EXEC sp_msforeachtable 'ALTER INDEX ALL ON ? REBUILD' ;
```

Shrinking data files should not be done on a regular basis. It should only be done as a one-time activity, if there is a genuine need. If you do need to shrink a data file, it is important to rebuild all indexes following the operation. This will avoid performance issues relating to heavy index fragmentation.

## 9.3 #45 Relying on autogrow

By default, if SQL Server runs out of space in a file, it will automatically grow the file in 64 MB chunks. By default, data files can grow until the volume on which they reside is full, while log files can grow to either 2 TB or until the volume becomes full. Using these default settings can cause issues for both data files and log files.

MagicChoc’s `MarketingArchive` database is a large database that grows by around 5 GB per day when an extract, transform, and load (ETL) process runs and pulls in historical data from the `Marketing` database. Some of the transactions are very large, and transactions run in parallel. This results in autogrow events occurring for both data and log files. The DBA team has used the default autogrowth settings.

The ETL processes are taking a long time to complete, and analysis shows that random processes, some of them very small, are taking a long time to complete and are blocking other transactions from completing. This is because relying on autogrow to manage file size is a mistake.

Under any circumstances, relying on autogrow to manage file sizes can be problematic. Growing files uses both disk and processor resources. In addition, if a transaction has to grow a log file, then it will block other transactions from writing to the log file until both the grow operation and the data modification have completed.

Using the default 64 MB growth setting on the data file(s) of a database that grows by 5 GB per day only serves to compound the issues caused by the overhead. Additionally, this pattern of small file growth increments in files can lead to disk fragmentation, which can also cause performance issues for sequential read operations on traditional spinning disks. Allowing log files to grow in small increments can lead to fragmentation of the transaction log, and we will explore this in depth later in this chapter.

When a file grows, it has to “zero out” the new space within the file. This consists of literally writing zeros to the file until the file is full. This accounts for much of the overhead of file growth and can be partially mitigated by using instant file initialization, which will be discussed in chapter 10.

Am I recommending that autogrowth be turned off? No, absolutely not. I am a firm believer that autogrowth should be turned on but that it should be used as a safety catch if a database suddenly grows unexpectedly rather than being used to manage file size on a day-to-day basis.

> [!TIP]
>
> For routine management, we should presize data files and log files based on their expected growth. We should then use autogrow to catch any unexpected events. We should also tune the increment that the files will grow by based on the size of the database. For example, we expect the `MarketingArchive` database to grow by 5 GB per day. Therefore, if we set the growth increment to be 5 GB, then the ETL process that runs overnight can grow the file once and once only each day. Then, provided we are monitoring for file growth, we will notice any anomalies the following morning and investigate. This approach prevents the ETL process from falling over due to a full file while giving us the opportunity to implement a strategic fix.

The statement in the following listing will configure the data file in the `MarketingArchive` database to grow in 5 GB chunks.

Listing 9.9 Changing the autogrowth increment

```sql
ALTER DATABASE MarketingArchive
    MODIFY FILE ( NAME = 'MarketingArchive', FILEGROWTH = 5GB ) ;
```

If we were to change the growth increment of files in the `model` system database, then this would become the default growth increment for new databases that are created on the instance. Of course, we could still override this when creating a database by specifying `WITH FILEGROWTH` in the `CREATE DATABASE` statement. In fact, I would strongly recommend making it a practice to always include file size and file growth settings whenever you create a database, especially in production.

We should not allow autogrow to manage the size of our data and log files on a day-to-day basis. Autogrowth should be enabled, but the files should be presized based on capacity planning. Autogrowth should be merely a contingency.

## 9.4 #46 Using multiple log files

SQL Server allows a database to have multiple data files and multiple log files. In some scenarios, splitting data into multiple files can be advantageous. Specifically, it can benefit parallelism, reduce contention on system pages, and make the database easier to migrate. It can also assist with advanced restore strategies.

So what about log files? Let’s think about a `MagicChoc` example. The `MarketingArchive` database receives data from multiple parallel transactions during the nightly ETL run. The ETL run has been quite slow, and examining the wait statistics, the DBA notices that there are significant waits on `WRITELOG` wait type, indicating a performance issue when writing to the transaction log. Therefore, the DBA decides to add an additional log file to alleviate the pressure on the log and improve performance. This is a mistake, however. Let’s discuss why.

Unlike data files, splitting the transaction log into multiple files offers no advantages. Log records are always written sequentially. This means that if we add an additional log file to a transaction log, then SQL Server will still write all transactions to the first log file. It will only use the second log file if the first log file becomes full. Therefore, there are no performance advantages to using multiple log files; however, there are also no performance penalties for using multiple log files. So is the practice really that bad?

The issue occurs when you need to restore the database. If the transaction log does not exist, then SQL Server will need to create it. As discussed in the previous section, it will usually also have to zero out the file. If you have two transaction log files, then it needs to zero out two files instead of one. This leads to an increased restore time.

The only time we should consider using multiple log files is as a temporary fix if we find ourselves in a situation where a transaction log has consumed all available space on a volume. In this scenario, it is acceptable to create a second log file on a different volume so that database operations can continue.

If we take this approach, we should only use it as a short-term measure while we implement a more strategic fix. This could involve expanding the volume on which the original log file resides; moving the log to a new, larger volume; or in some cases, exploring why the log is growing so large. There could be an underlying issue, such as backups not working.

What should the MagicChoc DBA have done differently to improve the performance of the ETL run? There is no hard and fast rule here, but in our scenario, where we have the pattern of a data warehouse being populated by an ETL run, there is a high chance that bulk inserts will be occurring. This is because data warehouses are often bulk loaded from online transaction processing (OLTP)–style databases or other sources on a scheduled basis, often nightly.

If this is the case, we could consider changing the recovery model of the database to bulk logged at the start of the ETL run. This will cause bulk operations to be minimally logged. This will reduce the number of log records that are written to the transaction log. The results are that less log space will be used, and there will be less I/O throughput required for the transaction log, easing the pressure on the disk subsystem. At the end of the ETL run, we would set the recovery mode back to `Full` and take a transaction log backup.

Switching back to full recovery and taking a log backup is important, because the bulk-logged recovery model does not support point-in-time recovery. This means that in a restore scenario, we would only be able to recover to the end of the transaction log backup. We would not be able to recover to a point in the middle.

The recovery model of the `MarketingArchive` database can be changed to `Bulk-Logged` using the statement in the following listing.

Listing 9.10 Setting the database recovery model to bulk logged

```sql
ALTER DATABASE MarketingArchive SET RECOVERY BULK_LOGGED WITH NO_WAIT ;
```

Another option for improving the I/O performance could be moving the log to a faster disk. Many SAN administrators use a blanket policy of RAID-5 or RAID-6 for all volumes. Because of the sequential nature of writes to a transaction log, however, the optimal RAID level is RAID-1.

> [!NOTE]
>
> It is not always possible for SAN administrators to support multiple RAID levels.

In summary, databases should only ever have a single transaction log file. Having multiple transaction logs does not improve performance, and it may increase recovery times. If a log volume runs out of space, we can create a second log file on a different volume, but this should only be a temporary workaround, not a permanent fix.

## 9.5 #47 Allowing logs to become fragmented

While most DBAs are aware of index fragmentation, far fewer are aware of transaction log fragmentation and the issues that this can cause. Let’s think about the `MarketingArchive` database. We created this database using the default size and growth settings. Therefore, the log file was initially created at 8 MB, and if the file runs out of space, it will grow in 64 GB increments, up to a maximum of 2 TB. The MagicChoc DBAs have left these default settings in place. We discussed earlier in this chapter how, in some circumstances, growing a log file can lead to an increase in I/O and the blocking of other transactions.

In this specific scenario, provided we are running SQL Server 2022 or higher, and provided that instant file initialization (to be discussed in chapter 11) is enabled, the log file will not need to be zeroed out, as it is growing in small, 64 MB chunks. In this scenario, however, allowing the log to grow using the default settings is a mistake, as it will cause log fragmentation. This, in turn, can lead to performance issues. To understand why, we need to understand a little about the architecture of the transaction log.

Inside a transaction log file, SQL Server creates several *virtual log files (VLFs)*. Inside a VLF, you have a number of *log blocks*, which are containers for log records and the unit in which transaction log records are flushed to disk. These *log records* provide a log of every data manipulation language (DML) and data definition language (DDL) statement that has been executed against a database. In `Full` recovery model, all allocated and deallocated pages and extents are also logged. Each log record is given a *log sequence number (LSN)*. The LSN consists of three parts: VLF sequence number, followed by the log block ID, and finally the log record ID.

When a transaction log is truncated, it’s not the log file itself that is truncated—it is the VLFs that the log file contains. Any VLFs that do not contain any active transactions will be truncated. A transaction may be marked as active for multiple reasons, including

* The VLF contains transactions that are not yet complete.
* The VLF contains transactions that have not yet been streamed to replicas in an availability group topology.
* The VLF contains transactions that have not yet been harvested by change data capture.

A transaction log file is never fully truncated; there is always at least one active VLF, but there could be many active VLFs at any given time.

A transaction log file has a cyclical nature. When the last VLF inside the log file becomes full, SQL Server will cycle to the front of the transaction log and start using the first VLF that has been truncated. This is illustrated in figure 9.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F04_Carter.png)<br>
**Figure 9.4 Transaction log cycle**

If there are no truncated VLFs within a log file, when the last VLF becomes full, then SQL Server will attempt to grow the log file, based on the autogrowth settings for that file. If it is unable to grow the file because the growth settings do not permit it or because the underlying storage is full, then a 9002 error will be thrown, indicating that the transaction log is full.

When a log file grows, the number of VLFs that are created will depend on the current size of the transaction log and the size increment that it is growing. The rules used to determine how many VLFs will be created state that, if the growth increment is less than 1/8 of the current log size, one VLF will be created. Otherwise:

* If the increment is less than 64 MB, four VLFs will be created.
* If the increment is between 64 MB and 1 GB, eight VLFs will be created.
* If the increment is more than 1 GB, 16 VLFs will be created.

This formula can lead to a log file containing a very large number of VLFs. *Log fragmentation* is where a transaction log has a disproportionate number of VLFs inside it. This can lead to performance issues when restoring a database, attaching a database, and recovering the database when the SQL Server instance starts up. It can also cause performance issues for technologies like AlwaysOn availability groups.

There is no set number for how many VLFs a log file should contain, but they can often start causing issues if there are more than 1,000. Each database must be considered on its own merits, but I generally aim for around two to four VLFs per GB of the total size of the log file. We should also avoid growing a large log file in more than 8 GB chunks, as having too few VLFs can also lead to problems.

The query in listing 9.11 can be used to determine if the `MarketingArchive` database has a fragmented log file. The subquery counts the number of VLFs for the specified database by querying the `sys.dm_db_log_info` DMF. The outer query joins these results to the `sys.master_files` system view and performs some simple calculations to convert the file size from KB to GB and calculate the minimum and maximum ideal number of VLFs.

Listing 9.11 Checking for log fragmentation

```sql
SELECT
      li.database_id
    , li.ActualVLFs
    , mf.size/1024/1024 AS SizeInGB
    , mf.size/1024/1024*2. AS MinIdealVLFs
    , mf.size/1024/1024*4. AS MaxIdealVLFs
FROM (
    SELECT database_id, COUNT(*) AS ActualVLFs
    FROM sys.dm_db_log_info(DB_ID('MarketingArchive'))
    GROUP BY database_id
) li
INNER JOIN sys.master_files mf
    ON li.database_id = mf.database_id
WHERE mf.type = 1 ;
```

If you discover that you need to resolve log file fragmentation, the log file should be cleared down by either backing it up (if the database is in FULL recovery model) or truncating it (if the database is using SIMPLE recovery model). We will then need to shrink the file to as small as possible using `DBCC SHRINKFILE`. Finally, we can set the growth increment to an appropriate value, depending on the target size of the transaction log.

We should always try to prevent our logs from becoming fragmented. If this does happen, we can experience performance issues during activities such as restore operations or when we are using features such as availability groups. We can remove existing fragmentation by shrinking the log file and then allowing it to grow using an appropriate increment.

## 9.6 #48 Failing to capacity plan

*Capacity planning* is the process of estimating the amount of resources that an application will need after a defined period of time. The period of time is usually in line with an organization’s hardware refresh cycle. For example, let’s assume that `MagicChoc` has a hardware refresh cycle of three years. Two years into the cycle, a new application is developed, which will run on VMs in the company’s data center. During the capacity-planning phase of this project, the task would be to estimate how much compute, memory, and storage the application will require after one year. This will inform the organization if it has sufficient resources on the SAN and within the virtualization cluster to run the application until the next hardware refresh or if new hardware will need to be procured.

One year later, at the end of the hardware refresh cycle, another capacity-planning operation will need to take place. This time, the task will be to determine the amount of resources the application will require in three years’ time. This will inform the organization how much additional compute and SAN capacity it needs to purchase in the refresh.

Let’s run with the scenario that we have just been discussing to see why failing to undertake capacity planning is a mistake. We are two years into the hardware lifecycle, and we have just launched the Historical Marketing Analysis application. The `MarketingArchive` database in development is promoted to production, with data files sized at 200 GB. We notice that there are 50 GB of free space in the data files, and therefore we feel that there is plenty of free space for natural growth, and because we are busy and capacity planning is hard, we decide not to engage in the process.

What we fail to realize is that the database pulls in 5 GB of data every day and has a data retention policy of three years. This means that, after one year, by the end of the hardware lifecycle, the data files in the database will be a total of 1.78 TB. We have additionally failed to consider how large the `TempDB` system database and the transaction log will need to be. With enough space for the transaction log and `TempDB`, plus factoring in enough headroom for unexpected growth, in a large data warehouse profile such as this, it is perfectly possible that you will need in the region of 3.2 TB. The much bigger problem is that there is only 2 TB of usable capacity left on the corporate SAN, and this has been earmarked for applications that have submitted their capacity requirements.

This scenario is likely to lead to a very difficult conversation with the business. It will potentially also lead to a long period of downtime while the storage team order and install additional SAN capacity. It is clear that failing to plan for capacity is a mistake, but it is a scenario that I see frighteningly often.

To avoid this mistake, the DBA team should have worked in conjunction with the development team to estimate future requirements. Calculating the storage capacity requirements of the data files for this application is quite straightforward. The application consumes a flat rate of around 5 GB of data per day. This is what we call a *linear growth pattern* and is illustrated in figure 9.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F05_Carter.png)<br>
**Figure 9.5 Linear growth pattern**

However, imagine that, instead of growing at a flat rate, the amount of data being consumed was doubling every month. Instead of a linear growth pattern, we would instead see an exponential growth pattern, which is characterized by a “dog-leg curve.” This is illustrated in figure 9.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH09_F06_Carter.png)<br>
**Figure 9.6 Exponential growth pattern**

The method of capacity planning we have used for the data files is typically associated with existing systems that have been running for some time. We will have a series of historical data points that we can chart and project forward into the future. Unfortunately, this is not always the case with new (greenfield) applications. In many scenarios, we will not have an even rate of data landing each day.

In these greenfield scenarios, we will typically need to speak to the application owner in the business and assess the amount of business growth that is expected. For example, if the solution is a sales application, we could ask what the sales projections are for the first year of the application.

Once we have as much information as we can harvest from the business, we can define rules and explode the data in the database. For example, in a new sales database, we could manually insert the data for 10 customers, with an average number of delivery addresses determined from interviewing the business. We could then manually insert one sales order for each customer.

We can then explode the data aligned with business expectations. For example, if the sales forecast suggested that in the first year 5,000 customers would be onboarded, and each would be placing an average of 10 orders, then we can use `CROSS JOINS` and numbers tables to duplicate the customer data by a factor of 500 and the order data by a factor of 5,000. This will give us a rough estimate of the expected size of the database.

Performing typical operations against the database once the data has been exploded will also allow us to assess the size requirements of the `TempDB` database and the transaction log file. This is an important consideration, especially in data warehouse scenarios, as the amount of space required for these files may be more than you expect. In my experience, it is not uncommon for each to require in the region of 25% of the size of the database’s data files.

It is also important to remember that we need headroom. For example, if we expect our combined data files, log file, and `TempDB` to be 1 TB, we should add an additional 20% for unplanned growth. Therefore, our estimate would be 1.2 TB.

> [!TIP]
>
> Capacity planning is always a finger in the air, and this is especially true for greenfield systems. It is never a waste of time, however. It is far better to have a best guess on growth than to have no idea whatsoever.

In cloud scenarios, the considerations for capacity planning are slightly different, but they are still equally valid. We need to remember that while we can easily scale an instance up to a larger size, with more memory and compute capacity, this involves a restart of the VM. This means that for 24/7 workloads there will need to be an outage to increase capacity.

Finite cloud

The elastic nature of cloud storage means there is a tendency to think that the cloud has “infinite capacity,” but this is not true. In fact, during the COVID-19 pandemic, at least one major cloud provider experienced capacity issues in some regions due to the explosion of cloud computing caused by the exodus from the office.

An even bigger consideration is cloud financial management. Contrary to an organization’s data centers, which tend to fall under a capital expenditure model, in cloud we move to an operational expenditure model. This means that there is a tangible cost associated with oversized instances and that you will likely have regular conversations with your procurement team, or cloud financial management team, asking for workloads to be right-sized (made smaller). Luckily, we can be proactive about this by reviewing sizing recommendations made by cloud-native tooling, such as Azure Advisor and AWS Trusted Advisor.

This is a perfectly legitimate consideration and contravenes what we have just discussed. The answer needs to be determined by the organization on a case-by-case basis, factoring in cloud capacity reservations, savings plans and discounts, committed spend, and ultimately the tradeoff between cost and potential short outages.

For example, it may be decided that a mission critical system with a five-9s availability requirement (which we will discuss in chapter 13) is hosted on a VM that is initially oversized based on the results of capacity planning. At the same time, it may be decided that less important applications are hosted on VMs that are right-sized and the downtime is acceptable when they need to grow.

To summarize, we should always consider capacity planning for our databases. On premises, we should look at this through a capital expenditure lens, but in cloud we should look at it through an operational expenditure lens and factor cost management into our planning and decisions.

## 9.7 #49 Always placing TempDB and log files on dedicated drives

It has long been recommended that data files, log files, and `TempDB` should always be split onto different volumes, as this provides a performance improvement. But is this still always the correct advice? Let’s return to MagicChoc to examine this question in a little more detail.

All of MagicChoc’s on-premises servers are VMs, with their storage hosted on a SAN. A new server has been built to host the `MarketingArchive` database, and we have built a SQL Server instance. Following official guidance, we have asked the storage team to present three data volumes and the system volume, and we place the `MarketingArchive` data files on one of the data volumes, the `MarketingArchive` log file on another, and `TempDB` on a third.

First, let’s think about this in terms of performance. All three volumes reside on a SAN. To access the data for all three volumes, we will be passing through the same network interface card, traversing the same network path, and using the same SAN fabric; then it transpires that all three volumes have been created on the same RAID array. In this scenario, we are going to see no performance improvement.

But are there any negative consequences? Usually, the answer is no, but I have seen some SAN software that requires the data files and log files to be on the same volume to perform an Application Consistent Snapshot, which can be very useful when upgrading a system to give a quick and easy restore point.

Application Consistent Snapshots

A snapshot is a special type of backup, taken at the storage layer, which uses copy-on-write technology to very quickly take a point-in-time snapshot of a volume. These normal snapshots are called Crash Consistent snapshots. They are not suitable for use with SQL Server, because if transactions are in-flight at the start of the snapshot, they are likely to have corrupted databases when they are restored.

Unlike normal snapshots, an Application Consistent Snapshot integrates with the Volume Shadow Copy Service to flush the memory to disk before the snapshot is taken. This means that, unlike normal snapshots, it can be used to recover SQL Server databases even if there were in-flight transactions at the point the snapshot started.

This, of course, does not mean that `TempDB` and log files should never be separated out. For example, the organizational standard may be to store all databases on a SAN; however, for a specific application, `TempDB` may have very high throughput requirements. In this scenario, it may be beneficial to place `TempDB` on a local M.2 drive.

In summary, if your databases are hosted on a SAN, there is no performance benefit in separating the data files, log files, and `TempDB` onto different volumes. It doesn’t usually cause a problem to do so, but in a small number of scenarios it may remove the ability to take Application Consistent Snapshots. There may be other valid reasons to move logs and `TempDB` to other volumes, however.

## 9.8 #50 Not regularly checking for corruption

Databases can become corrupted. It’s a fact. Corruption can be caused by multiple issues, including abrupt system shutdown, faulty storage, or even malicious activity. Database corruption is a nightmare. The only thing worse than database corruption is not knowing that there is database corruption.

It’s Friday night, and MagicChoc’s on-call DBA is just drifting off to sleep when the phone rings. It’s a P1 issue. While using a mission-critical database, a user has just received an error, and they can’t access the required data.

This is the last thing any DBA needs, as they are settling down on a Friday night, and it could have been so easily avoided by regularly checking for database corruption. It would also have been much better service to the business if the DBA team had noticed before the business experienced an issue.

We can simulate the issue that the user is experiencing by running the script in listing 9.12 to cause corruption in the `MarketingArchive` database. The first thing the script does is back up the `MarketingArchive` database. It is important to run this step so that you can restore from the backup. The script then dynamically creates a SQL statement, which runs the `DBCC WRITEPAGE` command to write a random value to a data page in the `ImpressionsArchive` table. Before we go any further, it very important to understand that `DBCC WRITEPAGE` is an undocumented, unsupported, and *incredibly dangerous feature*, which will write directly to a data page, bypassing the buffer cache. It is great for testing failure scenarios, but it should never be used unless you are 100% comfortable doing so and you have a backup of the database. It should also never be used in a production environment.

WARNING `DBCC WRITEPAGE` *is dangerous*. Do not use it unless you are 100% comfortable and are prepared to lose data. Never use it in a production environment.

Listing 9.12 Simulating data corruption

```sql
USE master ;
GO

BACKUP DATABASE MarketingArchive
TO  DISK = 'D:\Backups\MarketingArchive.bak' ;
GO

ALTER DATABASE MarketingArchive SET SINGLE_USER WITH NO_WAIT ;
GO

DECLARE @SQL NVARCHAR(MAX) ;

SELECT @SQL = 'DBCC WRITEPAGE(' +
(
       SELECT CAST(DB_ID('MarketingArchive') AS NVARCHAR)
) +
', ' +
(
       SELECT TOP 1 CAST(file_id AS NVARCHAR)
       FROM MarketingArchive.dbo.ImpressionsArchive
       CROSS APPLY sys.fn_PhysLocCracker(%%physloc%%)
) +
 ', ' +
(
       SELECT TOP 1 CAST(page_id AS NVARCHAR)
       FROM MarketingArchive.dbo.ImpressionsArchive
       CROSS APPLY sys.fn_PhysLocCracker(%%physloc%%)
) +
', 2000, 1, 0x61, 1)' ;

EXEC(@SQL) ;

ALTER DATABASE MarketingArchive SET MULTI_USER ;
GO
```

If we now run a `SELECT` statement against the `ImpressionsArchive` table, we will see an error, similar to the following:

```sql
SQL Server detected a logical consistency-based I/O error: incorrect
checksum (expected: 0x968b1542; actual: 0x96845542). It occurred during a
 read of page (1:71224) in database ID 11 at offset 0x00000022c70000 in
file 'C:\Program Files\Microsoft SQL
Server\MSSQL16.MSSQLSERVER\MSSQL\DATA\MarketingArchive.mdf'.
```

Of course, it is possible that the error occurred just before the data was accessed, in which case we would have stood no chance of catching it, but it is not unusual for corruption to go unnoticed for an extended period. This makes it much harder to find out exactly when it happened and much harder to fix.

Checking for database corruption is as easy as running the command in listing 9.13, and if we automate the check and schedule it to run periodically, we can configure our monitoring tooling to alert if corruption is found. If corruption is found, the command will return an error.

Listing 9.13 Checking for database corruption

```sql
DBCC CHECKDB('MarketingArchive') ;
```

It is important to check for database corruption on a regular basis. It is much easier to resolve the issue if the corruption is found quickly. It is also a much better service to the business if DBAs notice the problem before the business notices it. If you do discover database corruption, then the best way to fix it is to perform a page restore. This minimizes, and usually completely avoids, data loss. If this is not possible, then `DBCC CHECKDB` can be used to resolve corruption, but if the corruption is in data page(s), this will result in losing the data that was stored on those pages.

## 9.9 #51 Failing to automate

I have heard it said that “Good developers are lazy.” This is not meant as an insult. Instead, it is meant to imply that a good developer will spend twice as long coding something that can be reused to avoid having to write similar code over and over again. The same applies to DBAs. A good DBA will spend time automating as much as possible in an attempt to avoid performing manual tasks. We have already discussed automating SQL Server installations in chapter 8, but in this section I would like us to focus on automating SQL Server maintenance.

Let’s imagine that our MagicChoc DBA performs the majority of SQL Server maintenance manually. Backups are automated, because these are taken by the corporate backup tool (such as Commvault or NetBackup), but all SQL Server–level maintenance, such as index rebuilds, statistics updates, and consistency checks, are performed by DBAs as part of their daily routine. I have seen more than one accidental DBA fall into this trap, but it’s a big mistake, for multiple reasons.

First, performing mundane, repetitive tasks is really quite dull. This can lead to issues such as increasing staff turnover. Second, there is the opportunity cost. If the maintenance is automated, then DBAs can spend their time performing higher-value activities, such as diagnosing complex performance problems. Third, it often simply doesn’t happen. An urgent operational task will come in, and the DBA will drop the routine maintenance to deal with it. Finally, it can lead to human error, caused by people performing repetitive tasks on autopilot.

An even more common mistake that I see—so common in fact, that it is probably in the majority of environments that I have come across—is decentralized automation. To understand this approach, let’s imagine that our MagicChoc DBA is sick and tired of performing manual maintenance every day. Therefore, they decide to introduce some automation. They work through every single SQL Server instance and lovingly hand-craft SQL Server Agent jobs, which perform the routine maintenance on each server. Why is this a mistake?

The answer is because now, every morning, our DBA must wade through every single SQL Server instance and look at the SQL Server Agent job history for every single maintenance job that has been created. While this is somewhat quicker than actually performing the maintenance, it is still subject to the same problems. Specifically, it consists of mundane, time-consuming, manual effort, which is sometimes missed because urgent operational issues arise.

How could we do this better? The answer is to centralize maintenance. If an organization’s SQL Server estate is on-premises, this may involve having a centralized management server, which loops around each SQL Server instance in the estate and runs maintenance tasks.

This configuration works best when we include a registration step in the script or Desired State Configuration manifest that is used to install new SQL Server instances. It does have its complexities, however. When I implemented this in the past, I needed to build a simple scheduling engine, using tables, views, and stored procedures, to ensure that the correct jobs were performed against the correct instances on the correct schedule.

If an organization’s SQL Server estate is in the cloud, there are much more elegant ways of centralizing maintenance. For example, if our SQL server estate is hosted in Azure, we could use Azure runbooks, Azure functions, or a logic app to perform maintenance against Azure SQL Server VMs. If our estate was based around SQL Database, an elastic job would be a good option for configuring the maintenance.

Whatever tool we decide to use, it is important to have centralized, automated maintenance for SQL Server. This reduces manual effort and can mitigate multiple issues. These issues include high staff turnover, missed maintenance, and opportunity cost against higher-value work.

## 9.10 #52 Using cursors for administrative purposes

In chapter 5, we discussed cursors from a development point of view and why we should avoid using them. I have seen several DBAs sensibly enforce the standard of not allowing cursors but then use cursors themselves for their administrative scripts. The excuse for this is normally that the script does not iterate over many objects, so it doesn’t matter.

Imagine that our MagicChoc DBA team enforces a standard that disallows cursors in their environment. They are perfectly entitled to do so, because they are the ones who must troubleshoot performance issues in the production environment—sometimes at unsociable hours. They decide, however, to use cursors for their own administrative scripts.

One of the developers notices that the DBAs are using cursors and raises concern. He is a little annoyed and wants to know why DBAs can use cursors when his team cannot. The DBAs explain to the developer that their scripts are different because they only iterate over a few objects, so the performance impact is not noticeable.

The developer queries this, with the valid point that some of his queries only need to iterate over a handful of rows, so what is the difference? Faced with this, the DBAs reluctantly decide to grant an exception, allowing the developer to use cursors, providing that they do not iterate around many rows.

This starts a snowball effect, where exceptions are coming in from every angle. It becomes impossible to enforce the no-cursor policy. MagicChoc ends up with multiple cursors processing variable quantities of rows. Additionally, the application grows organically over time, and the handful of rows becomes a significant volume.

All of these issues could have been avoided by the DBAs simply following their own coding standards. Providing you understand the alternative techniques, there is never a good reason to use a cursor. For example, the query in listing 9.14 demonstrates how a DBA might use a cursor to iterate over each index in a database and rebuild it.

> [!TIP]
>
> DBAs will often use conditional logic when rebuilding indexes. The examples in this section simply rebuild all indexes. We will discuss conditional logic for index rebuilds in chapter 11.

Listing 9.14 Using a cursor to rebuild all indexes

```sql
DECLARE @Command NVARCHAR(MAX) ;
DECLARE @Table NVARCHAR(256) ;

DECLARE Tables CURSOR READ_ONLY FOR
SELECT name
FROM sys.tables ;

OPEN Tables ;

FETCH NEXT FROM Tables INTO @Table
WHILE @@FETCH_STATUS = 0
BEGIN
    SET @Command = 'ALTER INDEX ALL ON ' + QUOTENAME(@Table) + ' REBUILD ; ' ;
    EXEC(@Command) ;

    FETCH NEXT FROM Tables INTO @Table ;
END

CLOSE Tables ;
DEALLOCATE Tables ;
```

> [!NOTE]
>
> It is worth noting that earlier in this chapter we used the `sp_msforeachdb` stored procedure to iterate over tables, rebuilding indexes. This stored procedure also uses a cursor under the covers.

So how could we rewrite this script to use a more efficient method that avoids cursors? The answer is to use a trick with the XML data type, as shown in listing 9.15. The subquery generates a list of commands that we want to run and the `FOR XML` clause turns this list into XML format. Because we are using `PATH` mode with no XPath expressions, however, it becomes one single, continuous node with no markup. Because the outer query is expecting `NVARCHAR`, the XML is implicitly converted back to this data type so that it can be executed as dynamic SQL.

Listing 9.15 Rewriting the cursor to use a more efficient approach

```sql
DECLARE @SQL NVARCHAR(MAX) ;

SET @SQL = (
    SELECT 'ALTER INDEX ALL ON ' + QUOTENAME(name) + ' REBUILD ; '
    FROM sys.tables
    FOR XML PATH('')
) ;

EXEC(@SQL) ;
```

Not only is this query more efficient, but it is also less than half the lines of code. With small datasets, however, the main benefit is setting a good example for the development team and leading by example.

There is never a sound technical reason to use cursors. There is always a better approach. Although DBAs sometimes use cursors for administrative scripts, this sets a bad example for developers and can lead to political challenges that can snowball. Instead, DBAs should use more efficient techniques, such as the XML trick shown in this chapter.

## 9.11 #53 Failing to patch

Patching is a pain. Any DBA will tell you this, and I am not going to argue. There is always a customer-facing 24/7 system that the business does not want to bring down or a data warehousing application that has an ETL run that goes on for 12 hours and must be completed before the start of the business day.

This results in a mistake that is made by many DBAs, application teams, and application owners, which is to avoid patching of their “golden” systems. So what are the consequences of this mistake? Let’s look at this through the eyes of a MagicChoc DBA.

MagicChoc has two database systems, which it does not patch. It does not patch anything—operating system, middleware, or SQL Server. One of the servers is the database that supports the online sales database, which needs to be available 24/7. The other is the marketing data warehouse. This system processes a huge volume of data, and the ETL processes run all night and must be ready before the start of the next business day.

It is 3 a.m. on a cold night when the on-call DBA is awakened by the service delivery manager. There is a problem—a big problem. Nobody can access any database in the whole company. An attacker has exploited the unpatched servers and gained lateral movement, allowing them to perform a ransomware attack across the whole estate.

I am writing this text just after patch Tuesday in November 2023. In this month alone, the Cumulative Update (CU) for Windows Server fixed over 50 security flaws. Therefore, not patching on a monthly basis is a huge security risk and exposes organizations to all forms of cybercrime.

When we fail to patch, this also exposes our organizations to other risks that are not security related. Vendors such as Microsoft use CUs to fix bugs and provide stability improvements. Therefore, if we do not patch our systems, there is more chance of outages that could have been avoided. Sometimes, even minor new features are released via CU, which we will not be able to take advantage of if we have not patched.

So how can we ensure that all of our servers are patched while still meeting the business requirements? Let’s look at each of the scenarios in turn. First, we will discuss the requirement for the 24/7 system, which cannot be taken down for patching.

If a database has a 24/7 availability requirement, then it must have highly available infrastructure. This means using technology such as *AlwaysOn availability groups*, which uses log streams to synchronize replica databases across multiple servers and can be used to provide disaster recovery or high availability. When used for high availability, it will offer automatic failover of the databases in the event of an outage to the primary server. When used for disaster recovery, the failover will require manual intervention. This technology can also be used to resolve the patching issue.

In this scenario, we can patch the secondary node(s) in the cluster. Once they have been patched, we can perform a controlled failover from the primary replica to a secondary replica before patching the server that was originally hosting the databases. This does technically cause an outage, but the outage will only last a few seconds, rather than potentially lasting a couple of hours for a patching window. The outage is so short from the end user perspective that it is normally no more than a blip.

The second case is that of a data warehouse with an ETL run that takes all night and needs to be completed by morning. A data warehouse may not be suitable for availability groups, which we will discuss further in chapter 13, but it may be hosted on a clustered instance. A *failover clustered instance (FCI)* hosts databases on storage that is shared between multiple nodes in a cluster. This means that if one node fails, another can automatically take ownership of the database, providing automatic failover. Even if an FCI is used, however, even a short outage of around 30 seconds, which is the average time it takes an FCI to failover, may cause serious issues with an ETL run.

Therefore, the resolution to the patching issue is simply down to scheduling. It may be acceptable for a data warehouse to have a 30-second outage during the working day. If this is the case, then a similar method of rolling updates that we described for availability groups can be used. The difference is that the outage would be scheduled during the working day, rather than during a period where there is the least activity.

If this isn’t an option, there will often be a period on weekends where systems such as data warehouses are either not in use or used by far fewer consecutive users. In this scenario, we would simply schedule the outage to happen on a weekend.

To summarize, it is always important that we allow our servers to be patched, even the golden systems that are mission critical. The risks of not patching far outweigh the inconvenience of regular, monthly patching cycles.

## Summary

* Shrinking a database will result in near 100% fragmentation of indexes.
* Never autoshrink databases, as it will cause an endless loop of poor performance and increased resource utilization.
* If we must manually shrink a database, then we must remember to rebuild all indexes after the operation has completed.
* Never rely on autogrow as a means of managing database size, as this may lead to performance issues.
* Do not use multiple log files as that provides no benefit. If there are multiple transaction log files, the first file will fill up before the second file is used.
* Each transaction stored within the transaction log is given an LSN. The LSN identifies a transaction and consists of the VLF, log block, and transaction IDs.
* Transaction log files contain multiple VLFs. Additional VLFs are created automatically when a log file grows.
* Having too many VLFs is known as log fragmentation and can lead to performance issues.
* Internal index fragmentation refers to how full each page of an index is.
* External index fragmentation refers to how many pages of the index are out of sequence.
* To remove log fragmentation, we must shrink the transaction log and then allow it to grow in larger increments.
* Index metadata can be found in DMVs and DMFs.
* When examining index fragmentation, there is a particularly useful DMF called `sys.dm_db_index_physical_stats`. This DMF can return one row for every level of every index in a given database.
* Failing to capacity plan can lead to business issues such as unplanned expenditure. It can also lead to outages if there is a long lead time on new hardware.
* When capacity planning, we try to look for growth patterns, such as linear growth or exponential growth.
* It is not always necessary to place database files, transaction log files, and `TempDB` on different volumes.
* If all of our data is stored on a SAN, then it is common for multiple volumes to reside on the same physical RAID array.
* We should regularly check for database corruption, which can be caused by multiple issues, such as disk errors.
* Fixing database corruption early is easier and has less risk of data loss than not noticing the corruption for a prolonged period.
* Always look to automate wherever possible. Automation reduces manual effort and human error. It also allows DBAs to perform higher-value tasks, such as investigating performance issues.
* DBAs should not use cursors. Instead, they should lead by example and demonstrate to development teams that there is always a more efficient way of doing things.
* Even golden systems should be patched. We should not allow any servers to be excluded from regular patching.
* We can use technologies such as AlwaysOn availability groups or failover clustered instances to reduce outage times for patching.
* We can resolve many patching challenges with creative scheduling.
