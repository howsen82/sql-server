# 12 Backups

This chapter covers

* How to align backup strategies with organizational requirements
* The importance of testing backups and how to do this in a practical way
* How database snapshots and storage snapshots can complement a backup strategy
* The importance of considering extract, transform, and load processes when scheduling backups
* SQL Server’s three recovery models and when each is most appropriate
* Security considerations for database backups

In this chapter, we will discuss common mistakes that database administrators (DBAs) make when planning and implementing backups. Over the years, I have seen a tendency for backup strategies to be treated as something of an afterthought. The value of backups can be difficult to quantify—until such a point when we urgently need to recover a database, of course.

Real-life example

Some years ago, when working as a solution architect and lead developer on a project, I asked the DBA team to back up a production database and restore it over the top of the development database to refresh the environment. Unfortunately, the DBA took a backup of the development database and restored it over the top of production.

I therefore asked the DBA to urgently restore the previous night’s backup of the production database, only to be told that the backup had been failing and the most recent backup available was from three weeks prior.

Fortunately, I was able to rebuild the last three weeks of data from the source data, although this took a horribly long time. In some cases, the lack of backups can cause a company serious issues, including lost revenue, reputational harm, or regulatory noncompliance. There are even examples of organizations going out of business due to lack of backups.

We will begin this chapter by discussing backup strategies and ensuring that we have factored in the business requirements for recovery point objective (RPO) and recovery time objective (RTO). We will then move on to discuss mistakes that DBAs make when relying on snapshots as restore strategies.

There is an old adage that you do not have a backup until you have restored it, and we will explore the impacts of not testing backups. We will then move on to considering backup schedules. Here we will look specifically at the conflicts of scheduling maintenance windows around extract, transform, load (ETL) windows.

We will then look at the different recovery models that can be implemented in SQL Server and the impacts that using the wrong recovery model can have on performance and disk space, as well as the impact of not taking a backup after changing recovery models. We will discuss how we should take ad hoc backups in a way that avoids breaking the restore sequence before finally looking at the security implications of our backup strategy.

## 12.1 #78 Not considering RPO and RTO

*RPO* refers to how much data it is acceptable to lose in the event of a failure. *RTO* refers to how much time it will take to recover from a failure. These are both critical considerations when planning a backup strategy for a database.

If we ask the business how much data they can afford to lose in the event of a failure, their answer will inevitably be “None.” Similarly, if we ask the business the acceptable duration of an outage in the event of failure, their answer will usually be “I need instant recovery.”

If these assertions are true, then we should configure high availability and disaster recovery for the applications, which will be discussed in chapter 13. If, however, we dig a little deeper with the application owner and explain the costs involved in hosting the redundant, geographically dispersed servers required for such a topology, we will often find that the requirements are not as they first appeared. For example, if an application is considered P4 (where P1 is a mission-critical application), then it is unlikely that building a highly available topology would make good business sense, and, in fact, they can rely on a backup–restore strategy.

These conversations can be a little arduous sometimes, depending on the skills and personality of the application owner. Because of this, a mistake that I see many DBAs fall into is simply putting all databases on the same default schedule and avoiding the tricky conversations with application owners.

This is a mistake that we should avoid, however. If we simply use a default schedule, without understanding the RPO and RTO requirements, we can cause issues. For example, if our default schedule allows for 1 hour of data loss but it would cause serious business impact if they lost more than 30 minutes of data, then we are storing up problems for later down the line.

If, on the other hand, we had a default schedule that allowed for 30 minutes of data loss, but only a very small percentage of applications had an RPO of less than 4 hours, then we will be unnecessarily using resources to take backups that are not required. We will also be using more storage than we need. This has a cost associated with it, and if the storage is in cloud, it is a directly addressable cost.

If we fail to consider RTO, we could land in a situation where we have significant business disruption caused by not being able to recover the databases in an acceptable amount of time.

To understand how we can impact the RPO of a database, let’s remind ourselves of the different types of backup that we can take in SQL Server. The backup types at our disposal are detailed in table 12.1.

Table 12.1 Backup types

| Backup Type | Description |
| --- | --- |
| Full | Backs up all data within the database* |
| Differential | Backs up data pages within the database that have changed since the last full backup* |
| Transaction log | Backs up the transaction log, which includes a record of all transactions since the last transaction log backup |
| *It is possible to limit a backup to specific data files or filegroups. | |

A full backup has the most impact on resources and can cause a performance problem on a busy system. Therefore, full backups are usually taken in a maintenance window. The impact of a differential backup is variable. It depends on how many pages within the data files have changed since the last full backup. Therefore, while they are usually far more lightweight than a full backup, there can still be an impact on busy online transaction processing (OLTP) systems.

A transaction log backup does not touch the data files and simply backs up the transaction log. Because the transaction log records all transactions since the last log backup, it is the equivalent of an incremental backup in some systems. During a restore operation, these transactions are replayed into the database, meaning that we can perform a *point-in-time recovery*, which is where we can stop the restore at a specific point in time, in the middle of the transaction log backup file. This is very useful in situations where there has been user error leading to some data being mistakenly updated or deleted. We can stop the recovery immediately before the erroneous operation. This is not possible from a full or differential backup, as they are backing up data pages. Figure 12.1 illustrates a *restore chain*, which is the order in which we restore backups to recover a database.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F01_Carter.png)<br>
**Figure 12.1 Restore chain**

In a typical backup schedule, we may schedule a full backup to occur overnight when an application is not being used. We might also schedule a differential backup to occur at lunchtime, when there is minimal usage. To meet the required RPO, however, the key is transaction log backups. We will schedule these to be taken in line with the RPO. For example, if we have an RPO of 1 hour, we will schedule transaction log backups to happen on an hourly basis.

The biggest consideration with RTO is where the backups are stored and how quickly we can retrieve them. In the worst example I have seen, database backups were taken by an enterprise backup tool, which compressed and deduped them before sending them directly to tape. There was then a 6-hour service level agreement (SLA) to retrieve them from the tape robot.

A more common scenario is that an enterprise backup tool caches backups on disk for a period of time, usually a few days, before sending the disks to tape. In a cloud environment, the equivalent is an orchestration where the backup tool stores backups on instant access storage, such as S3 in AWS or Blob Storage in Azure for a few days. The data is then moved into archive storage, such as Azure Deep Archive or Azure Storage Archive Access Tier. This storage is not instant access. In the case of Deep Archive, for example, the SLA for retrieval is 24 hours, although in reality it is usually returned much faster.

We may also face situations where we have a backup team that is slow to respond. I have witnessed a request for the previous night’s backup being delayed by multiple hours, just waiting for someone in the backup team to pick the request off their queue.

In situations like these, if we have a short RTO, we may choose to avoid the enterprise tool taking database backups using a SQL Server Agent. Instead, it may be appropriate to schedule our own, native SQL Server backups and then just have the enterprise backup tool offload the backup files.

If we take this approach, however, we must be mindful that it becomes our responsibility to ensure that backups are successful and to remediate where required. We also need to be mindful of duplicating storage costs. If we do this for many large databases, the extra storage required on the database servers can mount up. If our servers are in cloud, this can result in a substantial, direct cost increase.

Therefore, if we do need to cache our backups locally, we need to ensure that we have processes in place to remove old backup files. The PowerShell script in listing 12.1 demonstrates how we can remove backup files that are older than three days, assuming a backup location of `D:\Backups`. We could schedule this to run with SQL Agent or with Windows Scheduled task.

Listing 12.1 Deleting old backups

```sql
$Path = 'D:\Backups\'
$Days = 3
$CutoffDate = (Get-Date).AddDays(-$Days)

Get-ChildItem -Path $Path -Recurse | Where-Object {$_.CreationTime -lt
$CutoffDate} | Remove-Item –Force -Recurse -Verbose -Confirm:$false
```

We should always consider the business requirements when planning a backup strategy for a given database. While it might seem easy to have a single backup schedule for all of our databases, we are simply storing up problems for the future. Instead, we should ensure that we understand the RPO and RTO of a database and plan the backup strategy for each database such that the requirements are met and without unnecessarily using resources.

## 12.2 #79 Using database snapshots as a recovery strategy

Database *snapshots* use copy-on-write technology to take a read-only, point-in-time snapshot of a database. These snapshots can be incredibly useful for a number of use cases. For example, snapshots are a great choice for generating reports of data at a given point in time. They are also great if we need to generate reports that compare current data to a point in time. What’s more, they can be used to protect data from human error. If someone accidentally deletes data, it is easier and quicker to copy data back in from a snapshot, or even revert the whole database to the snapshot, than it is to perform a restore to a different location and copy the missing data back in. They can also be used to roll back a failed deployment, providing the source database is still healthy.

It is clear that snapshots can complement a backup strategy. A mistake I have seen made, however, is to use snapshots as a replacement for a backup strategy. This is a situation that we must absolutely avoid. To understand why, we need to understand how database snapshots work.

The image in figure 12.2 illustrates the snapshot process. When a database snapshot is first created, it is entirely empty. This is known as a *sparse file*, and the technology is implemented using New Technology File System (NTFS) sparse files. Every time a page is modified in the source database, that page is copied over to the snapshot database, before the update is made. When a user queries the snapshot, SQL Server will see if the required data pages exist in the snapshot. If they do, they will be returned from the snapshot. If they do not, they will be returned from the source database.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F02_Carter.png)<br>
**Figure 12.2 Snapshot process**

You can see that the snapshot is entirely dependent on the source database. If the database were to be corrupted or deleted, the snapshot would also become unavailable. What’s more, even if the snapshot was available, its dependence on the source database means that it cannot be moved to a different instance. Therefore, if the instance or server were unavailable, we would not be able to recover from the snapshot.

While database snapshots are very useful for several use cases, including complementing a backup strategy, they cannot replace a backup strategy. A decision to implement database snapshots should have no bearing on our backup strategy.

## 12.3 #80 Using crash-consistent snapshots as a recovery strategy

Most enterprise-class storage solutions provide a snapshot solution, allowing us to perform a backup of a disk or a whole VM. The implementation of snapshot technology varies from vendor to vendor, so we will not dwell on that here. There are, however, some common concepts that we need to understand.

First, snapshots are *block-level backups*, which means that the blocks on the physical disk are being backed up, as opposed to files that are accessible at an operating system level. Second, there are different types of snapshots, each of which provides a different level of consistency.

The most basic level of consistency is a *crash-consistent snapshot*. With this model, the write order of data is preserved, and all blocks on the disk are backed up based on a specific, identical timestamp. This leaves files in a consistent state.

The problem is that SQL Server does not work in the same way as a filesystem. During a transaction, there could be some data in the buffer cache, pending a write to disk, and other data already written to disk. This means that when the disk or VM is restored, the database may be in an inconsistent state if transactions were in flight at the point of the backup.

Not understanding this leads to a mistake when storage and cloud teams can provide protection to VMs hosting databases by configuring crash-consistent snapshots. They do not realize that the snapshots may be causing corruption until the VM is restored.

Snapshots are often used for the quick recovery of a VM in the event of bad change. Therefore, many change plans will have a rollback plan, where a server is restored from a snapshot. If crash-consistent snapshots are being used, then this rollback plan may not suffice.

Another level of protection is an *application-consistent snapshot*. With this type of snapshot, databases are quiesced and data in memory is flushed to disk before the snapshot occurs. This ensures database consistency and makes this type of snapshot a safe option.

Application-consistent snapshots of disks hosting Windows servers are performed by using Volume Shadow Copy Service (VSS) to deal with the pending I/O. Support for application-consistent snapshots in Linux is patchy, however. For example, at the time of writing, VMware only supports Linux application-consistent snapshots with the implementation of pre-freeze and post-thaw scripts, which must be written manually. There is a similar limitation in Azure, but Microsoft provides a template configuration file on GitHub that we can populate. This configuration file can be found at <https://mng.bz/GNzO>.

Reversing the mistake

I have also seen this mistake flipped. In some organizations, teams do not use snapshots for protecting database servers at all. This is because they are aware of the corruption that can be caused by crash-consistent snapshots and therefore shy away from snapshots entirely.

In reality, application-consistent snapshots can provide a very useful mechanism by which to quickly recover from a failed deployment. Of course, they should only supplement a proper database backup strategy, as opposed to replacing it, but they still have value.

While it is unlikely to be the responsibility of a DBA to configure storage snapshots, it is very important for DBAs to understand the implications of snapshots and how they should be configured. This allows the DBA to provide input and advice to the storage or cloud team that is tasked with configuring snapshots.

Snapshots can provide a quick recovery and can be a good supplement to a database backup strategy. We should advise storage teams to only configure application-consistent snapshots for database servers, however. Crash-consistent snapshots should not be used, as they may lead to restoring corrupt databases.

## 12.4 #81 Not testing backups

Let’s go back to that old adage: “You do not have a backup until you have restored it.” There is certainly some truth in this. When we are performing a backup, we are performing a heavy I/O operation to a disk, and this is often over a network. There is every possibility that there could be a blip on the network, a blip on the storage, or a bad sector. Any of these could cause a bad write, and we would not know that our backup was corrupt until we tried to restore it and find that we are unable to.

I can imagine you rolling your eyes as you read the paragraph above and thinking “But I can’t restore all of my backups. I don’t have the time or the disk capacity to do so!” You would be right, and I would never evangelize for an unrealistic expectation. There is a middle ground where we can increase confidence in our backups without costing a fortune in time and disk resources, but it is an approach that is commonly missed by DBAs. Not finding a middle ground is a mistake that can come back and bite very hard.

So what is this middle ground? Well, there are multiple aspects to it. First, we need to check that our backups have been completed successfully. I know this sounds obvious, but it is surprisingly common for DBAs to either not check at all or simply to ignore any failures. Second, we should verify the integrity of our backups. We will discuss each of these points in the following sections.

### 12.4.1 Checking that backups completed successfully

If database backups are taken through an enterprise backup tool, then it is likely that a report will be sent out each day, detailing any failed backups. As DBAs, we should ask to be included in the distribution list of that report. We should review it with our first cup of coffee each morning and chase the backup team to remediate any backup failures by rerunning the job in an appropriate window. Of course, the longer a database has been without being backed up, the greater the risk exposure. If a backup fails multiple times, we should work with the backup team to investigate the cause.

If backups are taken using native SQL Server backups, we should check that the backups are successful and remediate any failures ourselves. This can sound like an impossible task, however, especially in large estates. Therefore, it is important that we add some alerting or reporting around it.

If we have a monitoring tool that is used to monitor our SQL Server instances, we can write a custom check that checks for the success of the backup jobs. If we do not have this option, however, there are multiple ways to achieve the same outcome using SQL Server–native features.

The first way would be to add SQL Agent alerts on the SQL Agent jobs that perform the backups. The trouble with this approach is that we would also need to enable Database Mail on every SQL Server instance. This is rather cumbersome to manage and is not in line with the CIS Level 1 security benchmark for SQL Server.

The second method would be to configure a Central Management Server. This can be achieved in SQL Server Management Studio by navigating to the View menu and selecting Registered Servers. This causes the Registered Servers window to be displayed. Here, we can create a Central Management Server. Under this newly created node, we can create a Server Group that contains all of the instances in the estate. Finally, in this group, we can register all of our instances. The Registered Servers window with Central Management Server is shown in figure 12.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F03_Carter.png)<br>
**Figure 12.3 Registered Servers window**

This allows us to run a query against a group of instances. When we do this, a column called `Server` will be added to the output of a query to let us know which instance the row has been returned from. Each morning, we can then run the query in listing 12.2 against the server group to see a list of any failed backups on the last run. The query uses the `ROW_NUMBER()` function to assign an incrementing number to each row. The `PARTITION BY` clause ensures that row numbers will be calculated for each individual server, and the `ORDER BY` clause will ensure that the latest execution receives the value of `1`. The outer query then filters all rows that do not have a row number of `1` (the latest execution).

WARNING To use this script, ensure that you replace the job name with your own. The script assumes you have used a consistent job name for your backup jobs across all instances.

Listing 12.2 Retrieving failed backup jobs for all servers

```sql
SELECT
      Server
    , name
    , message
    , run_status
FROM (
    SELECT
          Server
        , j.name
        , jh.message
        , jh.run_status
        , ROW_NUMBER() OVER(PARTITION BY Server ORDER BY run_date, run_time DESC) AS RowNumber
    FROM msdb.dbo.sysjobhistory jh
    INNER JOIN msdb.dbo.sysjobs j
        ON j.job_id = jh.job_id
    WHERE jh.run_status = 0
    AND j.name = 'Backups'
) Results
WHERE RowNumber = 1 ;
```

The third and most complex method is to create an inventory database that stores details of all SQL Server instances within the estate. This database will ideally be populated as part of our automated instance provisioning process. It can be used for many purposes; hence the possible data items that can be stored in it are endless. For this specific purpose, however, we would need to record the server and instance name, the name and encrypted password of a login with appropriate permissions, the databases hosted on the instance, and the backup schedule required for each database.

This will allow us to write a scheduling engine that can work out which databases are due to be backed up on which servers at any given time. We can then schedule a PowerShell script to run every minute using SQL Agent. This PowerShell script will pull a list of databases that need to be backed up from the scheduling engine. The script can then pull the user and password details for the relevant instance and iterate over each to back up relevant databases. The script can then write the results back to the database. This allows us to easily manage backups from a single location, including changing backup schedules. It also allows us to easily report on backup failures from a single location.

My personal preference is to use an inventory database with a scheduling engine. I have created this in a few roles over the years, and it has worked well. I tend to use it for scheduling all common database maintenance, not just backups. It is worth considering, however, that the first time I created this it took nearly three months of effort to get everything written and set up as per the requirements of the company I was working with. While subsequent implementations have been much quicker because I now have the base code, it still takes a considerable amount of time to get everything configured and tested. Therefore, it is not an appropriate option for many organizations.

If a custom scheduling engine is not appropriate for your environment, I would recommend using a Central Management Server. This option is not ideal, because it relies on features of SQL Server Management Studio, rather than just core SQL Server features. It is still preferable to trying to manage Database Mail on every instance, however.

### 12.4.2 Verifying backup integrity

SQL Server provides the possibility for us to verify that a backup is valid without actually restoring it. The process reads the entire backup set to ensure that it is complete and accessible. It also performs limited data checking in an attempt to highlight as many issues as possible. If the backup has been taken with a checksum, it will validate this. It will also ensure there is enough space on disk to perform the restore.

When taking a backup, it is possible to enable page checksums. This causes a checksum of each page to be calculated and stored in the page header before it is written to disk. When the page is read from disk, the checksum can be recalculated to ensure it matches. The command in the following listing demonstrates how to turn this option on for the `Marketing` database.

Listing 12.3 Turning on page checksums

```sql
ALTER DATABASE Marketing
SET PAGE_VERIFY CHECKSUM ;
```

If this option is turned on, these checksums will be validated when reading pages from disk. If we then use the `WITH CHECKSUM` option when taking the backup, SQL Server will generate a database-wide checksum. When we restore a backup, or in this use case, perform a backup verification, SQL Server will ensure that the database checksum is valid.

> [!NOTE]
>
> Using the `WITH CHECKSUM` option is the best option to ensure backup integrity, but it will have an impact on backup throughput. You will likely see a degradation in backup performance, especially for large databases.

The command in listing 12.4 takes a backup of the `Marketing` database using the `WITH CHECKSUM` clause to generate a checksum. We will then be able to validate the checksum when we verify our backup.

> [!TIP]
>
> When following examples in this chapter, be sure to change the backup location to match your own.

Listing 12.4 Backing up a database with `checksum`

```sql
BACKUP DATABASE Marketing
TO  DISK = 'D:\Backup\MarketingFull27122023.bak'
WITH
      NAME = 'Marketing-Full Database Backup'
    , CHECKSUM ;
```

We can verify our backup using the `RESTORE VERIFYONLY` command. This is demonstrated for the `Marketing` database in the followng listing.

Listing 12.5 Verifying a backup

```sql
RESTORE VERIFYONLY
FROM DISK = 'D:\Backup\MarketingFull27122023.bak'
WITH CHECKSUM ;
```

If we take backups using native SQL Server backups, we can add a job step after the step that performs the backup to run the `RESTORE VERIFYONLY` command. We should then ensure that this job step completes successfully using one of the techniques discussed in the previous section.

If backups are being taken using an enterprise backup tool, things get a little more complicated. Most market-leading tools support the `RESTORE VERIFYONLY` feature when using their SQL Server Backup Agent, but they don’t necessarily make it easy to run for all databases across the whole estate. In this instance, we will need to work with the backup team to create a scripted approach that uses the backup tool’s API.

It is unlikely to be practical to fully restore every single database backup that is taken. We can find a middle ground, however, by checking that backup jobs are successful and by verifying backup integrity. If you do have the infrastructure capacity to configure an automated restore, you should consider taking it. Even with `CHECKSUM` and `RESTORE VERIFY ONLY`, it is still possible to have a corrupt backup.

## 12.5 #82 Taking backups during an ETL window

A well-written ETL process will be rerunnable, or, to put it another way, it will support eventual convergence. This means that, if the process were to fail part of the way through, the process can be rerun and all data processed with no data duplication occurring. For an ETL process to be rerunnable, several conditions must be met. First, the source data must be available in its original form. This can be achieved through data retention, if the source data comes from flat files, or from a configurable API. It is more complicated if the source data comes from a transactional database and will usually involve caching the data. The second condition is that the ETL process must have parameterized dates. If a process is scheduled to run at 10 p.m. and has `WHERE DataDate = GETDATE()` hardcoded, it is not rerunnable. Technically, the code could be modified and the process manually run, but the spirit of rerunnable processes means that they can be rerun without any code changes. Finally, and arguably most importantly, the process must merge data into tables as opposed to inserting it. If a process uses `INSERT` statements, there is a good chance that rerunning the process will result in some form of data duplication in a complex ETL run. This is because even though a failure will result in a transaction being completed or rolled back, large, complex ETL processes require many transactions. Therefore, even though the data will be transactionally consistent, it may not be consistent through the lens of business logic.

Unfortunately, we must accept that not all ETL processes are well written. Imagine that we have an ETL process that is not rerunnable. It uses `INSERT` statements across multiple transactions, generating new rows in multiple tables. The tables have `IDENTITY` columns, so each row is unique to SQL Server but allows for duplicate business keys. This scenario is fairly common. Let’s assume that the process is configured to run at 1 a.m. and runs for 5 hours. Let us also assume that our backup schedule for this server runs a full backup at 2 a.m.

Within the first hour of running, the ETL process has extracted all of the data from various disparate sources and loaded staging tables in a data warehouse. It has also started populating tables within the star schema of the database, some of which require multiple layers of complex transformation. At 2 a.m. the backup begins, and the read phase of the backup completes 10 minutes later; this is the point of consistency for the backup—the point in time that we would restore to if we restore from this backup. The backup finishes at 2:18 a.m. and the ETL process completes at 5:56 a.m. But then disaster strikes. At just after 6 a.m., the storage fails completely and corrupts the database. We have no choice but to restore it from a backup. This issue is illustrated in figure 12.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F04_Carter.png)<br>
**Figure 12.4 Backup during ETL**

We restore the database from the backup that we took at 2 a.m., and the application support team reruns the ETL process. Unfortunately, when the business performs its checks on the recovered system, it finds that the data is inconsistent. Some but not all data has been duplicated. We restore the previous night’s backup, and the application support team reruns the last two nightly ETL runs. The business checks the data and finds that there are now duplicates for the previous day.

> [!NOTE]
>
> that, given the ETL process takes around 5 hours to complete and assuming the restores take 30 minutes and the data checks also take 30 minutes, we are now 18 hours into a P1 incident, and we have still not restored service. Everybody is very tired and very stressed, and the next day’s ETL run is due to start in 1 hour.

This situation could well be described as a nightmare. I know this because it is loosely based on a scenario that I encountered on a mission-critical system I inherited some years ago. There are only two solutions in this scenario. The first is for a developer or application support team to spend the next [insert a scarily high number] of hours manually running queries to deduplicate data in base tables and deleting data from the tables that store the complex calculations. They would then have to manually run some aspects of the ETL process to perform the complex calculations.

The second option would be to restore versions of the database from the last two backups. A developer or application support team would then spend the next [insert equally scarily high number] of hours attempting to merge data from the two databases to form a single, coherent dataset. They would likely then have to delete data from the tables that store the complex calculations. They would then have to manually run some aspects of the ETL process to perform those complex calculations again.

Either method is equally risky and time consuming. Both methods are likely to leave the data in an imperfect state, unless there is as much luck as there is skill involved in the fix. Depending on the type of data being fixed, this could potentially result in poor business decisions being made or even regulatory noncompliance. The simple moral of this story is that it is a mistake to take backups at the same time as an ETL process is running. DBAs should always assume the worst.

Another consideration, even if our ETL processes are well written, is performance. Complex ETL processes are often resource intensive and can require good disk performance. If we run backups at the same time as the ETL processes, then we are very likely to cause a performance impediment.

So what should we do differently? I have seen scenarios with long-running ETL processes, which means there is no time to perform a full backup of the database outside of business hours, either before or after an ETL process.

The answer depends very much on the data involved, but there are multiple possible solutions. The first possibility I tend to look for is what I call *creative scheduling*. Often a data warehouse is sized to allow for ETL processes to run efficiently within a window. This can mean the server is actually oversize for its use during the working day. If this is the case, we may be able to just take the full backup during the business day, when data is being read but not modified.

This scenario is illustrated in figure 12.5, which illustrates disk throughput over a 24-hour period, as a percentage of the maximum possible throughput. The working day has significantly lower throughput than the maximum, even when we take a full backup in the middle of the day.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F05_Carter.png)<br>
**Figure 12.5 Disk throughput over 24 hours**

If creative scheduling is not possible, then another option is to use differential backups. In this scenario, we would take a full database backup at weekends and a differential at the end of the ETL run each day. This can be a good, pragmatic solution but may not work in some cases. A differential backup contains all pages that have been modified since the last full backup. If we are running a large ETL process every night and also performing tasks such as index rebuilds (see chapter 11), then by the end of the week a differential backup may be so large that it also does not complete within the given window.

Finally, we could consider filegroup backups. This is my option of last resort, as it introduces both complexity and risk. In this scenario, we would back up different filegroups on different days of the week, as illustrated in figure 12.6. The complexity comes from having to piece together multiple backup files to perform the restore. The risk is that we may not be able to meet the required RPO.

Caution If we have an RPO of 24 hours (which is common in this kind of system), we would not meet the requirements with this approach. The business would need to accept this risk before we implemented it.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F06_Carter.png)<br>
**Figure 12.6 Filegroup backup strategy**

We should always consider ETL processes when scheduling backups and avoid them overlapping. Sometimes we can achieve this by simply scheduling backups to occur in a different window. If this isn’t possible, we could consider taking nightly differential backups and weekly full backups. As a last resort, we could consider filegroup backups, but this not only adds complexity but also has negative effects on the RPO.

## 12.6 #83 Always using the FULL recovery model on data warehouse and development systems

The database *recovery model* defines how a database is protected. There are three recovery models; `FULL`, `SIMPLE`, and `BULK LOGGED`. When a database is set to `SIMPLE` recovery model, we are only able to take full backups and differential backups. Transaction log backups are not possible. Full backups are mandatory to be able to restore the database, and differential backups are optional. When in `SIMPLE` recovery model, SQL Server automatically truncates the transaction log when a checkpoint occurs. This process is used to flush modified (dirty) pages from the buffer cache to disk. A checkpoint is triggered based on the specified recovery interval of the database and when certain operations, such as a backup, are performed. It can also be triggered manually by a DBA running the `CHECKPOINT ;` command. These recovery models are set out in table 12.2.

Table 12.2 Recovery models

| Recovery model | Full backups | Differential backups | Transaction log backups | Log truncation | Protection level provided |
| --- | --- | --- | --- | --- | --- |
| `SIMPLE` | Yes | Optional | No | At checkpoint | Transactionally consistent |
| `BULK LOGGED` | Yes | Optional | Yes | Log backup | Recovers to end of a backup |
| `FULL` | Yes | Optional | Yes | Log backup | Recovers to a specific point in time within a backup |

> [!NOTE]
>
> In recovery models other than `SIMPLE`, a checkpoint does not truncate the transaction log.

When we are in `FULL` recovery model, we must take both full and transaction log backups. Differential backups are, again, optional. The transaction log backup allows us to perform a restore to a specific point in time. This is opposed to full and differential backups, where we must restore the complete backup set. When in `FULL` recovery model, it is vital that we perform transaction log backups. This is because when in `FULL` recovery model, transaction logs are not automatically truncated. Instead, they are truncated as part of the log backup process. Therefore, if we do not back them up, they will just continue to grow until they have consumed all the disk space.

`BULK LOGGED` recovery model is a specialized model that does not allow point-in-time recovery. It can be used to improve the performance of bulk insert operations. We would usually only use this model for a short time, during a heavy ETL process, when the normal recovery model for the database is `FULL`.

There is a common misconception that `FULL` recovery model should be used for all databases. There is an even more common misconception that `FULL` recovery model should be used for all production databases but not for nonproduction databases. Following these misconceptions would be a mistake.

The choice of recovery model should be based primarily on the recovery requirements for a database. If we consider a development database, it is likely the data rarely changes. It is much more likely that the database schema changes, and if the developers are following modern development practices, these changes will frequently be checked into source control.

> [!TIP]
>
> See chapter 7 for more information on source control.

This means that the RPO for the database will be much higher than for a production database and there is unlikely to be a requirement to restore it to a specific point in time. This means there is no need to take transaction log backups. We can simply take a daily, or perhaps even a weekly, backup on the database. If we were to use `FULL` recovery model for such development databases, we would also need to manage the transaction log backups, which means additional effort for no benefit.

That explains why `SIMPLE` is often the correct recovery model for a development database, but what about data warehouses that run in production? Because they are production databases, then surely we will have a much lower RPO and a requirement to restore to a point in time, right?

Well, this is not often the case. Consider a typical data warehouse or data mart that is populated by a nightly ETL process and then is used for reporting during the day. In this scenario, the RPO of the database is actually 24 hours. The only need is to be able to restore the database to a point after the last ETL run. Therefore, `SIMPLE` recovery model is often the most appropriate choice.

I have seen several large data warehouses that have suffered performance problems on their nightly ETL runs because of a poor choice of recovery model. The issue is that when we are in `FULL` recovery model, much more information is written to the transaction log. SQL Server needs to be able to restore the database to any given point. Therefore, the logging is verbose, even including the specific pages that have been allocated to complete a transaction.

In `SIMPLE` recovery model, however, SQL Server only needs the transaction log to be able to roll back a transaction and leave the database in a consistent state. Therefore, far less data needs to be written. To follow from the example of `FULL` recovery model recording every allocated page, `SIMPLE` recovery model, by contrast only needs to record the allocation of extents. Because there are eight pages in an extent, this can be as low as 1/8 of the data.

We should always make the best possible decision for a specific use case. This means that we should consider the most appropriate recovery model for any given database, as opposed to using a blanket rule.

## 12.7 #84 Using SIMPLE recovery model for OLTP databases

In the last section, we explored the consequences of the inappropriate use of the `FULL` recovery model. In this section, we will flip that mistake around and explore the consequences of inappropriate use of the `SIMPLE` recovery model.

I have seen some environments where DBAs have used `SIMPLE` as their default recovery model for all databases, including databases with OLTP-style workloads, in production. Reasons for this include problems with the transaction log filling up the disk and the system being too busy to tolerate more than one backup a day. These reasons are often erroneous, however. Let’s explore them.

Regarding the transaction log filling up the disk, we explored log truncation in the previous section and the fact that, in `FULL` recovery model, transaction logs are truncated at the point of a transaction log backup. Therefore, we can avoid the transaction log continuously growing and consuming all disk space by ensuring we take transaction log backups. If we take a transaction log backup and the log is not truncated, then we need to investigate the reason. The query in the following listing will determine why the log was not truncated on the previous attempt.

Listing 12.6 Exploring the reason the log was not truncated

```sql
SELECT
      name
    , log_reuse_wait_desc
FROM sys.databases ;
```

Many of the reasons that a log fails to truncate are transient and down to unfortunate timing, such as no checkpoint occurring since the last backup, an active backup or restore process being in operation, or the creation of a database snapshot being in progress.

There are some other reasons that are usually transient but that, if they occur repeatedly and cause the transaction log to start growing wildly, should be investigated. For example, a common reason is that there was an active transaction. This is usually transient, but if our data-tier application has many long-running transactions, this can cause problems—and not just issues for log backups but also issues with lock contention or deadlocks. Developers may need to address this to avoid transactions running for extended periods of time.

If the reason for the log not being truncated is either `REPLICATION` or `AVAILABILITY_REPLICA,` this means that transactions are being applied to the secondary databases. If this persists, there may be an issue with delivering or applying these transactions. We should check our monitoring tools and dashboards to ensure that our replication or AlwaysOn availability group topology is healthy and to understand the latency in applying transactions.

A wait type of `XTP_CHECKPOINT` can occur when we use memory-optimized tables. Memory-optimized checkpoints flush the data streams of persistent memory-optimized tables to disk. A log truncation issue related to memory-optimized checkpoints can occur for several reasons that need to be dealt with. First, in older versions of SQL Server, such as 2014 or 2016, there were some bugs that caused issues with memory-optimized table checkpoints, and we should apply the latest service pack and cumulative update.

Another reason for this issue is that, when we have memory-optimized tables, automatic checkpoints only occur when the transaction log has 1.5 GB of transactions since the last checkpoint. For smaller systems, I have seen this lead to not enough data being written to the log to ever cause a checkpoint to occur, and therefore log truncation never happens. If this is the case, we may need to schedule a manual checkpoint to occur prior to the transaction log backup. If our backups are scheduled with SQL Agent, we can simply add a job step before the step that takes the log backup. If our backups are taken by an enterprise backup tool, we will have to collaborate with the backup administrators to add a prebackup step into the backup policy.

> [!TIP]
>
> If a server has 16 cores and 128 GB of RAM or more, large checkpoints will be enabled. This means that the checkpoint only occurs after 12 GB has been written to the log since the last checkpoint.

It is important to understand that, although referred to as log truncation, it is actually virtual log files (VLFs) that are truncated. Truncation can only happen as far as the first active VLF. If any VLF other than the VLF currently in use fails to truncate, the `log_reuse_wait_desc` column will show the reason. The transaction log illustrated in figure 12.7 would show a `log_reuse_wait_desc` of `ACTIVE_TRANSACTION`, although two VLFs are truncated.

This means that all but one of the VLFs may have truncated and there is no issue. We can determine the number of VLFs in use by running the query in listing 12.7. This query returns the total number of VLFs, the number of VLFs in use, and the reason for the last truncation failure (which is the same as `log_reuse_wait_desc`) for the `Marketing` database from the `sys.dm_db_log_stats` DMF.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F07_Carter.png)<br>
**Figure 12.7 VLF truncation**

Listing 12.7 Determining how many VLFs are active

```sql
SELECT
      total_vlf_count
    , active_vlf_count
    , log_truncation_holdup_reason
FROM sys.dm_db_log_stats(DB_ID('Marketing')) ;
```

When we think about a system being too busy to accept more than one backup a day as a reason for not using `FULL` recovery model, we need to take a step back and think about the implications of this. There are two main points to consider.

First, we need to consider the RPO of the database. If a database is so busy that it is difficult to schedule multiple backups, then it is likely that the database has a high rate of change. This means that the database may have a very short RPO. If this is the case, it is likely that taking a single, full backup each day will not meet our recovery requirements.

The second thing we should consider is the impact of taking the log backup. While a full backup can cause a performance impact on the database and is best scheduled during a period of low or no use, a transaction log backup has comparatively small impact. The transaction log is usually no more than 20% of the size of the database in total and will not necessarily be full at the point we take the backup. In fact, the more often we take a transaction log backup, the fewer VLFs will need to be backed up and the smaller the impact.

If a transaction log backup is causing performance issues on a data-tier application, there are several things that we can consider. We could consider taking log backups more frequently, which would reduce their duration. If we are using AlwaysOn availability groups, we could schedule our backups to occur on a secondary server, meaning that there is no performance impact on the primary. Finally, if a transaction log backup is causing performance issues, we need to ask ourselves if the server is appropriately sized. While I usually don’t advocate for “throwing more hardware at the problem,” a server needs to meet its use case, and that includes catering to nonfunctional requirements such as RPO.

If we do not use `FULL` recovery model for databases with OLTP-style workloads, then we also need to consider the functionality that will not be available. High availability and disaster recovery technologies, such as AlwaysOn availability groups and log shipping, only work in `FULL` recovery model. Transactional replication, which is a data distribution technology, will not work without `FULL` recovery model either.

It is usually a mistake to use `SIMPLE` recovery model for a database that supports OLTP-style workloads in a production environment. We can overcome issues with using `FULL` recovery model by ensuring that we have a transaction log backup strategy or by troubleshooting issues with long-running transactions and memory-optimized tables.

The most important reason for using `FULL` recovery model is to ensure that we have an RPO that meets the needs of the business. We should also consider, however, high availability and data distribution technologies, which rely on `FULL` recovery model.

## 12.8 #85 Not backing up after changing recovery model

A common mistake I see when a preproduction database is promoted in place to become a full production database is that the recovery model is changed from `SIMPLE` to `FULL` and transaction log backups are scheduled alongside the full backups and left to run. This sounds perfectly reasonable, doesn’t it? So why is it an issue?

Let’s imagine that we promote our database to production at 2 p.m. We have a full backup scheduled at midnight, and we have scheduled transaction log backups to occur every 30 minutes. This means that after the promotion there will be 19 transaction log backups before the next full backup.

Let’s see this in action. The script in the following listing creates a new database called `PromotionDB`, with the `SIMPLE` recovery model, and then takes a backup of the database.

Listing 12.8 Creating a database and taking a full backup

```sql
CREATE DATABASE PromotionDB ;                            ①
GO

ALTER DATABASE PromotionDB SET RECOVERY SIMPLE ;         ②
GO

USE PromotionDB ;
GO

CREATE TABLE dbo.Incidental (
    ID    INT
) ;
GO

BACKUP DATABASE PromotionDB                              ③
TO  DISK = 'D:\Backups\PromotionDBSimpleBackup.bak' ;    ③
GO
```

① Creates the database

② Ensures SIMPLE recovery model is set

③ Performs a full backup of the database

Now let’s imagine that it’s time to promote the database. The script in listing 12.9 changes the recovery model to `FULL`. It then simulates user activity by inserting some data into the table.

Listing 12.9 Promoting the database

```sql
ALTER DATABASE PromotionDB SET RECOVERY FULL ;
GO

INSERT INTO dbo.Incidental
SELECT object_id
FROM sys.all_objects ;
```

Next, we can simulate our SQL Agent job, which runs the transaction log backups, by using the command in the following listing.

Listing 12.10 Backing up the transaction log

```sql
BACKUP LOG PromotionDB
TO DISK = 'D:\Backups\PromotionDBLogBackupInFull.trn' ;
```

Oh dear! Running this command to back up the transaction log fails with the following error:

```sql
Msg 4214, Level 16, State 1, Line 1
BACKUP LOG cannot be performed because there is no current database backup.
Msg 3013, Level 16, State 1, Line 1
BACKUP LOG is terminating abnormally.
```

But we clearly took a full database backup in listing 12.8. So what is the problem?

The trouble comes from a little-known fact: after changing a database from `SIMPLE` recovery model to `FULL` recovery model, we must initialize the new recovery model with a full backup. If we do not, then effectively the database remains in `SIMPLE` recovery model until a full backup has been taken.

Therefore, in our example, there will be 19 failed transaction log backups before the next full database backup. When the scheduled full backup runs at midnight, then the issue will sort itself out and the subsequent log backups will succeed. This does not help in the meantime, however. If we are unlucky enough to have an issue on the first day and need to perform a restore, we will not be able to.

So what should we do differently? Well, it’s fairly simple. We just need to ensure that when we change the recovery model of the database, we also perform a full backup of the database. We should ensure that this is part of our deployment process when we are promoting a database.

## 12.9 #86 Scheduling log backups immediately after a full backup

I have seen, on more than one occasion, an accidental DBA, knowing that they need to take transaction log backups to enable point-in-time restores, create a SQL Agent job to perform backups on the schedule depicted in figure 12.8. This job will take a full database backup and then immediately take a transaction log backup.

Creating a backup schedule like this is a mistake, but let’s explore why. The reason the DBA has scheduled a transaction log backup is to enable point-in-time recovery. Technically, the solution works. The transaction log holds all transactions since the last transaction log backup.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH12_F08_Carter.png)<br>
**Figure 12.8 Mistaken backup schedule**

Imagine that this SQL Agent job is scheduled to run at midnight every day. If there is an incident at 10 a.m. in which someone accidentally deletes some data and we are tasked with restoring the database to 9:55 a.m., then we can achieve this. We would take another transaction log backup and then restore the data to the desired point in time.

The problem occurs if there is a catastrophic failure. At 10 a.m., the disk array that the database is stored on fails, and we need to restore the database to a new server. In this scenario, we are unable to take a transaction log backup. Our only option is to restore the full backup. In this scenario, the log backup that we took immediately after the full database backup is useless.

What we should do instead is schedule our transaction log backups in line with the required RPO of the database. If the maximum acceptable data loss for a database is 1 hour, we should schedule our log backups to happen once per hour. While scheduling a log backup immediately after a full database backup will technically allow for point-in-time recovery, it does nothing to help achieve the required RPO.

## 12.10 #87 Not using COPY_ONLY backups for ad hoc backups

There are occasions when we will need to take a database backup that is outside of our usual backup schedule. This is often to provide a restore point immediately prior to a change or because we want to refresh a development environment with the latest cut of data from production. If we take a normal full backup, however, we can cause issues with our restore sequence. Imagine that we have the following backups scheduled:

* 1 a.m.—Full backup
* 6 a.m.—Differential backup
* 12 p.m.—Differential backup
* 6 p.m.—Differential backup
* Transaction log backups every hour

Under normal operations, in the event of a failure occurring at 1:30 p.m., our restore sequence would be as follows:

* Restore full backup from 1 a.m.
* Restore differential backup from 12 p.m.
* Restore transaction log backup from 1 p.m.

Now imagine that we performed a change at 10 a.m. and, as part of this change, we took a full database backup as our rollback strategy. The change was successful, so we discarded the full backup.

The issue is that, when we took the full backup at 10 a.m., this became the differential base for the differential backups that are taken at 12 p.m. and 6 p.m., and because we discarded it we are no longer able to restore these differential backups.

Therefore, the restore sequence in our scenario becomes

* Restore full backup from 1 a.m.
* Restore differential from 6 a.m.
* Restore seven transaction log backups, from 7 a.m. to 1 p.m.

If we are unaware that a backup was taken at 10 a.m., then we are likely to attempt the standard restore sequence and waste valuable time trying to debug why we cannot restore that differential from 12 p.m.

What should we do differently? Backups in SQL Server have a feature called `COPY_ONLY`. This is designed specifically for the use case where we need to take an ad hoc backup outside of our normal schedule. Specifically, it will not impact the differential base and therefore not impact our restore sequence.

The command in the following listing demonstrates how to take a `COPY_ONLY` backup of the `Marketing` database.

Listing 12.11 Taking a `COPY_ONLY` backup

```sql
BACKUP DATABASE Marketing
TO  DISK = 'D:\Backups\MarketingFullCopyOnly.bak'
WITH COPY_ONLY ;
```

If we take differential backups, then ad hoc backups should be taken using the `COPY_ONLY` option to avoid affecting the differential base and changing our restore sequence. Personally, as a good practice, I always use `COPY_ONLY` for ad hoc backups, even where no differentials are taken. There is no negative impact of doing so, and it means that I don’t have to worry about checking the backup schedules.

## 12.11 #88 Forgetting that backups are part of our security posture

We usually consider backups from the perspective of data recovery. Unfortunately, however, we live in a world where we must also consider security risks, such as ransomware attacks. Backups are a key aspect in protecting our organizations against this form of attack but only if we consider it when we are planning our backup strategy. So what might happen if we do not take ransomware into consideration when planning our backup strategy?

Imagine that we take backups using SQL Server’s native backup functionality and store them on a file share. An attacker strikes, and we suddenly find that our databases have been deleted. We attempt to restore them from the backups, only to find that the file share has been encrypted and we cannot access it. At best, this will be very costly. At worst, there will be regulatory issues and brand damage. At this point, it will become clear that our backup strategy was a mistake.

To avoid the attack being successful, we need to consider providing an air gap for our backups. If we take backups using an enterprise backup tool, then depending on our environment, it is likely that our backups will already be offloaded to a tape robot, where tapes can be ejected and are therefore not subject to attack, or that we will use a backup provider’s cloud-based ransomware protection. This usually involves offloading the backups to cloud storage, which is owned and managed by the backup vendor and therefore is not subject to the attack either.

If we take our backups using SQL Server native backup, however, then we must take responsibility for ensuring that our backups are protected. The best way to achieve this is to offload our backups to cloud-native storage where we can implement storage, lifecycle policies. For example, Azure Blob Storage provides us with time-based retention policies, which include container-level write once, read many storage, which will prevent blobs within the container from being altered for the duration of the retention period.

As an additional security method, we should ensure our database backups are encrypted. To encrypt our backups, we must first ensure there is a database master key in the `master` database. We must also ensure there is a certificate created in the `master` database that can be used to encrypt our backups. The script in the following listing demonstrates how to create the prerequisite cryptographic objects in the `master` database.

Listing 12.12 Creating cryptographic objects required to encrypt backups

```sql
USE master ;
GO

CREATE MASTER KEY                                      ①
    ENCRYPTION BY PASSWORD = 'Pa$$w0rd' ;
GO

CREATE CERTIFICATE CertForBackupEncryption             ②
   WITH SUBJECT = 'Backup Encryption Certificate' ;
GO
```

① Creates a database master key

② Creates the certificate

Before we use our certificate, it is very important that we back it up to a safe location. If we lose our certificate, there will be no way to recover our encrypted backups. Essentially, we will be performing a ransomware attack on ourselves! The command in the following listing demonstrates how to back up a certificate.

Listing 12.13 Backing up a certificate

```sql
BACKUP CERTIFICATE CertForBackupEncryption
TO FILE = 'c:\Certs\CertForBackupEncryptionCert'
WITH PRIVATE KEY (
    FILE = 'c:\certs\CertForBackupEncryptionPK' ,
    ENCRYPTION BY PASSWORD = 'Pa$$w0rd'
) ;
```

With these objects in place, we can now use the command in listing 12.14 to perform an encrypted backup. Using the `WITH ENCRYPTION` option, we specify the desired encryption algorithm (in our scenario, that is AES 256) and the certificate that should be used to encrypt the data.

Listing 12.14 Performing an encrypted backup

```sql
BACKUP DATABASE Marketing
TO  DISK = 'D:\Backups\MarketingFull256.bak'
WITH ENCRYPTION (
      ALGORITHM = AES_256
    , SERVER CERTIFICATE = CertForBackupEncryption
) ;
```

A full list of supported encryption algorithms in SQL Server 2022 is as follows:

* AES 128
* AES 192
* AES 256
* Triple DES

> [!TIP]
>
> We should be mindful that the longer the algorithm, the higher the performance impact.

When we consider backups, we should consider security as well as recovering from disaster. We should ensure that our backups are stored on immutable storage; ideally this storage will be disconnected from our network. We should also limit exposure by ensuring that our backups are encrypted.

## Summary

* Point-in-time restore refers to restoring a database to a specific point in time in the middle of a transaction log backup.
* A restore chain refers to the sequence of backup files that must be restored.
* RPO defines the acceptable amount of data loss in the event of a disaster.
* RTO defines the acceptable duration of downtime in the event of a disaster.
* When planning your backup strategy, ensure that you consider both the RPO and RTO and define your strategy accordingly.
* Database snapshots use copy-on-write technology to provide a point-in-time snapshot of a database.
* A database snapshot can complement a backup strategy, but it cannot replace it because the snapshot relies on data pages in the source database.
* Storage snapshots back up data at the block level and provide a means of quickly recovering a volume.
* If storage snapshots are used with SQL Server, it is important that application-consistent snapshots are used.
* Crash-consistent snapshots can cause database corruption, as they do not use VSS Writer to flush pending I/O to disk.
* It is important to validate that backups have been successful. You can do this using `RESTORE WITH VERIFY ONLY`.
* Avoid taking backups during an ETL window because not all ETL processes are rerunnable. This can lead to the backups being worthless.
* `FULL` recovery model allows for transaction log files to be backed up.
* In `FULL` recovery model, the log is only truncated during the transaction log backup process.
* `SIMPLE` recovery model only allows for full and differential backups to be taken.
* In `SIMPLE` recovery model, the transaction log is automatically truncated when a checkpoint occurs.
* A checkpoint flushes data that has been modified in memory to the disk.
* You do not always need to use a `FULL` recovery model for nonproduction databases or data warehouses, even when they are in production. The required RPO may not justify it.
* In general, use `FULL` recovery model for OLTP-style databases in production. Not doing so can impact the RPO and stop you from using high-availability features.
* After changing the recovery model of a database, always perform a full backup. If you change a database from `SIMPLE` recovery model to `FULL` recovery model, the change will not take effect until a full database backup has been taken.
* Do not schedule a single transaction log backup to happen after a full database backup. While this will allow for point-in-time recovery, it will not help achieve database RPO requirements.
* If you need to take an ad hoc backup outside of your routine backup schedule, use the `WITH COPY_ONLY` option. This will avoid impacting the differential base and therefore altering your restore sequence.
* Remember that backups play an important role in security. Specifically, they can help protect against ransomware attacks.
* Ensure that your backups are either air gapped or that they reside on immutable storage with an appropriate retention policy.
* Ensure that your backups are encrypted to offer an additional layer of protection against cyberattacks.
