# 4 Database design

This chapter covers

* Design mistakes in SQL Server and why it is important to avoid them
* The mistake of failing to normalize your database
* Mistakes made when designing and creating keys

In this chapter, we will discuss common mistakes that are made when designing databases. These mistakes can cause multiple challenges, leading to issues such as poorly performing code.

Design mistakes are introduced at the earliest stage of the development lifecycle before any code has been written. To illustrate this, let’s focus on MagicChoc. The company feels that its processes are too fragmented, so it has commissioned a new application, which will combine its non-internet sales and procurement functions into a single interface with a single backend. For this purpose, the MagicChoc management team has stated that they want to store the following data items:

|  |  |
| --- | --- |
| * Sales Order Date * Sales Order Number * Sales Person Name * Sales Person Email * Sales Area Name * Sales Area Manager * Customer Company Name * Customer Contact Name * Customer Contact Email * Customer Invoice Address * Customer Delivery Addresses * Sales Order Delivery Due Date * Sales Order Delivery Actual Date * Sales Order Item * Sales Order Quantities * Currier Used for Delivery * Product Name * Product Stock Level | * Product Next Manufacture Date * Product Next Manufacture Quantity * Back Order Manufacturing ID * Product Type Name * Product Type Description * Product Category Name * Product Category Description * Product Subcategory Name * Product Subcategory Description * Supplier Name * Supplier Contact Name * Supplier Contact Email * Supplier Address * Purchase Order Date * Purchase Order Number * Purchase Order Items * Purchase Order Quantities |

In this chapter, we will design the table structure that we need to store this data. Along the way, we will explore how to avoid common design mistakes. Specifically, we will look at the issues that can arise if we fail to normalize our database. We will then look at the performance problems that can be associated with a poor choice of primary key. Finally, we will explore the consequences of not creating foreign key constraints.

## 4.1 #11 Failing to normalize

Many times, I have asked developers, “Is the database normalized?” The answer invariably is “Yes!” Unfortunately, on many occasions, the developers were relatively inexperienced in database design and misunderstood my question to mean “Have you designed an Online Transaction Processing (OLTP) database as opposed to a data warehouse?” In reality, instead of normalizing their database, they have just used their own judgment to decide on the database schema.

To design our database schema, we will create an Entity Relationship Diagram (ERD), which is a diagram that describes the *entities*, which are objects within a data model, and *attributes,* which are used to describe the data object. An ERD also describes how objects relate to each other within the model.

> [!TIP]
>
> An ERD is often the code-level diagram within a C4 model when C4 is being used to document the design of a database. Therefore, in this section, we will implicitly build a C4 code diagram.

In the following sections, we will first explore the wrong way to design a database schema, which is to use the judgment approach. In this approach, we will use nothing but our experience to organize the data. Because everybody’s experience is not only at different levels but also simply different, there are multiple issues that can arise when we use this unstructured approach. We will examine some of the more common issues that can occur, such as nonatomic values and data duplication.

Finally, we will explore how to avoid our mistakes by using normalization to design our schema. We will explore this structured approach to database modeling, which was first developed by Edgar Codd in the 1970s and has really stood the test of time. It is a set of methodical steps that is used to remove redundancy from our data. When using this approach, we will follow a strict set of rules, which are aimed at avoiding data duplication and making the schema as efficient as possible. We will also touch on how we can test our normalization by using an Entity Relationship Diagram and look at the concept of data generalization.

### 4.1.1 Designing a schema using judgment

Let’s use our judgment to design the schema of the new sales and procurement database that MagicChoc has requested. It wants to model 35 data items, so let’s make things a bit easier and break them down into sections. Let’s start with the Sales Order data—specifically, the following data items:

|  |  |
| --- | --- |
| * Sales Order Date * Sales Order Number * Sales Person Name * Sales Person Email * Sales Area Name * Sales Area Manager | * Sales Order Delivery Due Date * Sales Order Delivery Actual Date * Sales Order Item * Sales Order Quantities * Currier Used for Delivery |

The Sales Order Number attribute will be unique, so this looks like a good contender as a key for our entity. The troublesome aspect is the relationship between Sales Order Item and Sales Order Quantities. We notice that there is an obvious *one-to-many relationship*. This is a relationship between entities where a single record in one entity can be associated with multiple entities in another. In this instance, there could be more than one product ordered on any given order, so we decide to split the items ordered into a different entity. We also think that, as we have a requirement to store general product details, it would make sense to leave out the product details and just key to the Products entity, which we will design shortly.

Keys

Relational databases join tables together using the concept of keys. There are two basic types of keys—a primary key and a foreign key. A primary key is used to uniquely identify a row in a table. A foreign key is used to point to a primary key value or a unique constraint from a secondary table.

For example, imagine a table of employees. You need to record the office where each employee works. You could add an `Office` column to the `Employees` table, but that would mean duplicating the same handful of office names (and possibly addresses, phone numbers, etc.) over and over again. So if you move the office details out into an `Offices` table, you can use a primary key, let’s say `OfficeID`, to uniquely identify each office. Then, you can use a foreign key in the `Employees` table (often, but not always, with the same name as the primary key). This foreign key will refer to the values in the primary key column of the `Offices` table to ensure that only values are entered into the foreign key, which exists in the primary key. This enforces what is known as *referential integrity*.

If a key has a business meaning, such as `SocialSecurityNumber`, then it is known as a *natural key*. If a key does not have business meaning, such as an arbitrary, incrementing number, like `EmployeeID`, then it is called an *artificial key*.

A natural key may consist of multiple columns. If a key consists of multiple columns, it is called a *composite key*. It’s best to minimize the use of composite keys when possible.

Similarly, as we assume customers may place multiple orders, and we know we need to store customer details as part of the requirements, we decide we should create a key in the `Orders` table that will join the `Customers` table when we create it. Finally, we notice that there is not a good candidate for a primary key, so we will create an artificial key. This leaves us with the beginning of an ERD, which can be seen in figure 4.1.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F01_Carter.png)<br>
**Figure 4.1 ERD for Sales Orders**

Next, we will look at the requirements around Product data, where we need to store the following data:

|  |  |
| --- | --- |
| * Product Name * Product Stock Level * Product Next Manufacture Date * Product Next Manufacture Quantity * Backorder Manufacturing ID * Product Type Name | * Product Type Description * Product Category Name * Product Category Description * Product Subcategory Name * Product Subcategory Description |

We decide to have a table for Products, and as there is no obvious natural key, we will create an artificial key, which will link to the ProductID attribute in the Sales Order Details entity. We also notice that there could be a one-to-many relationship between products and product categories, so we decide to create a separate Product Categories entity, which we will join to the Products entity with another artificial key. If we combine these entities with the Sales Orders entities, we will have the ERD shown in figure 4.2.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F02_Carter.png)<br>
**Figure 4.2 ERD with Products added**

Next, we will build the Customers entity, which we know will have the following attributes:

|  |  |
| --- | --- |
| * Customer Company Name * Customer Contact Name * Customer Contact Email | * Customer Invoice Address * Customer Delivery Addresses |

To avoid a wide, sparse table, we decide to move addresses into a different entity. We also realize that we can have a single Addresses entity that keys to both Customer Invoice Address and Customer Delivery Address. This avoids duplicating data if the invoice address is the same as the delivery address. We achieve this by adding flags for each address type. We will create artificial keys for both entities. Appending these entities to our existing design gives us the ERD shown in figure 4.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F03_Carter.png)<br>
**Figure 4.3 ERD with Customers added**

Finally, let’s add the entities for Suppliers and Purchase Orders. The data attributes that we need to model for these entities are

* Supplier Name
* Supplier Contact Name
* Supplier Contact Email
* Supplier Address
* Purchase Order Date
* Purchase Order Number
* Purchase Order Items
* Purchase Order Quantities

We decide that we will model our Suppliers and Purchase Orders in a way similar to how we have modeled our Customers and Sales Orders. We will have a Suppliers entity, which will link to our existing Addresses entity to store supplier addresses. We will split our purchase orders into two entities: one for Purchase Order Headers and the other for Purchase Order Details. Again, this is to allow us to order multiple items in the same order without having to duplicate the order data for each item. We decide to create an artificial key for Suppliers and Purchase Order Details but to use the Purchase Order Number as a natural key in the Purchase Order Headers entity.

If we add these remaining entities to our existing design, then we will have the ERD shown in figure 4.4. So, there, we have our completed design for a database schema. That wasn’t so hard, was it? But what is the problem with using this approach? The problem is that there are multiple mistakes in this schema design that will make our database very hard to work with and potentially cause performance problems. In the next section, we will discuss some of the mistakes and their consequences.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F04_Carter.png)<br>
**Figure 4.4 ERD with Suppliers and Purchase Orders**

### 4.1.2 Problems with our database schema

At first glance, our database schema may look fairly sensible, but if we take a closer look at some of our design choices, we will see where problems will occur. Let’s start by looking at customer and supplier names, which is one of the easiest issues to spot.

Both the `SupplierName` attribute of the Suppliers entity and the `CustomerName` attribute of the Customers entity will store first names and last names. This can cause various issues. Imagine that our sales application has mail merge functionality, and we wish to start letters to customers with “Dear Robert,” where Robert is the first name of a customer. To retrieve this first name, we would need to run a query such as

```sql
SELECT SUBSTRING(CustomerName, 0, CHARINDEX(' ', CustomerName, 1))
FROM Customers ;
```

This not only makes our code more complex to read and write but is also less efficient than simply selecting a `FirstName` column from a table. Therefore, we should always make sure our values are atomic. An *atomic value* is the one that cannot be broken down further and still retain its meaning. For example, the name Peter A. Carter could be broken down into three atomic values—first name, middle initial, and last name. The argument that it could be broken down into 12 characters doesn’t hold because these values would have no contextual meaning.

> [!TIP]
>
> The exception to this rule is when, after normalizing a schema, you intentionally make the decision to denormalize your data. For example, you may decide to store your address data as JSON, as discussed in chapter 3. Other reasons to avoid normalization include where we are working with analytical data or creating staging tables.

Another problem lies in our `ProductCategories` entity. This entity consists of both product categories and subcategories. But there are likely to be multiple subcategories within a category. This means that we will have to duplicate the `ProductCategoryName` and `ProductCategoryDescription` for each product subcategory. Duplicating the data wastes space, which will make the table bigger and hence slower to read, as well as taking up more memory and making index operations less efficient. The larger problem, however, is with inserts and updates. These will become much less efficient as we will need to update product category details on multiple rows. We could have avoided this by breaking the data down into more entities and joining them with a primary/foreign key relationship.

We have decided to store `SalesAreaName` in the `SalesOrderHeader` entity. This means that the name will be duplicated multiple times, as it can apply to multiple rows. It also means that our Sales Area Manager has to be repeated for every record. Again, if sales area details had been broken into a different entity and joined back with a primary/foreign key relationship, then this could have been avoided.

We have a similar issue with the `SupplierContactName`. Not only is the value not atomic, but depending on the business rules, we could also end up duplicating the data. What if two of our suppliers were owned by the same person? Also, what if we have future requirements to store additional contact information, such as a contact-specific phone number? This will just compound the issue, as well as make our suppliers entity very wide.

This leaves us with many questions. How could we have avoided these design problems? Are there any more issues with our design? How can we even test it to find out, without building the entire schema and populating it with data?

The answer to all these questions is to use the process of normalization to design our data schema. We will discuss how to use this process in the following section.

> [!NOTE]
>
> An additional issue we would not be able to spot without an understanding of the business data and business rules is that the sales order numbers will be in the format ABC1234D-E12. This will give us a very wide primary key. This topic will be discussed in a later section of this chapter.

### 4.1.3 Designing a database schema using normalization

As mentioned earlier in the chapter, normalization is a formal data modeling process that was first created by Edgar Codd in the 1970s and has really stood the test of time. There are 10 normal forms, or steps, in the process, which have been added to the methodology over time. The most recent addition of Essential Tuple Normal Form was added as recently as 2012. Specifically, the normal forms are

* First Normal Form (1NF)
* Second Normal Form (2NF)
* Third Normal Form (3NF)
* Elementary Key Normal Form (EKNF)
* Boyce-Codd Normal Form (BCNF or 3.5NF)
* Fourth Normal Form (4NF)
* Essential Tuple Normal Form (ETNF)
* Fifth Normal Form (5NF)
* Domain-key Normal Form (DKNF)
* Sixth Normal Form (6NF)

In the vast majority of cases, there is no need to go past 3NF. It is also important to note that other problems can arise if we overnormalize a schema. Therefore, this chapter will focus on modeling our data schema into 3NF.

To model data into 1NF, we will be ensuring that all attributes are atomic and uniquely named and that their order is irrelevant. We will also ensure that repeating groups of attributes are moved into a different entity and that the order of rows is not important.

When we model data into 2NF, we will ensure that all attributes are dependent on all attributes that constitute a composite primary key. If they don’t, we will move them into a new entity. Finally, when we model the data into 3NF, we will ensure that attributes are not dependent on attributes that are not currently part of the primary key. If they are, then again, this is a sign that they should be moved to a new entity.

Normalization tips

While some data normalization tools are available, I honestly find that the best tool for the process is a simple spreadsheet. Personally, for smaller datasets, such as the one we are discussing here, I use a column per normal form, with row gaps between entities. This allows me to compare my models side by side and think through the logic that I am using.

Another important tip for normalization is to understand your data. This means working with the business analyst, or the application owner, to understand the business meaning and business rules behind each attribute. Failure to do this will inevitably result in false assumptions, which affect your model.

Ensuring you have this understanding will help you, or your business analyst, create another important document called a *data dictionary*. Again, this is often created within a spreadsheet and details information such as

* Attribute name
* Attribute description/meaning
* Business rules, such as expected values, uniqueness, etc.
* Example data
* Data type
* Upstream dependencies
* Data lineage

*Data lineage* is important in complex data-tier applications. It tracks the path that a data item has taken to reach the table that it currently resides in. For example, if we had a database used for internet marketing, you may be storing a cookie ID. The lineage here could be something like

1. Tracking ID (in Impressions CSV delivered from cookie provider) >
2. TrackingID (in `Cookies` table in staging schema) >
3. CookieID (In `Cookies` table marketing schema)

Finally, don’t be afraid to make mistakes. Build your model, test it, and, if you have made a mistake, then simply fix it, rinse, and repeat.

Before we begin, we will arrange our attributes into three broad data types: Sales, Purchasing, and Products, which relate to both Sales and Purchasing. This gives us our starting data in an unnormalized form, as shown in the following:

```sql
Sales
Sales Order Date
Sales Order Number
Sales Person Name
Sales Person Email
Sales Area Name
Sales Area Manager
Customer Company Name
Customer Contact Name
Customer Contact Email
Customer Invoice Address
Customer Delivery Addresses
Sales Order Delivery Due Date
Sales Order Delivery Actual Date
Sales Order Item
Sales Order Quantities
Currier Used for Delivery

Purchasing
Supplier Name
Supplier Contact Name
Supplier Contact Email
Supplier Address
Purchase Order Date
Purchase Order Number
Purchase Order Items
Purchase Order Quantities

Products
Product Name
Product Stock Level
Product Next Manufacture Date
Product Next Manufacture Quantity
Back Order Manufacturing ID
Product Type Name
Product Type Description
Product Category Name
Product Category Description
Product Subcategory Name
Product Subcategory Description
```

First Normal Form

For a relation to be in 1NF, it must meet the following rules:

* All attributes are atomic.
* Every attribute has a unique name.
* The order of attributes is irrelevant.
* Repeating groups are removed.
* All rows must be unique (there must be a key).

NOTE In normalization, a relation is the equivalent of an entity in an ERD or a table in a database.

In our example, every attribute already has a unique name, and the order of attributes is irrelevant, so we are off to a good start. We do, however, need to break down some of our attributes to be atomic. We also need to check our data for repeating groups, which are groups of columns that repeat. We also need to find a *candidate key*, which is a set of columns that form a *minimal super key*. A super key is a key that uniquely identifies every row, and it is minimal if the removal of any column in that key would result in an inability to uniquely identify every row.

First, let’s identify keys that will be used to uniquely identify rows in each relation. The Sales data can be uniquely identified by the Sales Order Number. Therefore, there is no need to use a *composite key*, which is a key consisting of multiple attributes. If a key consists of a single attribute, then it is known as a *prime attribute*.

The Purchasing data can be uniquely identified by the Purchase Order Number. Therefore, it is a super key in its own right, and we can again use this as a single attribute candidate key.

The Products data cannot be identified by the Product Name alone, as some parts ordered from suppliers have the same name as the products that are sold to customers. Therefore, we need to use both the Product Name and the Product Type Name to uniquely identify any given record, and these two attributes will therefore form our candidate key.

This process has already helped us to spot the first issue with our data model. Both our Sales data and our Purchasing data hold product names (Sales Order Items and Purchase Order Items, respectively). The issue is that we have found that we require both Product Name and Product Type Name to uniquely identify a Product. Therefore, let’s add Product Type Name to each of those entities. We should also add relationships to each relation that joins to the Products relation. While we are there, let’s change the names of the Sales Order Items attribute and the Purchase Order Items attribute to be Product Name, to make the relationship clear.

Next, we should look for nonatomic values. The nonatomic values in our unnormalized data, which we need to break down, are

* *Sales Person Name*—We will break this down to Sales Person First Name and Sales Person Last Name.
* *Sales Area Manager*—We will break this down to Sales Area Manager First Name and Sales Area Manager Last Name.
* *Customer Contact Name*—We will break this down to Customer Contact First Name and Customer Contact Last Name.
* *Customer Invoice Address*—We will break this down into the following attributes:

  + Invoice Address Street
  + Invoice Address Area
  + Invoice Address City
  + Invoice Address Zip Code
  + Invoice Address Country
* *Customer Delivery Address*—We will break this down into the following attributes:

  + Delivery Address Street
  + Delivery Address Area
  + Delivery Address City
  + Delivery Address Zip Code
  + Delivery Address Country
* *Product Name and Product Type Name (in the Sales relation)*—This is a tricky one. This attribute will store multiple products that have been ordered on a single order, but we can’t really break this down into multiple attributes, as there is not a defined number of items that can be placed on a single order. Therefore, we will separate this out into a different relation and carry down the Sales Order Number, which we have selected as the key, so that the new relation can join back to the original relation. We will also bring with it the Sales Order Quantities (which we will shorten to Quantity), which would otherwise suffer from the same issue.
* *Supplier Contact Name*—We will break this down into Supplier Contact First Name and Supplier Contact Last Name.
* *Supplier Address*—We will break this down into the following attributes:

  + Supplier Address Street
  + Supplier Address Area
  + Supplier Address City
  + Supplier Address Zip Code
  + Supplier Address Country
* *Product Name and Product Type Name (in the Purchasing relation)*—As with the Sales relation, we will need to move these attributes out to a new relation and carry across the Purchase Order Number so that we can join the entities. Again, we will bring the Purchase Order Quantities attribute and shorten it to Quantity.

Finally, we need to look for repeating groups. You may have already noticed from the bulleted list that the attributes that make up the Invoice Address are an exact repeat of the attributes that make up the Delivery Address within the Sales relation. Therefore, we should move both of these addresses out into a new relation, which we will call Addresses. We will add an additional attribute to the new relation, for which the candidate key will be Street and Zip Code. We will then bring these back into the Sales relation twice, once for invoice address and again for delivery address.

> [!TIP]
>
> While it’s not explicitly called for within the rules of normalization, at this point it would make sense to add Supplier Address to the Addresses relation as well. We will then have a single relation that stores all addresses in our database. This minimizes duplication and, in the specific case of address, means that a single address validation leaves it ready to be consumed by any entity.

Data generalization

By adding all three address types to a single relation, we have performed a data modeling technique called generalization, where we pull together attributes from a set of relations or entities and create a new entity that is more generic.

A more advanced type of *generalization* would be to create supertypes and subtypes. With this technique, we may, for example, decide to generalize salespeople, customer contacts, supplier contacts, and sales area managers. All their common attributes, such as First Name, Last Name, and Email, would be moved into a People entity.

Some of the person types may have unique attributes, however. For example, a salesperson may also have attributes for Sales Target and Employee Number. In this case, we could create an additional entity for Sales Persons that would contain the unique attributes and join back to the People entity.

In this instance, People becomes a supertype. Sales Persons is a subtype of the People entity. There is a good example of this modeling within the AdventureWorks database, which is a SQL Server sample database. In this database, the `Employees` table is a subtype of the Person supertype.

The AdventureWorks database can be downloaded from <https://mng.bz/o0od>.

The following structure illustrates how our data is modeled into 1NF, using the previous discussion points:

```sql
    Sales
    Sales Order Date
PK  Sales Order Number
    Sales Person First Name
    Sales Person Last Name
    Sales Person Email
    Sales Area Name
    Sales Area Manager First Name
    Sales Area Manager Last Name
    Customer Company Name
    Customer Contact First Name
    Customer Contact Last Name
    Customer Contact Email
FK  Invoice Address Street
FK  Invoice Address Zip Code
FK  Delivery Address Street
FK  Delivery Address Zip Code
    Sales Order Delivery Due Date
    Sales Order Delivery Actual Date
    Currier Used for Delivery

      Sales Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Sales Order Number

    Address
PK  Street
    Area
    City
PK  Zip Code

    Purchasing
    Supplier Name
    Supplier Contact First Name
    Supplier Contact Last Name
    Supplier Contact Email
FK  Supplier Address Street
FK  Supplier Address Zip Code
    Purchase Order Date
PK  Purchase Order Number

      Purchase Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Purchase Order Number

    Products
PK  Product Name
    Product Stock Level
    Product Next Manufacture Date
    Product Next Manufacture Quantity
    Back Order Manufacturing ID
PK  Product Type Name
    Product Type Description
    Product Category Name
    Product Category Description
    Product Subcategory Name
    Product Subcategory Description
```

So that’s great. Our relations are starting to look a bit more like a database schema. But having a database modeled to 1NF is usually considered a bad design. So let’s move on and start looking at how we can bring our database into 2NF.

Second Normal Form

For relations to be in 2NF, they must comply with the following rules:

* The relations must be in 1NF.
* All attributes must be functionally dependent on the whole of the key.

Okay, so we know that our relations are already in 1NF, but how do we ensure functional dependency on the whole of the key? Well, first it is worth noting that this rule only applies to relations that have a composite key. By definition, if a natural key consists of a prime attribute, then all other attributes must be dependent upon it.

In our example, we only have a single attribute that is not dependent on all attributes within a composite key. This is the Product Type Description attribute within the Product relation. This attribute is solely dependent on the Product Type Name attribute. Therefore, let’s pull the Product Type Name and Product Type Description attributes out into a separate relation, where Product Type Name becomes the prime attribute. We will also push the Product Type Name back up to the Product relation as a foreign key.

Therefore, the following represents our data when modeled in 2NF:

```sql
    Sales
    Sales Order Date
PK  Sales Order Number
    Sales Person First Name
    Sales Person Last Name
    Sales Person Email
    Sales Area Name
    Sales Area Manager First Name
    Sales Area Manager Last Name
    Customer Company Name
    Customer Contact First Name
    Customer Contact Last Name
    Customer Contact Email
FK  Invoice Address Street
FK  Invoice Address Zip Code
FK  Delivery Address Street
FK  Delivery Address Zip Code
    Sales Order Delivery Due Date
    Sales Order Delivery Actual Date
    Currier Used for Delivery

      Sales Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Sales Order Number

    Address
PK  Street
    Area
    City
PK  Zip Code

    Purchasing
    Supplier Name
    Supplier Contact First Name
    Supplier Contact Last Name
    Supplier Contact Email
FK  Supplier Address Street
FK  Supplier Address Zip Code
    Purchase Order Date
PK  Purchase Order Number

      Purchase Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Purchase Order Number

      Products
PK    Product Name
      Product Stock Level
      Product Next Manufacture Date
      Product Next Manufacture Quantity
      Back Order Manufacturing ID
PK FK Product Type Name
      Product Category Name
      Product Category Description
      Product Subcategory Name
      Product Subcategory Description

    Product Types
PK  Product Type Name
    Product Type Description
```

Third Normal Form

Lastly, we need to transform our relations into 3NF. The rules of 3NF state

* Relations must be in 2NF.
* All attributes must be nontransitively dependent on the key.

Our relations are already in 2NF, which is great. But we need to ensure *nontransitive dependence*, which means that attributes are not dependent on any nonprime attributes within the relation.

If we look through our data, we will see that we have quite a few examples of attributes that depend more on nonprime attributes than they do the candidate key. The first example is in the Sales relation: the Sales Person First Name and the Sales Person Last Name. It is reasonable to expect that the Sales Person Email will be unique, which means that it could be used to identify each unique combination of these attributes. Therefore, we will move all three of these attributes into a new Sales Person relation, where Sales Person Email is the prime attribute, and bring the Sales Person Email attribute back up into the Sales relation as a foreign key.

The Sales relation also has Sales Area Manager First Name and Sales Area Manager Last Name. Both of these attributes can be uniquely identified by the Sales Area Name attribute. Therefore, we will bring them down into a Sales Areas relation and retain Sales Area Name as a foreign key.

The Customer Contact First Name, Customer Contact Last Name, Customer Email, along with the delivery address keys and the invoice address keys can all be uniquely identified by the Customer Company Name attribute. Therefore, we will move these out into a Customers relation and join it back on the Customer Company Name key. This is an interesting example, however, as once we have created this new relation, you may notice that we can also now identify the Customer Contact First Name and Customer Contact Last Name by using the Customer Email. Therefore, we should move these attributes out to yet another relation, called Customer Contact, which will join back to the Customer relation using the Customer Contact Email as a key.

In a similar fashion, when we look at the Purchasing relation, we will see that the Supplier Contact First Name, Supplier Contact Last Name, and the Supplier Address keys can all be uniquely identified using the Supplier Name and therefore should move into a Suppliers relation. But the Supplier Contact First Name, Supplier Contact Last Name, and Supplier Contact Email should then move down to a Supplier Contact relation, keyed on the Supplier Contact Email.

> [!TIP]
>
> Supplier Contacts, Customer Contacts, and Sales Person would be good candidates for generalization. If we wanted to generalize this data, we could create a single Contacts entity, which would hold all contact types. To make filtering easier, we could create a Contact Type attribute in the entity. This entity would then join to the Suppliers, Customers, and Sales entities.

In the Products relation, the Product Category Name, Product Category Description, and Product Subcategory description can all be uniquely identified by the Product Subcategory name, so we will move these attributes down into a Product Subcategories relation. Furthermore, we should then move the Product Category Description down into a Product Categories relation, which is keyed on the Product Category Name attribute.

Finally, the `ProductNextManufactureDate` and `ProductNextManufactureQuantity` attributes can be identified by the `BackOrderManufacturingID` attribute, so we will move these attributes down to a new Back Orders relation.

We should now find that our data is in 3NF, as shown in the following:

```sql
    Sales
    Sales Order Date
PK  Sales Order Number
FK  Sales Person Email
FK  Sales Area Name
FK  Customer Company Name
    Sales Order Delivery Due Date
    Sales Order Delivery Actual Date
    Currier Used for Delivery

    Sales Person
    Sales Person First Name
    Sales Person Last Name
PK  Sales Person Email

    Sales Areas
PK  Sales Area Name
    Sales Area Manager First Name
    Sales Area Manager Last Name

    Customer
PK  Customer Company Name
FK  Customer Contact Email
FK  Invoice Address Street
FK  Invoice Address Zip Code
FK  Delivery Address Street
FK  Delivery Address Zip Code

    Customer Contact
    Customer Contact First Name
    Customer Contact Last Name
PK  Customer Contact Email

      Sales Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Sales Order Number

    Address
PK  Street
    Area
    City
PK  Zip Code

    Purchasing
FK  Supplier Name
    Purchase Order Date
PK  Purchase Order Number

    Suppliers
PK  Supplier Name
FK  Supplier Contact Email
FK  Supplier Address Street
FK  Supplier Address Zip Code

    Supplier Contact
    Supplier Contact First Name
    Supplier Contact Last Name
PK  Supplier Contact Email

      Purchase Order Details
PK FK Product Name
PK FK Product Type Name
      Quantity
PK FK Purchase Order Number

      Products
PK    Product Name
      Product Stock Level
FK    Back Order Manufacturing ID
PK FK Product Type Name
FK    Product Subcategory Name

    Product Types
PK  Product Type Name
    Product Type Description

    Product SubCategories
FK  Product Category Name
PK  Product Subcategory Name
    Product Subcategory Description

    Product Categories
PK  Product Category Name
    Product Category Description

    BackOrders
    ProductNextManufactureDate
    ProductNextManufactureQuantity
PK    BackOrderManufacturingID
```

We should now take the time to tidy up a few things. First is our relation names. After the modeling, Sales is no longer a meaningful name for the data it represents. Let’s change this to Sales Order Header. Also, Purchasing no longer adequately represents its attributes, so we should change it to Purchase Order Header.

Wide primary keys are bad. I won’t go into too much detail here, as we will be discussing this in the next mistake, but currently all of our keys are *natural keys*, which are keys that have business meaning. Where we have composite keys, we should replace these with *artificial keys*, which are keys that store an arbitrary value with no business meaning—usually an incrementing number. We should also replace keys that are text-based with artificial keys. Again, this makes the keys narrower.

With this in mind, we will make the following changes:

* Sales Person will have a new key called Sales Person ID.
* Sales Areas will have a new key called Sales Area ID.
* Customers will have a new key called Customer ID.
* Customer Contacts will have a new key called Customer Contact ID.
* Sales Order Details will have a new key called Sales Order Details ID.
* Addresses will have a new key called Address ID.
* Suppliers will have a new key called Supplier ID.
* Supplier Contacts will have a new key called Supplier Contact ID.
* Purchase Order Details will have a new key called Purchase Order Details ID.
* Products will have a new key called Product ID.
* Product Types will have a new key called Product Type ID.
* Product Subcategories will have a new key called Product Subcategory ID.
* Product Categories will have a new key called Product Category ID.

Finally, we should remove all spaces from our relation and attribute names. This is to make them database friendly when we create our database tables and columns. If we had any special characters or reserved words, which are words that are used by SQL Server, such as “SELECT” or “table,” in our relation and attribute names, we should consider changing these as well. If we did not take this step, then we would have to use delimited identifiers when we referenced the objects in SQL Server. This means encapsulating the name in square brackets. For example, `SELECT MyColumn FROM MyTable` would become `SELECT [MyColumn] FROM [MyTable]` Delimited identifiers are considered poor practice because they clutter our code and because, if we need to use them, we have not followed the standard rules and conventions for naming identifiers.

We have now finished our design, but how can we test it? There is no hard and fast answer to this, but we will get a good idea if we create an ERD. When examining the ERD, we should notice that all entities that need to be joined are joined with a 1:many join type. If we find any many:many or 1:1 joins, then there is likely a mistake in the model. So let’s see how our model looks if we build an ERD. This ERD can be seen in figure 4.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F05_Carter.png)<br>
**Figure 4.5 ERD from normalized design**

Our final task is to write a script that will create our database objects. The script in listing 4.1 will generate these objects, some of which we will use in later mistakes.

NOTE There is an intentional mistake in the script. The `PurchaseOrderDetails` table has a `PurchaseOrderNumber` column, but a foreign key has not been created, to join it back to the `PurchaseOrderHeaders` table. We will explore the consequences of this in a later section of this chapter.

Listing 4.1 Creating the database objects

```sql
CREATE DATABASE MagicChoc ;
GO

USE MagicChoc ;
GO

CREATE TABLE dbo.BackOrders (
    BackOrderManufacturingID          INT    NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductNextManufactureDate        DATE   NOT NULL,
    ProductNextManufactureQuantity    INT    NOT NULL
) ;

CREATE TABLE dbo.ProductCategories (
    ProductCategoryID             SMALLINT         NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductCategoryName           NVARCHAR(64)     NOT NULL,
    ProductCategoryDescription    NVARCHAR(256)    NULL
) ;

CREATE TABLE dbo.ProductSubcategories (
    ProductSubcategoryID             SMALLINT         NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductCategoryID                SMALLINT         NOT NULL
        REFERENCES dbo.ProductCategories(ProductCategoryID),
    ProductSubcategoryName           NVARCHAR(64)     NOT NULL,
    ProductSubcategoryDescription    NVARCHAR(256)    NULL
) ;

CREATE TABLE dbo.ProductTypes (
    ProductTypeID             SMALLINT        NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductTypeName           NVARCHAR(64)    NOT NULL,
    ProductTypeDescription    NVARCHAR(256)   NOT NULL
) ;

CREATE TABLE dbo.Products (
    ProductID                   INT             NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductName                 NVARCHAR(64)    NOT NULL,
    ProductStockLevel           INT             NOT NULL,
    BackOrderManufacturingID    INT             NULL
        REFERENCES dbo.BackOrders(BackOrderManufacturingID),
    ProductTypeID               SMALLINT        NOT NULL
        REFERENCES dbo.ProductTypes(ProductTypeID),
    ProductSubcategoryID        SMALLINT        NOT NULL
        REFERENCES dbo.ProductSubcategories(ProductSubcategoryID)
) ;

CREATE TABLE dbo.Addresses (
    AddressID    INT              NOT NULL
        IDENTITY    PRIMARY KEY,
    Street       NVARCHAR(128)    NOT NULL,
    Area         NVARCHAR(64)     NULL,
    City         NVARCHAR(64)     NOT NULL,
    ZipCode      NVARCHAR(10)     NOT NULL
) ;

CREATE TABLE dbo.SupplierContacts (
    SupplierContactID           INT              NOT NULL
        IDENTITY    PRIMARY KEY,
    SupplierContactFirstName    NVARCHAR(32)     NOT NULL,
    SupplierContactLastName     NVARCHAR(32)     NOT NULL,
    SupplierContactEmail        NVARCHAR(256)    NOT NULL
) ;

CREATE TABLE dbo.Suppliers (
    SupplierID           INT             NOT NULL
        IDENTITY    PRIMARY KEY,
    SupplierName         NVARCHAR(32)    NOT NULL,
    SupplierContactID    INT             NOT NULL
        REFERENCES dbo.SupplierContacts(SupplierContactID),
    SupplierAddressID    INT             NOT NULL
        REFERENCES dbo.Addresses(AddressID)
) ;

CREATE TABLE dbo.PurchaseOrderHeaders (
    PurchaseOrderNumber    INT     NOT NULL    PRIMARY KEY,
    SupplierID             INT     NOT NULL
        REFERENCES dbo.Suppliers(SupplierID),
    PurchaseOrderDate      DATE    NOT NULL
) ;

CREATE TABLE dbo.PurchaseOrderDetails (
    PurchaseOrderDetailsID    INT    NOT NULL
        IDENTITY    PRIMARY KEY,
    ProductID                 INT    NOT NULL
        REFERENCES dbo.Products(ProductID),
    Quantity                  INT    NOT NULL,
    PurchaseOrderNumber       INT    NOT NULL               ①
) ;

CREATE TABLE dbo.CustomerContacts (
    CustomerContactID           INT              NOT NULL
        IDENTITY    PRIMARY KEY,
    CustomerContactFirstName    NVARCHAR(32)     NOT NULL,
    CustomerContactLastName     NVARCHAR(32)     NOT NULL,
    CustomerContactEmail        NVARCHAR(256)    NOT NULL
) ;

CREATE TABLE dbo.Customers (
    CustomerID             INT             NOT NULL
        IDENTITY    PRIMARY KEY,
    CustomerCompanyName    NVARCHAR(32)    NOT NULL,
    CustomerContactID      INT             NOT NULL
        REFERENCES dbo.CustomerContacts(CustomerContactID),
    InvoiceAddressID       INT             NOT NULL
        REFERENCES dbo.Addresses(AddressID),
    DeliveryAddressID      INT             NOT NULL
        REFERENCES dbo.Addresses(AddressID)
) ;

CREATE TABLE dbo.SalesAreas (
    SalesAreaID                  SMALLINT        NOT NULL
        IDENTITY    PRIMARY KEY,
    SalesAreaName                NVARCHAR(32)    NOT NULL,
    SalesAreaManagerFirstName    NVARCHAR(32)    NOT NULL,
    SalesAreaManagerLastName     NVARCHAR(32)    NOT NULL
) ;

CREATE TABLE dbo.SalesPersons (
    SalesPersonID           SMALLINT         NOT NULL
        IDENTITY    PRIMARY KEY,
    SalesPersonFirstName    NVARCHAR(32)     NOT NULL,
    SalesPersonLastName     NVARCHAR(32)     NOT NULL,
    SalesPersonEmail        NVARCHAR(256)    NOT NULL
) ;

CREATE TABLE dbo.SalesOrderHeaders (
    SalesOrderNumber               NCHAR(12)       NOT NULL    PRIMARY KEY,
    SalesOrderDate                 DATE            NOT NULL,
    SalesPersonID                  SMALLINT        NOT NULL
        REFERENCES dbo.SalesPersons(SalesPersonID),
    SalesAreaID                    SMALLINT        NOT NULL
        REFERENCES dbo.SalesAreas(SalesAreaID),
    CustomerID                     INT             NOT NULL
        REFERENCES dbo.Customers(CustomerID),
    SalesOrderDeliveryDueDate      DATE            NOT NULL,
    SalesOrderDeliveryActualDate   DATE            NULL,
    CurrierUsedforDelivery         NVARCHAR(32)    NOT NULL
) ;

CREATE TABLE dbo.SalesOrderDetails (
    SalesOrderDetailsID    INT          NOT NULL    IDENTITY    PRIMARY KEY,
    ProductID              INT          NOT NULL
        REFERENCES dbo.Products(ProductID),
    Quantity               INT          NOT NULL,
    SalesOrderNumber       NCHAR(12)    NOT NULL
        REFERENCES dbo.SalesOrderHeaders(SalesOrderNumber)
) ;
GO
```

① This column is missing a foreign key constraint, which should join it back to the PurchaseOrderHeader table.

Modeling data using normalization can seem complex at first, but after a few attempts, it starts to come naturally. It certainly has many benefits over the judgment modeling approach and is far less prone to errors. It allows us to apply a methodology that has well and truly stood the test of time.

## 4.2 #12 Using a wide primary key

As mentioned in the previous section, we have selected `SalesOrderNumber` as the primary key of the `SalesOrderHeaders` table, but the format of the order numbers is ABC1234D-E12. This means that our column is of data type `NCHAR(12)`. This means that for each row, the column uses 24 bytes of space, as opposed to the 4 bytes of space that an `INT` would take. Even a `BIGINT` would only consume 8 bytes of space.

We spoke about the benefits of using the smallest possible data types in chapter 3, but this data has letters in it. Depending on the business rules, we may be able to use a `CHAR(12)`, but even that will consume 12 bytes of storage per row—so what can we do? And what is the significance of it being a primary key column?

To answer these questions, let’s think about our indexing strategy for the `SalesOrderHeaders` table. The primary key column, by default, will have a *clustered index*. A clustered index organizes data into a *B-tree*. This is a structure consisting of multiple layers of index pages, which provide pointers to index pages on lower levels of the structure. There is always one root level, consisting of a single page, known as the *Index Allocation Map (IAM)*. There are then zero or more intermediate levels, depending on the size of the table. The bottom (leaf) level of a clustered index is the actual data pages of the table.

> [!TIP]
>
> There is a full discussion around indexes in chapter 11.

Because the data within the index is ordered, records can be found very quickly by using an operation called a *clustered index seek*. This operation traverses the levels of an index to locate a record. This is opposed to a *clustered index scan*, which is an operation where every page of the leaf level of the index has to be searched until the required record is found. These operations, along with an example of the B-tree structure, are illustrated in figure 4.6. You will notice that whenever a single row is searched, the maximum number of pages read by a seek operation is equal to the number of levels of the B-tree. The maximum number of pages read by an index scan, however, is equal to the number of data pages that comprise the leaf level of the index and therefore the table. If multiple rows can be returned by the operation, then a seek is used to find the start point for reading the leaf level.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F06_Carter.png)<br>
**Figure 4.6 Clustered index seeks and scans against a B-tree**

> [!TIP]
>
> Because a clustered index orders the data pages of the table itself, there can only ever be a single clustered index on a table. A table that does not have a clustered index is called a *heap*. In a heap, records are stored in no particular order. Heaps are often used for purposes such as staging tables, which have large inserts of ephemeral data, where order is irrelevant, as this can optimize performance.

The business has told us that they often search sales orders by filtering the order date. Therefore, we want to optimize these queries by creating a nonclustered index on the `SalesOrderDate` column. A nonclustered index creates a B-tree structure that is very similar to a clustered index. The difference is that instead of the leaf level of the index containing actual data, it contains pointers to the data within the clustered index or heap. Because the leaf level is just pointers, this means that the actual data is not being ordered. Thus we can have multiple nonclustered indexes on a single table. The pointers to the clustered index consist of the clustered index key. This means that a wide clustered key is replicated to all nonclustered indexes.

> [!NOTE]
>
> If a nonclustered index has `INCLUDE` columns, then the actual data values of these columns will be stored in the index along with the pointer to the index or heap.

Additionally, the business has told us that sales order queries will usually include sales area information and customer information. This data is stored in different tables, so we will also want to create nonclustered indexes on the `SalesAreaID` and `CustomerID` columns, which are the foreign keys for these tables. By creating nonclustered indexes on the foreign keys, the keys are ordered in the same way as their primary key counterparts in the tables that we join to. This can allow for more efficient join operations.

To see the consequences of having a wide primary key, let’s first add some data to the relevant tables. The script in listing 4.2 adds a small amount of data to the relevant tables. It then creates the nonclustered indexes on the `SalesOrderHeaders` table. There is no need to create the clustered index, as this was created automatically when we created the primary key on the table.

Listing 4.2 Adding data and creating nonclustered indexes

```sql
INSERT INTO dbo.SalesPersons (
    SalesPersonFirstName,
    SalesPersonLastName,
    SalesPersonEmail
)
VALUES
    ('Robin', 'Wells', 'robin.wells@magicchoc.com'),
    ('Jack', 'Jones', 'jack.jones@magicchoc.com'),
    ('Jane', 'Smith', 'jane.smith@magicchoc.com') ;

INSERT INTO dbo.SalesAreas (
    SalesAreaName,
    SalesAreaManagerFirstName,
    SalesAreaManagerLastName
)
VALUES
    ('US', 'Lucy', 'Sykes'),
    ('Euro', 'Ashwin', 'Kumar'),
    ('APAC', 'Emma', 'Roberts') ;

INSERT INTO dbo.Addresses (Street, Area, City, ZipCode)
VALUES
    ('744 Saxon Rd', NULL, 'Crawfordsville', '47933'),
    ('267 Old York Ave.', NULL, 'Reno', '89523'),
    ('923 Taylor Ave.', NULL, 'Charlotte', '28205'),
    ('942 Cactus Street', NULL, 'Albany', '12203'),
    ('32 Selby Drive', NULL, 'Pittsfield', '01201'),
    ('65 New Street', 'Landford', 'Salisbury', 'SP5 2QP') ;

INSERT INTO dbo.CustomerContacts (
    CustomerContactFirstName,
    CustomerContactLastName,
    CustomerContactEmail
)
VALUES
    ('Ralphie', 'Buchanan', 'Ralphie.Buchanan@Pitt.com'),
    ('Bettie', 'Peters', 'bpeters@cookingschmooking.co.uk'),
    ('Zackery', 'McEachern', 'Zackery.McEachern@wilsonindustries.com') ;

INSERT INTO dbo.Customers (
    CustomerCompanyName,
    CustomerContactID,
    InvoiceAddressID,
    DeliveryAddressID
)
VALUES
    ('Pitt and Co', 1, 1, 2),
    ('Cooking Schmooking', 2, 3, 4),
    ('Wilson Industries', 3, 6, 6) ;

INSERT INTO dbo.SalesOrderHeaders (
    SalesOrderNumber,
    SalesOrderDate,
    SalesPersonID,
    SalesAreaID,
    CustomerID,
    SalesOrderDeliveryDueDate,
    SalesOrderDeliveryActualDate,
    CurrierUsedforDelivery
)
VALUES
    ('COO1634D-U06', '20230501', 1, 1, 1, '20230503', '20230503', 'LHD'),
    ('WIL1635D-E16', '20230616', 2, 2, 3, '20230630', NULL, 'GoodSpeed
International'),
    (‚PIT1636D-U04', ‚20230616', 3, 1, 1, ‚20230706', NULL, ‚LHD') ;
GO

CREATE NONCLUSTERED INDEX NI_SalesOrderHeaders_OrderDate
    ON SalesOrderHeaders(SalesOrderDate) ;
CREATE NONCLUSTERED INDEX NI_SalesOrderHeaders_SalesAreaID
    ON SalesOrderHeaders(SalesAreaID) ;
CREATE NONCLUSTERED INDEX NI_SalesOrderHeaders_CustomerID
    ON SalesOrderHeaders(CustomerID) ;
```

Next, to demonstrate the issue, we need to see what is stored on the index pages of one of the indexes that we have just created. Let’s use the `NI_SalesOrderHeaders_OrderDate` index here, but we could pick whichever index we like.

There are three steps that are required to view inside the index pages. The first step is to determine the index ID of our index. This is not the object ID, which is unique across the instance. Instead, this is the index identifier within the table. We can list the details for a given index by interrogating `sys.indexes` and filtering by the name of the index, as shown in the following listing.

Listing 4.3 Determining the index ID

```sql
SELECT
    name
  , index_id
  , type_desc
FROM sys.indexes
WHERE name = 'NI_SalesOrderHeaders_OrderDate' ;
```

The index ID that I have is 2, but you may find the ID is different if you created the indexes in a different order. Now that we have the index ID, the next step is to use the `DBCC IND` command to list out all of the pages that make up the index.

The command lists out the IAM chain of the index, from the root level through to the leaf level. There is one row returned for every page within the index that includes page numbers, index level, and page pointers for traversing the index, as well as other useful information.

The command accepts three parameters, namely, database name (or database ID), table name, and index ID. The following listing demonstrates how to use this command.

Listing 4.4 Using `DBCC IND` to discover index page numbers

```sql
DBCC IND ('MagicChoc', 'SalesOrderHeaders', 2) ;
```

WARNING My results are shown next, but please note that you will almost certainly find that your page numbers differ. Please make sure you use your own page numbers in the following examples.

The results are shown in figure 4.7. You will notice that there are only two pages. The first page is the root page of the index. There will always be exactly one root page, regardless of index size. There are no intermediate levels of the index because it is too small. The leaf level of the index consists of exactly one page, because there are only three records in the table, so everything sits comfortably inside a single page.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F07_Carter.png)<br>
**Figure 4.7 Results of `DBCC IND`**

The final step is to use another DBCC command, called `DBCC PAGE`. This command can be used to display the contents of a data or index page. The command accepts four parameters, namely, the database name, the file ID, the page ID (taken from `DBCC IND`), and a parameter that specifies the formatting option. It is important to note, however, that before running `DBCC PAGE`, we must ensure that trace flag 3604 is turned on. Trace flags are used to toggle features on and off, and we will discuss some of these in chapter 10. This particular flag sends DBCC output to the console. The following listing demonstrates how to use this command.

Listing 4.5 Using `DBCC PAGE` to view the page contents

```sql
DBCC TRACEON(3604) ;
DBCC PAGE('MagicChoc', 1, 600, 3) ;
```

My results from running this command are displayed in figure 4.8. As expected, you will see the index key value in the `SalesOrderDate` (key) column. But look at the column next to it: the `SalesOrderNumber` (key) column. The value of the primary key is stored in every row. This provides the pointer back to the data in the clustered index.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH04_F08_Carter.png)<br>
**Figure 4.8 Results of `DBCC PAGE`**

Therefore, the primary key is not only taking up 12 bytes per row in the table but also up to 12 bytes for every row in every single nonclustered index on the table. What is worse is that our nonclustered indexes are not unique. If a nonclustered index is unique, then the clustered key value is only stored on the leaf level of the index. For nonunique, nonclustered indexes, however, the clustered key value is stored on every row at all levels of the index.

This wide key is not only using additional disk space and memory, but it is also going to damage the performance of the indexes. Remember that a page can only store 8,000 bytes of data. Therefore, the wider the key, the more index pages are required. In turn, SQL Server then must read more pages to retrieve the required data.

In this scenario, where we either find ourselves with a wide column as our primary key or we find that our primary key is a composite of multiple columns, my advice is to create an artificial primary key and automatically populate it with `IDENTITY`. If we need to enforce uniqueness on our natural key, we can still achieve this without bloating our clustered and nonclustered indexes by building a unique nonclustered index on the column.

Globally unique identifiers as primary keys

One of the examples of wide primary keys I have seen is the use of globally unique identifiers (GUIDs).

GUIDs can be generated by using the `NEWID()` function and provide a mechanism for generating values that are unique across the whole server. We sometimes must use GUIDs as primary keys, however. For example, there are some COTS (commercial off the shelf) products that specifically mandate it.

There are two considerations with using GUIDs as a primary key. The first is the size. GUIDs are 16 bytes wide, so we need to ask ourselves: do we really need a value that is globally unique, or do we just need it to be unique within the context of the table? If it needs to be unique across a database, then there may be other methods, such as the `SEQUENCE` feature, that can achieve this in a more appropriate way.

The other consideration is the random nature of a GUID. A clustered index, by definition, orders the data within the table by the values in the clustered index, which is usually based on the primary key. If these values are random, then SQL Server will have to keep reordering the data pages. This means that the index can suffer from a problem known as *page splits*.

When a page split occurs, data will be moved between pages to ensure that the data remains ordered. This will lead to increased IO and reduced performance for `UPDATE` and `INSERT` operations. Additionally, it will lead to index fragmentation, which we will discuss in chapter 11. Fragmentation will reduce the performance of `SELECT` operations until such time as the index is rebuilt.

This issue can be offset by using a low fill factor on the clustered index and rebuilding indexes at a low rate of fragmentation, but generally I recommend creating a nonclustered primary key and then generating the clustered index on a column of integer data type. We can then create the primary key on the column containing the GUIDs.

## 4.3 #13 Not using foreign keys

Missing foreign key constraints is a mistake that I have seen some less experienced developers make, usually citing the reason that if they need to quickly insert data into a table, then having the constraints make it too time consuming. I have most often seen this view taken, in situations like ours, when the key is a natural key and therefore has business meaning and is controlled by a frontend application.

The challenge with this train of thought is that a frontend application simply cannot guarantee data integrity in the same way that SQL Server constraints can. To discuss this topic in more detail, let’s use the example of our `PurchaseOrderHeaders` and `PurchaseOrderDetails` tables.

You may remember that there is a mistake in our database creation script in listing 4.1, resulting in the `PurchaseOrderNumber` column existing in the `PurchaseOrderDetails` table, but as a foreign key was not created, there is no join back to the `PurchaseOrderHeaders` table.

Before we go any further, let’s run the script in the following listing to insert data into the relevant tables.

Listing 4.6 Inserting data into product and purchasing tables

```sql
INSERT INTO dbo.ProductCategories (
    ProductCategoryName,
    ProductCategoryDescription
)
VALUES
    ('Raw Ingridience', NULL),
    ('Machine Parts', 'Parts used by manufacturing for machine maintenance'),
    ('Misc', 'Office supplies and other miscelaneous stock items'),
    ('Services', 'Non-stock purchases, such as transport'),
    ('Confectionary Products', NULL),
    ('Non-confectionary Products', NULL) ;

INSERT INTO dbo.ProductSubcategories (
    ProductCategoryID,
    ProductSubcategoryName,
    ProductSubcategoryDescription
)
VALUES
    (1, 'Chilled Ingredience', 'Ingredience that must be kept between 1C
and 5C'),
    (1, 'Frozen Ingredience', 'Ingredience must be kept below -18C'),
    (1, 'Ambient Ingredience', 'Ingredience that should be kept in cool,
dry storage'),
    (2, 'Line 1 Components', 'Components required for manufacturing line 1'),
    (2, 'Line 2 Components', 'Components required for manufacturing line 1'),
    (2, 'Line 3 Components', 'Components required for manufacturing line 1'),
    (2, 'Line 4 Components', 'Components required for manufacturing line 1'),
    (3, 'Office Supplies', 'Stationary, etc'),
    (3, 'Misc', NULL),
    (4, 'Curriers', NULL),
    (4, 'Building Maintenance', NULL),
    (5, 'Boxes of chocolates', NULL),
    (5, 'Sweets', NULL),
    (5, 'Chocolate Bars', NULL),
    (6, 'Packaging', 'Product Packaging'),
    (6, 'Merchandise', 'Non-core items which are procured and sold, such as mugs and branded gifts') ;

INSERT INTO dbo.ProductTypes (ProductTypeName, ProductTypeDescription)
VALUES
    ('Purchased Product', 'Products which are purchased'),
    ('Sold products', 'Products which are sold'),
    ('Traded Products', 'Products which are both bought and sold') ;

INSERT INTO dbo.SupplierContacts (
    SupplierContactFirstName,
    SupplierContactLastName,
    SupplierContactEmail
)
VALUES
    ('John', 'Smith', 'john.smith@smithfields.com'),
    ('John', 'Doe', 'john.doe@unknownengineering.com'),
    ('Michael', 'Knight', 'mknight@knightridercurriers.com') ;

INSERT INTO dbo.Addresses (
    Street,
    City,
    ZipCode
)
VALUES
    ('8648 Columbia Street', 'Beachwood', '44122'),
    ('83 Addison Dr.', 'Westerville', '43081'),
    ('508 Mill Pond Street', 'Clinton Township', '48035') ;

INSERT INTO dbo.Suppliers (
    SupplierName,
    SupplierContactID,
    SupplierAddressID
)
VALUES
    ('Smithfeilds', 1, 7),
    ('Unknown Engineering', 2, 8),
    ('Knight Rider Curriers', 3, 9) ;

INSERT INTO dbo.Products (
    ProductName,
    ProductStockLevel,
    ProductTypeID,
    ProductSubcategoryID
)
VALUES
    ('Large head sprocket', 3, 1, 4),
    ('Long weight', 6, 1, 4),
    ('Staples', 8900, 1, 8),
    ('Magic Mug', 38, 3, 16),
    ('Massive Magic Box', 18, 2, 12),
    ('Delivery', -1, 3, 10) ;
```

We will now simulate our frontend application inserting purchase orders into the table using the script in listing 4.7. The application is turning on `XACT_ABORT` and wrapping both `INSERT` statements inside a transaction. That’s good. It means if one statement fails, the other will also fail, helping to keep the data consistent.

Listing 4.7 Adding a purchase order

```sql
SET XACT_ABORT ON ;

BEGIN TRANSACTION
    SELECT @@TRANCOUNT ;

    INSERT INTO dbo.PurchaseOrderHeaders  (
        PurchaseOrderNumber,
        SupplierID,
        PurchaseOrderDate
    )
        VALUES
            (6826, 2, '20230601'),
            (6827, 2, '20230617') ;

    INSERT INTO dbo.PurchaseOrderDetails (
        ProductID,
        Quantity,
        PurchaseOrderNumber
    )
    VALUES
        (4, 3, 6826),
        (5, 4, 6827),
        (4, 1, 6827) ;
COMMIT
```

But let’s think about the driver for leaving out a foreign key: the fact that we may want the ability to quickly and easily update a table. Consider the following scenario. We have a call from the head of procurement, who says, “There has been a problem with a purchase order. I need you to fix it in the backend, as the application isn’t letting me deal with it. I need you to move the ordered items from purchase order 6827 onto purchase order 6828—pronto!”

We are under pressure, so we, perfectly rationally, do exactly what the head of procurement asked us to do, by running the statement in the following listing.

Listing 4.8 Amending a purchase order

```sql
UPDATE dbo.PurchaseOrderDetails
SET PurchaseOrderNumber = 6828           ①
WHERE PurchaseOrderNumber = 6827 ;
```

① This would not have been possible if a foreign key constraint was used.

Unfortunately, what the head of procurement really wanted us to do was “Create a new purchase order, with a purchase order number of 6828, transfer the ordered items across, and then delete purchase order 6827.”

So now we are in a situation where our data is in an inconsistent state. If we were to run a query, such as the query in listing 4.9, which returns details of purchase orders from across the two tables, then neither purchase order 6827 nor purchase order 6828 would be returned.

If we had created a foreign key constraint, then this would not have been possible because referential integrity would have been enforced, and we would not have been able to update the `PurchaseOrderNumber` to a value that did not exist in the `PurchaseOrderHeaders` table.

Listing 4.9 Returning purchase order information

```sql
SELECT
      poh.PurchaseOrderNumber
    , poh.PurchaseOrderDate
    , pod.ProductID
    , pod.Quantity
FROM dbo.PurchaseOrderHeaders poh
INNER JOIN dbo.PurchaseOrderDetails pod
    ON poh.PurchaseOrderNumber = pod.PurchaseOrderNumber ;
```

To avoid this pitfall, ensure that you always use foreign key constraints. We can fix the mistake in the MagicChoc database by running the script in listing 4.10. Before creating the constraint, we fix the data values; otherwise, the constraint creation would fail.

Listing 4.10 Adding a foreign key constraint

```sql
UPDATE dbo.PurchaseOrderDetails
SET PurchaseOrderNumber = 6827
WHERE PurchaseOrderNumber = 6828 ;

ALTER TABLE dbo.PurchaseOrderDetails ADD CONSTRAINT
    FK_PurchaseOrderDetails_PurchaseOrderHeaders
        FOREIGN KEY (PurchaseOrderNumber)
            REFERENCES dbo.PurchaseOrderHeaders(PurchaseOrderNumber) ;
```

When tables have relationships with each other, it is always good practice to create primary and foreign key relationships between them. Even if logic is built into the application to avoid inconsistent data, this logic will not be able to deal with mistakes that happen outside the application.

## Summary

* Always normalize your data, rather than trying to design your schema using judgment, to avoid mistakes.
* Where appropriate, consider using data generalization to create supertypes and subtypes, as this can enhance your database design.
* Use an ERD to help validate, as well as to document your design. This diagram will visualize the entities with their attributes, as well as their primary and foreign keys, with the relationships between them.
* Avoid using wide columns or multiple columns as a primary key, as the primary key will often become the clustered index and thus will be replicated in your nonclustered indexes and can lead to performance degradation.
* Always use a foreign key constraint, where tables are related to each other, to avoid data consistency issues.
