# 11 Indexes

This chapter covers

* Index fragmentation
* Index maintenance
* How indexes interact with extract, transform, load processes
* Database Engine Tuning Advisor
* Columnstore indexes

In this chapter, we will first explore some common mistakes and misconceptions around fragmentation. Understanding index fragmentation can be crucial to a well-performing database. For example, index fragmentation is often considered universally bad, but we will explore how too little internal fragmentation can lead to bad page splits.

We will then turn our attention to index maintenance and look at various mistakes that can lead to an increase in index fragmentation and suboptimal performance. These mistakes range from not rebuilding indexes at all to rebuilding them indiscriminately. We will also assess how index maintenance interacts with updating statistics.

We will explore how indexes impact extract, transform, load (ETL) operations. Specifically, we will explore bulk load operations and discuss how our index strategies are crucial to a performant ETL run. After this, we will take a look at Database Tuning Advisor and think about why it might not be a good idea to rely too heavily on this feature.

Finally, we will look at columnstore indexes, which can add incredible performance improvements to analytical and data warehouse–style workloads. Despite being introduced to SQL Server over a decade ago, they're still not used by many people. We will discuss why this is and why it is a mistake.

In this chapter, code examples will use the `Marketing` and `MarketingArchive` databases that we created in previous chapters. Therefore, I recommend using these databases if you would like to follow along. Before starting, however, I provide a quick refresher on index concepts that we introduced in chapter 4.

If we create a table with no clustered index, this is known as a *heap*. In a heap, data is stored in no particular order. Instead of an index root, there is a simple Index Allocation Map that stores a list of all pages allocated to the heap. For larger tables, this means that SQL Server has to work hard to find any given value. Specifically, almost any query `SELECT` statement issued against a heap will require SQL Server to read every single data page that makes up the table. A heap is illustrated in figure 11.1.

> [!TIP]
>
> Queries that include keywords such as `TOP` may not require a read of all pages in the table.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F01_Carter.png)<br>
**Figure 11.1 A heap stores pages in no particular order**

If we create a clustered index on a table, then SQL Server builds a B-tree structure, such as the structure illustrated in figure 11.2. This operation orders the pages in a table using the clustered key. The clustered key is usually built on the primary key of the table. If you have a wide primary key, however, it is possible to build it on a different, unique column.

A clustered index allows for read operations to be performed more efficiently, and we will discuss the different index operations that are available later in this chapter. The leaf level of the B-tree structure is the actual data pages of the table. This means that the data pages of the table are stored in the order of the clustered key. For this reason, we can only have a single clustered index on a table.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F02_Carter.png)<br>
**Figure 11.2 A clustered index creates a B-tree and orders data pages**

A *nonclustered index* is a B-tree structure built on a different column(s) within the table. SQL Server can use these indexes to improve the performance of operations such as joins, filters, and aggregations. The leaf level of a nonclustered index contains pointers to the data pages of the heap or clustered index. Because it does not impact the order of the actual data pages, we can have multiple nonclustered indexes on a table. In fact, a table can support up to 256 nonclustered indexes, although having too many can have a negative impact on write operations and also consumes space on disk, and potentially in memory.

## 11.1 #67 Assuming internal fragmentation is always bad

One of the first things we are taught as database administrators (DBAs) is that fragmentation is bad: don’t allow our indexes to become fragmented, or performance will suffer. This is not quite the full story, however, and can lead to problems. To understand why, we need to remember the two types of fragmentation that can occur. *External fragmentation* refers to index pages becoming out of physical order. *Internal fragmentation* describes the amount of free space on index pages. In this section, we will focus on why internal fragmentation can be problematic.

Imagine that we have a database that supports transactional workloads. That is, there are frequent `INSERT`, `UPDATE`, and `DELETE` statements being issued against the database, and this includes inserts to a table that contains customer information. To improve performance, we have created a nonclustered index on the `LastName` column, and this index has the default settings, which means that, when the index was built, SQL Server had a target to completely fill every index page, only leaving enough space for a single row.

A user runs an `INSERT` statement to add a new customer and a new row, including a new `LastName`, is inserted into the table. Because there is an index on the `LastName` column, a new row also needs to be inserted into the index page of a nonclustered index.

The diagram in figure 11.3 depicts the process that needs to happen for the new row to be inserted. First, SQL Server tries to insert the value, in this case `Crane`, to the page, but discovers there is no room. It then creates a new page, which will cause external fragmentation and moves half of the data from the original page to the new page. There is now enough space in the page for SQL Server to insert the row. The pointers between the pages also need to be updated.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F03_Carter.png)<br>
**Figure 11.3 Adding a new row to a full index page**

This process is known as a *page split*, and it’s a bad type of page split. It generates additional I/O and causes external fragmentation in the process. We should try to minimize this type of page split occurring whenever possible.

> [!TIP]
>
> Page splits can be caused by updates as well as inserts. If, for example, a `VARCHAR(60)` column held a value that was 5 characters long and was updated to hold a value 60 characters long, then the increase in the size of the data could result in a page split. This is known as an *expansive update*.

In the preceding example, the bad page split occurred on a nonclustered index, but page splits can also occur on clustered indexes. This tends to happen when the clustered key is not sequential, but it can also be caused by expansive updates when the clustered key is sequential.

Mini-mistake—page splits are not always bad

Just as many people believe that fragmentation is always bad, many people think page splits are always bad. In fact, a page split is a perfectly normal operation in SQL Server. If page splits never occurred, then it would not be possible to add new rows to a table or index.

The difference between good and bad page splits depends on which page in the index is split. If the last page in the index is split because we have added a new row to an index that is full, then a new page is allocated at the end of the index, and new rows can be inserted. In this instance, data is not moved to the new page. A new page is simply allocated to the table. This is regarded as a good page split.

In the preceding example, however, the split happened in the middle of the index, resulting in external fragmentation (out-of-order pages) and avoidable I/O. This is regarded as a bad page split, and we should try to avoid this scenario.

So if having full pages causes bad page splits, should we just accept a high level of internal fragmentation on all of our indexes? Well, no. It is very much dependent on the particulars of our environment.

For example, imagine we have a database supporting data warehouse–style workloads. There is a nightly ETL process that drops the indexes, bulk loads data, and then rebuilds the indexes. During the day, there are no `INSERT`, `UPDATE`, or `DELETE` statements executed against the database. In this scenario, there is no opportunity for page splits to occur, so we will probably want our pages to be as full as possible.

In a transactional environment, things are a little more complicated, and we are going to have to make tradeoffs. If our pages have very low internal fragmentation (the pages are very full), then there is an increased risk of page splits. If the pages have very high internal fragmentation (the pages have lots of free space), however, then it takes more pages to store the same amount of data. This means that SQL Server needs to read more pages from disk, which means more I/O and that data pages will not stay in the cache for so long. This in turn means that the *buffer cache hit ratio* will drop. This means that pages will need to be read from the disk more often.

Therefore, it is not possible to give a firm recommendation for target internal fragmentation levels. The correct level will depend on many factors, ranging from read and write performance of the disk subsystem to the number and type of transactions that are performed against the database. As a starting point, however, for data warehouse–style workloads, pages as full as 95% to 100% full may be appropriate. For busy transactional systems, the ideal level may be as low as 60% to 70%.

We have talked a lot about internal fragmentation and ideal levels of *page density*, which means how full the pages are. What we have not discussed is how to configure the target page density. We discuss this next.

SQL Server uses a setting called `FILLFACTOR` to determine the target page density for pages on the leaf level of an index. It also has a setting called `PAD_INDEX`, which can be used to control page density at the intermediate levels of the index. The root level always consists of exactly one page, so is not subject to either internal or external fragmentation.

The default value for `FILLFACTOR` is `0`, which means 100% full. If this is replaced with any other value, then this becomes the target page density percentage. `PAD_INDEX` can only be configured as `ON` or `OFF`; `OFF` is the default option. If we configure `PAD_INDEX` as `ON`, then it will apply the `FILLFACTOR` level to the intermediate pages. It is not possible to configure a page density that is different from the leaf level.

`PAD_INDEX` is arguably less important than `FILLFACTOR`, but for large indexes with multiple intermediate levels, a page split can propagate right up the B-tree, significantly compounding its impact. Therefore, `PAD_INDEX` is worth enabling for indexes on very large tables that have a transactional workload and are suffering from page splits.

To configure the `FILLFACTOR` for an index or enable `PAD_INDEX`, we need to rebuild the index. The command in the following listing sets the `FILLFACTOR` of the `ImpressionUID` index on the `Marketing.Impressions` table of the `Marketing` database to be 90% and enables `PAD_INDEX`.

Listing 11.1 Setting a `FILLFACTOR` and enabling `PAD_INDEX`

```sql
ALTER INDEX ImpressionUID ON marketing.Impressions REBUILD
WITH (
    FILLFACTOR = 90,
    PAD_INDEX = ON
) ;
```

Many DBAs are taught to believe that index fragmentation is always bad. Internal fragmentation can actually be better than having 100% full pages, however. This is because overly full pages can lead to a large number of bad page splits. We need to trade off the number of page splits against the number of pages used to store our indexes. The correct `FILLFACTOR` will be unique to the requirements of a specific workload.

## 11.2 #68 Believing that external fragmentation causes problems for all queries

External fragmentation causes performance degradation. There are no two ways about it. A trap that I see many DBAs fall into, however, is using index rebuilds as their very first troubleshooting step whenever a user complains of performance problems. In some cases, this can take some time, and in situations where there is no chance that index fragmentation is the root cause, we risk wasting valuable time.

For example, imagine that users are complaining that queries are taking a long time to run against a database. Many of the queries that run are returning either a scalar value or a single row. In these scenarios, as we will see shortly, the root cause of the issue is unlikely to be fragmentation, so running an emergency rebuild of all indexes in the database is unlikely to help. What’s more, depending on how we rebuild the indexes, we have the potential to degrade performance even more for the duration of the maintenance.

To understand this, we need to understand the different operations that can be performed against an index. These operations are seek, scan, and lookup. A *seek* operation starts at the root page of an index and traverses each level of the B-tree structure to find the row it’s looking for. This process, illustrated in figure 11.4, is the fastest operation for retrieving a small number of rows.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F04_Carter.png)<br>
**Figure 11.4 Index seek**

An index *scan* comes into effect when SQL Server has to read a higher percentage of rows from an index. In this situation, a seek operation would be inefficient, so instead it scans all pages at the leaf level of the B-tree. This is illustrated in figure 11.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F05_Carter.png)<br>
**Figure 11.5 Index scan**

While index seeks and index scans apply to both clustered and nonclustered indexes, an index *lookup* applies only when a nonclustered index has been used to satisfy the query. SQL Server uses the nonclustered index to perform operations such as filtering data through the `WHERE` clause but then needs to perform a lookup operation to the data pages of the clustered index or heap to return additional columns. This is illustrated in figure 11.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F06_Carter.png)<br>
**Figure 11.6 Index lookup**

Clearly, if the index pages are out of order, an index scan operation will be degraded. Therefore, we can all categorically agree that external fragmentation will result in suboptimal performance for index scan operations.

If we are performing a seek operation, however, then the pages being out of order at the leaf level will have no impact on query performance. Therefore, it is a misconception to think that external fragmentation will impact the performance of seek operations.

The same is true for an index lookup from a nonclustered index to a clustered index or a key lookup from a nonclustered index to a heap. The leaf level of the nonclustered index contains a pointer to the associated data page in the table. Therefore, fragmentation will not generate additional I/Os for the lookup.

We can clearly see that external fragmentation does not impact all queries, and if SQL Server is performing a low ratio of scan operations against an index, then fragmentation is unlikely to be the root cause, and our attention is best spent investigating other possible causes.

We can quickly assess the ratio of index scans against other index operations by using the `sys.dm_db_index_usage_stats` dynamic management view (DMV). This object returns a row for every index in the instance that has been used since the last time the Database Engine cycled. Each row contains the number of index operations, as well as the last time they were performed.

The query in listing 11.2 returns information for indexes within the `Marketing` database. It joins these results to the `sys.indexes` catalog view to return the name and type of the index.

Listing 11.2 Discovering index usage by index operation

```sql
SELECT
      OBJECT_NAME(ius.object_id) AS TableName
    , i.name AS IndexName
    , i.type_desc
    , ius.user_seeks
    , ius.user_scans
    , ius.last_system_lookup
    , ius.last_user_seek
    , ius.last_user_scan
    , ius.last_user_lookup
FROM sys.dm_db_index_usage_stats ius
INNER JOIN sys.indexes i
    ON ius.index_id = i.index_id
    AND ius.object_id = i.object_id
WHERE DB_NAME(database_id) = 'Marketing' ;
```

> [!TIP]
>
> `sys.dm_db_index_usage_stats` does not include information about memory-optimized indexes. While this type of index is beyond the scope of the book, it is worth noting that their usage statistics can be found in `sys.dm_db_xtp_index_stats`.

Don’t think that fragmentation impacts all index operations. It will have a negative impact on index scans but will not affect seek operations. Therefore, if an index has a high ratio of seek operations, fragmentation is unlikely to be the root cause of the issue. Our time is best spent investigating other possible causes.

## 11.3 #69 Reorganizing indexes to fix page density

We have two options for maintaining indexes in SQL Server: reorganizing indexes and rebuilding indexes. When we rebuild an index, SQL Server creates a new B-tree structure and removes the original. This allows for near-perfect removal of fragmentation but has quite a high resource cost. Reorganizing an index, on the other hand, has a much lower cost but is limited in what it can achieve. Instead of creating a new B-tree structure, it condenses leaf-level pages up to the level of the `FILLFACTOR` in an attempt to remove internal fragmentation and attempts to reorganize the leaf-level pages to remove external fragmentation.

Imagine we have an index with an intentionally low `FILLFACTOR` of 60%. As data is inserted into the index, the pages become full. Because of the low page density, however, there is very little external fragmentation.

A mistake that I have seen many a DBA make in this scenario is to base their index maintenance on external fragmentation alone and reorganize the index. The trouble with this approach is that when we reorganize an index, it condenses pages up to the level of the `FILLFACTOR` but it does not reduce page density down to the level of the `FILLFACTOR`.

The result of this is that even though we have recently reorganized the index, it will very soon become highly externally fragmented because the pages are still full after reorganizing, and therefore bad page splits will soon start occurring.

We will then have to rebuild the index to repair the external fragmentation and reduce page splits. This means that, in a short space of time, we will end up both reorganizing and rebuilding the index when we could simply have rebuilt it in the first place.

We can check the level of internal and external fragmentation by using the `sys.dm_db_index_physical_stats` dynamic management function (DMF). This function accepts the parameters detailed in table 11.1.

Table 11.1 `sys.dm_db_index_physical_stats` parameters

| Parameter | Description |
| --- | --- |
| `database_id` | The database ID of the database for which we wish to return results. `NULL` returns details for all databases. |
| `object_id` | The object ID of the table for which we wish to return results. `NULL` will return information for all tables. |
| `index_id` | The index ID for which we wish to return results. `NULL` will return results for all indexes. |
| `partition_number` | The partition number for which we wish to return results. `NULL` will return information for all partitions. |
| `mode` | Can be specified as `LIMITED`, `SAMPLED`, or `DETAILED`. `LIMITED` will only scan the root and intermediate levels of the B-tree. `SAMPLED` will scan 1% of the pages within a table* and `DETAILED` will scan every page of the index. |
| * If the table has fewer than 10,000 pages, then `DETAILED` mode will be used. | |

The function returns one row for every level of every index in scope. The key columns of interest for determining the level of fragmentation are `avg_fragmentation_in_percent`, which details the level of external fragmentation, and `avg_page_space_used_in_percent`, which details the page density.

> [!TIP]
>
> While `avg_fragmentation_in_percent` and `avg_page_space_used_in_percent` are the key columns for determining fragmentation, other columns are also important to help us interpret the data. We will discuss this more in the next mistake.

The query in listing 11.3 illustrates how to return the internal and external fragmentation for all indexes in the `Marketing.Impressions` table of the `Marketing` database. The `index_level` column refers to the level of the B-tree, where 0 is the leaf level.

Listing 11.3 Determining index fragmentation

```sql
DECLARE @object_id BIGINT
SET @object_id = (                              ①
    SELECT object_id                            ①
    FROM sys.objects                            ①
    WHERE name = 'Impressions'                  ①
) ;                                             ①

SELECT                                          ②
      OBJECT_NAME(ips.Object_id) AS TableName   ②
    , i.name AS IndexName                       ②
    , ips.avg_fragmentation_in_percent          ②
    , ips.avg_page_space_used_in_percent        ②
    , index_level                               ②
FROM sys.dm_db_index_physical_stats(            ②
    DB_ID('Marketing'),                         ②
    @object_id,                                 ②
    NULL,                                       ②
    NULL,                                       ②
    'DETAILED'                                  ②
) ips                                           ②
INNER JOIN sys.indexes i                        ③
    ON ips.index_id = i.index_id                ③
    AND ips.object_id = i.object_id             ③
ORDER BY
      ips.object_id
    , ips.index_id
    , index_level DESC ;
```

① Populates a variable with the object_id of the Impressions table

② Returns the fragmentation levels from sys.dm_db_index_physical_stats

③ Joins to the sys.indexes system view to return the name of the index. The join must be on both the object_id and the index_id (unique within the table).

Reorganizing an index can be a useful means of reducing small amounts of external fragmentation, as well as internal fragmentation in a lightweight manner. Where a low page density is required to avoid page splits, however, it is important to consider page density as well as external fragmentation when planning index maintenance. We can check the fragmentation of an index using the `sys.dm_db_index_physical_stats` dynamic management view.

## 11.4 #70 Misinterpreting fragmentation statistics

A common complaint that I hear from DBAs is that “rebuilding my index doesn’t remove the fragmentation.” Even worse, I have heard this used as a reason for not rebuilding indexes. In fact, rebuilding indexes certainly does remove fragmentation, and the reason for believing otherwise is driven by a misunderstanding of how to interpret the results of `sys.dm_db_index_physical_stats`. Take the query in listing 11.4 as an example of the mistake I have seen DBAs make. The query pulls data for the `Impressions` table from `sys.dm_db_index_physical_stats`. Because the output returns a row for each level of the B-tree, the query calculates the average internal fragmentation for each index on the table.

Listing 11.4 Mistakenly aggregating internal fragmentation statistics

```sql
SELECT
      i.name AS IndexName
    , AVG(ips.avg_page_space_used_in_percent)
        AS AveragePageDensity                       ①
FROM sys.dm_db_index_physical_stats(
                  DB_ID('Marketing')
                , OBJECT_ID('marketing.Impressions')
                , NULL
                , NULL
                , 'DETAILED'
    ) ips
INNER JOIN sys.indexes i
    ON ips.index_id = i.index_id
    AND ips.object_id = i.object_id
GROUP BY i.name
ORDER BY IndexName ;
```

① Averages the data across root, intermediate, and leaf levels of the B-tree

If we assume that the indexes have just been rebuilt and the `FILLFACTOR` of the indexes is set to `0` (100% full—enough free space for one row) then you might be surprised by the results. My results are shown in figure 11.7, but your results may vary.

You will notice that the average page density of indexes 1 and 2 is 66% and 58%, respectively. This is against a target of 100%. Therefore, at first glance, it looks as if the internal fragmentation has not been removed, but this is because we are asking the “wrong question.”

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F07_Carter.png)<br>
**Figure 11.7 Average internal fragmentation when aggregated across all levels of the B-tree**

Let’s examine this in more detail by homing in on the `ImpressionUID` index and running the query shown in listing 11.5. This query does not aggregate the data. It returns one row for each level of the index, and this time we have added the index name to the join to limit the number of rows returned (we could also use a `WHERE` clause). You will also notice that we have included the `index_level`, `page_count`, and `record_count` columns from `sys.dm_db_index_physical_stats`. This will give us a lot more context.

Listing 11.5 Returning granular details with contextual information

```sql
SELECT
      i.name AS IndexName
    , ips.avg_page_space_used_in_percent
        AS PageDensity                       ①
    , ips.index_level                        ①
    , ips.page_count                         ①
    , ips.record_count                       ①
FROM sys.dm_db_index_physical_stats(
                  DB_ID('Marketing')
                , OBJECT_ID('marketing.Impressions')
                , NULL
                , NULL
                , 'DETAILED'
    ) ips
INNER JOIN sys.indexes i
    ON ips.index_id = i.index_id
    AND ips.object_id = i.object_id
    AND i.name = 'ImpressionUID' ;
```

① Pulls raw data with no aggregation. This results in one row being returned for each level of the index.

While your results may differ, the results I received are shown in figure 11.8.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F08_Carter.png)<br>
**Figure 11.8 Results of detailed internal fragmentation showing fragmentation at each level**

You will notice that level 0 (which is the leaf level) has the target page density. It also has 3,832 pages. Level 1 (the intermediate level) has a 91% page density but only has 17 pages. Level 2 (the root level) only has a page density of 6% but consists of a single page.

The root level has such a low page density because it does not contain enough rows to fill more than 6% of the page. The average page density for the intermediate level is also impacted by the low number of pages. One page that is not full is a much higher percentage when aggregated as an average than when 1 page out of 3,832 is not full.

This means that, if we aggregate the averages for each index level, the results are skewed, and it appears as if the average page density has dropped to 66% when in fact the meaningful average density is 99%, which is perfectly acceptable.

We should be careful when interpreting the results of `sys.dm_db_index_physical_stats`. Double-aggregating results will lead, in many cases, to false results, which can lead us to incorrect assumptions about our indexes. This could in turn lead to issues such as unnecessarily maintaining indexes or even thinking we can neglect index maintenance altogether. In most cases, we should only be concerned about fragmentation at level 0, which is always the leaf level of the index.

## 11.5 #71 Not rebuilding indexes

The most common reason I hear for failing to rebuild indexes is that a database is used 24/7 and there is no maintenance window. Therefore, index maintenance is not possible, as SQL Server will hold object locks for the duration of the rebuild, stopping users from accessing the table. In some scenarios where DBAs have this philosophy, index reorganization is relied upon as a substitute. In other scenarios, index maintenance is simply not performed.

This approach is a mistake and will almost always lead to degraded performance due to both internal and external fragmentation, which in turn will lead to suboptimal performance whenever index scans are required. A far more appropriate approach would be to use online index rebuilds. This feature was introduced as far back as 2005, but many DBAs just aren’t aware of it.

When an online index operation is performed, instead of holding restrictive locks for the duration of the operation, a schema stability lock is acquired at the start of the operation, and a schema modification lock is acquired at the end of the operation. Both of these locks are only held for a short period. It is noteworthy, however, that if the schema lock is blocked by other transactions, it will itself block other transactions that are behind it in the queue. During the main phase of the index operation, the only lock held is an `intent shared` lock on the table, which does not block other transactions.

It is also worthy of note that, when using online index rebuilds, we should ensure that we use a `MAXDOP` of `1`. `MAXDOP` determines how many cores will be used for an operation. Setting `MAXDOP` to `1` means that only a single core will be used. The way index fragments are allocated to processors in an online rebuild means that, if `ALLOW_PAGE_LOCKS` is turned on and this is the default option, then we actually have a risk of increasing, rather than removing, external fragmentation. If we were to turn `ALLOW_PAGE_LOCKS` off, then users of the index would not be able to escalate from row to page locks, meaning many more object locks will be acquired.

NOTE An online index rebuild takes longer than an equivalent offline index rebuild.

The command in the following listing demonstrates how to rebuild the `ImpressionUID` index as an online operation.

Listing 11.6 Rebuilding an index online

```sql
ALTER INDEX ImpressionUID
ON marketing.Impressions REBUILD
WITH(
    ONLINE = ON,
    MAXDOP = 1
) ;
```

> [!NOTE]
>
> For the rest of this section, I will use the term *maintenance window* to refer to a period of lower user activity, as opposed to an offline window.

Since the release of SQL Server 2017, it has also been possible to pause and resume online index rebuilds if they exceed a maintenance window. The command in the following listing will pause the operation if it takes longer than 1 minute to complete.

Listing 11.7 Resumable online index rebuild

```sql
ALTER INDEX ImpressionUID
ON marketing.Impressions REBUILD
WITH(
    ONLINE = ON,
    RESUMABLE = ON,
    MAX_DURATION = 1,
    MAXDOP = 1
) ;
```

This would allow us to restart the index rebuild in the next maintenance window, using the command shown in the following listing.

Listing 11.8 Resuming an online index rebuild

```sql
ALTER INDEX ImpressionUID
ON marketing.Impressions RESUME
WITH(
    MAXDOP = 1
) ;
```

If there is no paused rebuild waiting to be restarted, the command will fail. Therefore, I would recommend that you use a `TRY..CATCH` block in this process, as shown in listing 11.9. This script will attempt to resume an index rebuild. If no paused rebuild is available; then it will start a new rebuild operation.

Listing 11.9 Resuming or starting a new rebuild

```sql
BEGIN TRY
    ALTER INDEX ImpressionUID
    ON marketing.Impressions RESUME
    WITH(
        MAXDOP = 1
    ) ;
END TRY
BEGIN CATCH
    ALTER INDEX ImpressionUID
    ON marketing.Impressions REBUILD
    WITH(
        ONLINE = ON,
        RESUMABLE = ON,
        MAX_DURATION = 1,
        MAXDOP = 1
    ) ;
END CATCH
```

Not performing index rebuilds will lead to external fragmentation. It will also lead to pages that have a higher-than-target page density. Both of these issues can negatively impact the performance of queries. To work around this, consider online index rebuilds. These rebuilds take longer than offline operations but do not block user transactions for the duration of the operation. They can also be paused at the end of a maintenance window and then resumed in the next window.

## 11.6 #72 Rebuilding all indexes indiscriminately

It is important to maintain our indexes. Doing so, however, results in increased resource utilization. Therefore, to keep this utilization to a minimum, we should only rebuild indexes that require it. The mistake that I see some DBAs make is to simply rebuild all indexes, regardless of the level of fragmentation. An example of this approach would be the simple script in listing 11.10, which could be scheduled to run in a SQL Server Agent job. The script constructs a list of `ALTER INDEX..REBUILD` statements as XML and then converts it into a single script in `NVARCHAR(MAX)` before executing the script.

> [!TIP]
>
> Further details on avoiding the use of cursors for DBA activity can be found in chapter 9.

Listing 11.10 Rebuilding all indexes indiscriminately

```sql
DECLARE @SQL NVARCHAR(MAX) ;

SET @SQL = (
    SELECT ' ALTER INDEX ' +
        i.name +
        ' ON ' + s.name +
        '.' +
        o.name +
        ' REBUILD ;'
    FROM sys.indexes i
    INNER JOIN sys.objects o
        ON i.object_id = o.object_id
    INNER JOIN sys.schemas s
        ON s.schema_id = o.schema_id
    WHERE i.type_desc <> 'HEAP'              ①
        AND o.type_desc = 'USER_TABLE'       ①
    FOR XML PATH('')
) ;

EXEC(@SQL) ;
```

① The only filters are HEAP and USER_TABLE, meaning that all indexes on all user tables will be rebuilt.

This approach will be particularly problematic for large databases with many indexes due to the prolonged period of intense resource utilization that it will cause. Instead of taking this approach, it is advisable to only rebuild indexes with a high level of fragmentation.

The script in listing 11.11 builds on the previous example, adding `sys.dm_db_index_physical_stats` into the query, which allows us to examine index fragmentation levels as well as the number of pages that make up the index. This means we can add additional filters for `page_count` and both external fragmentation and page density. The page density filter is looking for an average page density greater than the index target fill factor, as this cannot be fixed by reorganizing an index.

Listing 11.11 Rebuilding indexes based on fragmentation

```sql
DECLARE @SQL NVARCHAR(MAX) ;

SET @SQL = (
    SELECT
        ' ALTER INDEX ' +
            i.name +
            ' ON ' +
            s.name +
            '.' + o.name +
            ' REBUILD ; '
    FROM sys.dm_db_index_physical_stats(
        DB_ID(),
        NULL,
        NULL,
        NULL,
        'DETAILED'
    ) ips
    INNER JOIN sys.indexes i
        ON i.object_id = ips.object_id
        AND i.index_id = ips.index_id
    INNER JOIN sys.objects o
        ON i.object_id = o.object_id
    INNER JOIN sys.schemas s
        ON s.schema_id = o.schema_id
    WHERE i.type_desc <> 'HEAP'                        ①
        AND o.type_desc = 'USER_TABLE'                 ①
        AND (
            ips.avg_fragmentation_in_percent > 20 OR   ②
            ips.avg_page_space_used_in_percent >
            CASE
                WHEN i.fill_factor = 0
                    THEN 100
                ELSE i.fill_factor
            END    #B
        )
        AND ips.page_count > 1000                      ③
        AND ips.index_level = 0                        ③
    FOR XML PATH('')
) ;

EXEC(@SQL) ;
```

① We still filter out system tables and heaps.

② We filter out any indexes that do not have fragmentation issues

③ We filter out tiny indexes and nonleaf levels.

NOTE The level of external fragmentation that should trigger a rebuild should be dependent on the workload profile of the specific application. The 20% that I have used here is illustrative only and not meant as a best practice recommendation.

We should not rebuild all indexes, as this unnecessarily consumes server resources. Instead, we should look to rebuild only those indexes that require it based on fragmentation levels and page density.

## 11.7 #73 Updating statistics after rebuilding indexes

Many years ago, I wrote a blog post about the world’s worst maintenance routine. It was based on a maintenance routine I had just found in a company I was working with at the time. It ran on a nightly basis and performed the following tasks:

1. Rebuild all indexes
2. Update all statistics
3. Shrink the database

We have already discussed why shrinking databases as a matter of course is not a good idea, as it causes near-perfect fragmentation. Therefore, it is easy to see why performing this operation just after rebuilding all of our indexes would be a really bad idea. In this section, however, we will dive into why updating all statistics just after rebuilding all indexes is also a bad practice.

To understand why, we need to understand how statistics are updated. By default, if we were to run `UPDATE STATISTICS` against a table, it would update all statistics that have been created against columns and indexes on that table. It would update these statistics by using the default sample size. This default sample is based on the following rules:

* If the table is < 8 MB: scan 100% of rows.
* If the table > 8 MB: sample between 10% and 30% of rows, based on the number of rows.

If we want to execute all statistics in the database, we would use the system-stored procedure called `sp_updatestats`. If we run `sp_updatestats` without parameters, it will execute a cursor that loops around every table and issues `UPDATE STATISTICS` using the default values. The result of this will be that all column and index statistics on all tables larger than 8 MB are updated using a sample rate of between 10% and 30%.

We then need to consider that, when we rebuild an index, it automatically updates the index statistics using a full 100% sample size. Therefore, if we rebuild an index, it has statistics based on 100% of rows in the index. If we then update the statistics on that index using default values, we actually reduce the quality of those statistics while unnecessarily consuming resources.

What should we do differently? Well, in the vast majority of cases, automatically updating statistics is acceptable. There is an option to automatically update statistics as required, and an additional option to update them asynchronously, which will prevent the process from blocking the query that triggered the statistics update. Although the asynchronous option does mean that the query that triggered the update will not benefit from the updated statistics, subsequent queries will. It will also prevent the query that triggered the update from being blocked until the statistics finish updating.

Automatically updating statistics is on by default, but if it has been turned off, then we can toggle it back on, using the command in the following listing.

Listing 11.12 Enabling `AUTO_UPDATE_STATISTICS`

```sql
ALTER DATABASE Marketing
    SET AUTO_UPDATE_STATISTICS ON ;
```

If we need statistics to update asynchronously, which of course has the limitation that the query that triggered the update will not benefit from them, then we can do so using the command in listing 11.13. Note, however, that this option requires `AUTO_UPDATE_STATISTICS` to also be turned on. Otherwise, it will have no effect.

Listing 11.13 Enabling `AUTO_UPDATE_STATISTICS_ASYNC`

```sql
ALTER DATABASE Marketing
    SET AUTO_UPDATE_STATISTICS_ASYNC ON ;
```

If we are dealing with a specific performance issue, where we need to ensure that queries always have the very latest statistics, we can use `UPDATE STATISTICS` against a specific set of tables. If this needs to happen on the same schedule as index rebuilds, we can use the `COLUMNS` keyword to limit the update to the column statistics and ignore index statistics. This is demonstrated for the `marketing.Impressions` table in listing 11.14. The `FULLSCAN` keyword will use a 100% sample rate.

Listing 11.14 Updating column statistics only for a given table

```sql
UPDATE STATISTICS marketing.Impressions
WITH
      FULLSCAN
    , COLUMNS ;
```

WARNING When updating statistics manually, consider the tradeoff of performance enhancements that up-to-date statistics provide against the cost of recompiling the query.

If we did find ourselves in a position where we had to update the column statistics for all tables in a database, then we could use the same approach as we took to rebuilding indexes. This is demonstrated in the following listing.

Listing 11.15 Updating all column statistics in a database

```sql
DECLARE @SQL NVARCHAR(MAX) ;

SET @SQL = (
    SELECT
        ' UPDATE STATISTICS ' +
        s.name +
        '.' +
        t.name +
        ' WITH FULLSCAN, COLUMNS ; '
    FROM sys.tables t
    INNER JOIN sys.schemas s
        ON t.schema_id = s.schema_id
    FOR XML PATH('')
) ;

EXEC(@SQL) ;
```

> [!TIP]
>
> For a large subset of tables, this query could be enhanced to filter by a schema or table name pattern.

We should avoid updating statistics manually unless we have a specific use case that requires it. If we must update statistics manually, we should avoid updating index statistics after performing index rebuilds, as this may leave us with worse statistics.

## 11.8 #74 Not optimizing index maintenance for our needs

As SQL Server has evolved, a raft of index enhancements has been added to the product that allows us to customize our index maintenance for our specific environment. Despite this, however, it is quite rare to see DBAs taking advantage of these enhancements. Almost invariably, less seasoned DBAs and accidental DBAs continue to use the most basic syntax for index rebuilds. This is a mistake, however, as they are often missing out on performance optimizations that could impact maintenance or index usage.

We have discussed some of these enhancements already, such as online index rebuilds and resumable index operations. There are still more, even less commonly used enhancements, however, that we should consider. In this section, we will explore the considerations for `MAXDOP`, which is rarely specified for index rebuilds. We will also consider the use of `SORT_IN_TEMPDB`. Finally, we will look at `OPTIMIZE_FOR_SEQUENTIAL_KEY`.

### 11.8.1 Considerations for MAXDOP

Many DBAs are familiar with `MAXDOP`, which controls the maximum number of processors that can be used to execute a query that SQL Server has algorithmically calculated meets the parallelization cost threshold. Many DBAs, however, do not understand the implications of this setting in the context of index rebuilds.

If we do not specify `MAXDOP` for an index rebuild, SQL Server will use the default `MAXDOP` setting of the database. Not considering the `MAXDOP` for index rebuilds is a mistake, because there are performance tradeoffs that we should take into account.

To understand this, consider the diagram in figure 11.9. The diagram assumes a `MAXDOP` of 4 and illustrates that the index is broken into fragments, with one fragment assigned to each of the four processors. The fragments are then stitched back together to form the new index.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F09_Carter.png)<br>
**Figure 11.9 Parallel index rebuild creates an index fragment for each processor.**

You will notice that while values have been correctly ordered within each fragment, there is still fragmentation within the index because consideration has not been given to the order of values between fragments. Therefore, it is important that we consciously consider the optimum `MAXDOP` to use during our index rebuilds based on the needs of our environment. If we have a very short maintenance window and our indexes fragment quickly, throwing multiple processors at the rebuild will mean we can reduce the time it takes for the rebuild to complete. If we have no time constraints, however, performing the rebuilds as a single threaded operation will reduce the final fragmentation levels and therefore improve performance for index scan operations.

### 11.8.2 Considerations for SORT_IN_TEMPDB

`SORT_IN_TEMPDB` is an index rebuild option that I have never seen used to its full potential. When the option is used, the intermediate sorting is performed in `TempDB` as opposed to the user database, which is where the sorting will happen by default. On the rare occasions when I have seen it being used, it has been because DBAs have been using it to work around an issue where they are short on disk space on their data volume and do not have enough free space to rebuild their larger indexes. In this scenario, when `TempDB` is stored on a different volume, DBAs push the intermediate storage requirement to a different volume. This workaround may have some validity if we have SQL Server running on a physical server, but for virtual or cloud machines, we really should just allocate more space to the struggling volume.

The benefit of `SORT_IN_TEMPDB`, which I have seen almost universally missed, is actually in relation to the performance of the maintenance operation. In environments where `TempDB` is stored on fast storage, such as local M.2 and data files that are stored on slower SSD disks, we can reduce the time of an index rebuild operation by using the `SORT_IN_TEMPDB` option for large indexes. The process is illustrated in figure 11.10.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F10_Carter.png)<br>
**Figure 11.10 Optimizing performance of intermediate sorts by performing them on faster storage**

The tradeoff that we need to consider here is disk space. While we are reducing the space requirements for the user database, we are increasing the storage requirement overall. When we perform the intermediate sort operations in the user database, the extents used for the sort operation are released at roughly the same rate as they are allocated to the new index structure. When we sort the results in `TempDB`, however, we need enough space in `TempDB` to store the entire intermediate sorting while also requiring enough space in the user database to store the final B-tree structure.

### 11.8.3 Understanding OPTIMIZE_FOR_SEQUENTIAL_KEY

Index keys will sometimes be sequential and sometimes nonsequential. An example of a sequential index key would be a clustered key that is built on an `IDENTITY` column. The values (unless we reseed the column) will be ever increasing. An example of a nonsequential key would be a clustered index key built on a `GUID` column. New values that are inserted may need to be inserted anywhere within the index to keep the index in order.

When we have a sequential index that has a high volume of inserts, we can suffer from performance issues caused by page latch contention on the final page of the index. We can have so many inserts that a queue of page latches forms, with all of them moving at the speed of the slowest request. For example, if one insert operation is delayed because it needs to perform a page split, all of the operations lined up behind it will need to sit and wait for their page latch to be granted. This is known as *last page insert contention*.

Unbeknown to many DBAs, SQL Server 2019 introduced a new feature into indexes, known as `OPTIMIZE_FOR_SEQUENTAL_KEY`. If we build an index specifying this option, SQL Server will apply optimizations that are intended to alleviate this issue.

When we turn this option on, SQL Server uses a flow-control mechanism, which assesses threads before the point where they request a page latch. It assesses the state of the thread and the processor on which the thread is running. It uses this information to prioritize threads that are likely to complete in a single processor cycle, which increases throughput.

It is really important that we only use this option when we have a specific use case that requires it, however. For the use case that it was designed for—indexes that have a sequential key and the throughput of threads requiring latches is far greater than the available number of processors—the option can yield a substantial performance improvement. For indexes that are not suffering from throughput issues, however, the option can result in performance degradation—even if the key is sequential.

The command in listing 11.16 demonstrates how to enable the option for the `ImpressionUID` index on the `marketing.Impressions` table. You will notice that the `ALTER INDEX` statement uses the `SET` clause. The `SET` option is applied immediately, and there is no need to rebuild the index.

Listing 11.16 Enabling `OPTIMIZE_FOR_SEQUENTIAL_KEY`

```sql
ALTER INDEX ImpressionUID
ON marketing.Impressions
SET (
    OPTIMIZE_FOR_SEQUENTIAL_KEY = ON
) ;
```

## 11.9 #75 Not disabling indexes for bulk load

Well-maintained indexes and a good indexing strategy are vital to ensuring optimal read performance. There is a downside, however, and that is the negative performance implication on write operations. A common mistake is to perform bulk loads into a table that has multiple nonclustered indexes.

Consider the `ImpressionArchive` table in the `MarketingArchive` database. This table could potentially be subject to a bulk insert of millions of rows on a single load. This load will require thousands of I/O operations to allocate new pages to the table and write the data.

But now consider that two nonclustered indexes have been created to support reporting requirements. Not only do the data pages of the table need to be updated, but each of the nonclustered indexes must be updated. The row size in a nonclustered index is smaller than the row size of a clustered index because it only contains the nonclustered index key, any included columns, and a pointer to the clustered index. This is opposed to the full row of data contained in the clustered index. Despite this, for a large insert, page splits are inevitable, and they are far more likely to be bad page splits. This process is illustrated in figure 11.11.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F11_Carter.png)<br>
**Figure 11.11 Inserting new rows into a table with nonclustered indexes**

We can imagine the amount of additional I/O (and time) required for the updates to the nonclustered indexes. Instead of using this approach, which is also likely to add external fragmentation to the nonclustered indexes, it is often more performant to disable the indexes and then rebuild them when the load is complete.

> [!TIP]
>
> It is also worth noting that the updates required to the nonclustered indexes will also impact the I/O required to write to the transaction log during the `INSERT` operation.

The script in listing 11.17 demonstrates how to disable all nonclustered indexes on the `ImpressionsArchive` table, perform the `INSERT` operation, and then rebuild all of the nonclustered indexes. > [!NOTE]
>
> that disabling the clustered index will result in the table becoming inaccessible. Therefore, we will use a dynamic script based on metadata in `sys.indexes` to disable the nonclustered indexes instead of using the `ALL` keyword. For the rebuilds, however, we are using the `ALL` keyword, which means the clustered index will be included. If we did not want the clustered index to be rebuilt, we would have to script the index rebuilds using the same approach as the disable indexes operation.

Listing 11.17 Disabling and reenabling indexes

```sql
DECLARE @SQL NVARCHAR(MAX) ;

SET @SQL = (
    SELECT
        ' ALTER INDEX ' + name + ' ON ImpressionsArchive DISABLE ; '
    FROM sys.indexes
    WHERE object_id = OBJECT_ID('ImpressionsArchive')
        AND type > 1
    FOR XML PATH ('')
) ;

EXEC(@SQL) ;
GO

INSERT INTO MarketingArchive.dbo.ImpressionsArchive (
    ImpressionUID,
    ReferralURL,
    CookieID,
    CampaignID,
    RenderingID,
    CountryCode,
    StateID,
    BrowserVersion,
    OperatingSystemID,
    BidPrice,
    CostPerMille,
    EventTime
)
SELECT
      ImpressionUID
    , ReferralURL
    , CookieID
    , CampaignID
    , RenderingID
    , CountryCode
    , StateID
    , BrowserVersion
    , OperatingSystemID
    , BidPrice
    , CostPerMille
    , EventTime
FROM Marketing.marketing.Impressions ;
GO

ALTER INDEX ALL ON dbo.ImpressionsArchive REBUILD ;
GO
```

Although nonclustered indexes improve performance for read operations, they can have a negative impact on the performance of `INSERT` and `UPDATE` statements. For bulk load operations, it can be beneficial to disable and reenable indexes, instead of performing the required index updates during the bulk load process.

## 11.10 #76 Relying too heavily on Database Engine Tuning Advisor

Database Engine Tuning Advisor (DTA) is a graphical tool, accessible from the Tools menu of SQL Server Management Studio, which can offer DBAs guidance on the creation and removal of indexes, indexed views, partitioning strategies, and potential changes to the physical design of a database. In this section, we will focus on its guidance around indexing strategies.

A common mistake that I see less experienced DBAs make is to run DTA against a workload and then implement all recommendations without further consideration. The trouble with this approach is that it does not take into consideration workloads that are not included in the sample provided, and, more importantly, it does not reflect any business knowledge.

When we run DTA, we select the database(s) for which we want to receive a recommendation and then provide it with a workload to analyze. This workload can be in the form of a SQL trace file (or table) or we can point it at the plan cache, or, if Query Store is implemented, we can run it against that. Any queries that are not contained in the workload provided will not be considered, of course. This means that it is easy for business-critical queries to be missed.

In addition, tools of this nature can simply never have the business knowledge that a skilled DBA or developer will have. For example, imagine that we have a critical month-end process that takes a long time to run and has a short ETL window to run in. If we have provided a SQL trace file that does not cover the night when the month-end process ran, then DTA may recommend we drop an index, which will cause the process to overrun its next window.

Real-life example

The most telling example I have seen of this mistake dates back around 10 years. A new DBA at a company I was working with ran DTA and then implemented all recommendations without reviewing or truly understanding them.

The problem was that he provided the plan cache as a workload a couple of hours after the Database Engine service had been restarted. He was unaware that the plan cache is cleared on an instance restart.

The end result was that he dropped around 90% of the indexes across all databases on the instance. To make matters worse, this was before the days when SQL Server objects were routinely stored in source control. All in all, it was a very long weekend fixing the issue!

DTA can be a very useful tool for getting ideas about how we can improve our index strategy. It has limitations, however, and its recommendations should never be implemented blindly. We should always assess the results and use our business knowledge to determine how its implementation may impact processes that were not captured in the workload that was analyzed.

## 11.11 #77 Not using columnstore indexes

Up until now, we have focused on traditional indexes, which use a B-tree structure to organize data, but what if I told you that there was another type of index called a *columnstore* and that this type of index could provide a 100× performance improvement for data warehouse–style queries? Wow. But what if I then told you that adoption of columnstore indexes is really low among data warehouse developers and DBAs who manage those systems? What’s going on? Surely we would all like our queries to complete 100× faster, right? Well, yes, and that’s why not using them can be a big mistake. Before we dive into why, let’s take a moment to explore what a columnstore index is.

Data storage complexities

SQL Server storage optimizations mean that certain data items may be stored outside of a row. For example, a string more than 8,000 bytes long will be moved to a different allocation unit for overflow values. Also, if features such as filestream are implemented, then data items could even be stored outside of the Database Engine. For the purposes of this illustration, however, we will keep things simple and work with the grossly simplified assumption that all data items are stored within the data pages of a table.

With this rather large caveat in mind, we can say that, in a traditional index, data is organized by storing rows on pages. Therefore, the minimum content of a single data page is an entire row of data. Therefore, the leaf level of a clustered index is rows of data. In the root and intermediate levels of a clustered index and also in all levels of a nonclustered index, the data pages still contain rows of data, but these rows of data simply point to other data pages.

In a *columnstore index*, the organization of data is pivoted. Instead of organizing data into rows, data is organized into columns. Pages are then used to store collections of column values, as opposed to collections of rows.

A columnstore index is split into segments, each containing between 102,400 and 1,048,576 rows. Each segment contains metadata about the data it contains, and this means that the query engine can eliminate (skip) any segments that are not relevant. Within each segment, data is then split into columns, with each column stored on a different set of pages. The index is then compressed quite substantially, which further reduces the number of pages that need to be read to satisfy a query. The difference in how data is organized is illustrated in figure 11.12.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH11_F12_Carter.png)<br>
**Figure 11.12 How data is organized in a columnstore index**

The way columnstore indexes are structured makes them incredibly powerful for data warehouse–style queries. If we create columnstore indexes on fact tables, then queries that require fact aggregation can see a large performance enhancement.

So that brings me back to the question of why adoption is low, and frankly it is a case of first impressions count. When the first version of columnstore indexes was introduced, in SQL Server 2012, they were limited to say the least. Only nonclustered columnstore indexes were supported, and we could only create one per table. They were not supported on indexed views, and we could not use them in combination with change tracking or change data capture.

The biggest limitation, however, was that, once we created a columnstore index on a table, the table became read-only. This meant that we had to drop the index before we could perform any `INSERT`, `UPDATE`, or `DELETE` statements. To compound this issue, columnstore indexes take longer to build than B-tree indexes.

Overall, the first version of columnstore indexes did not work for the vast majority of use cases. The problem is that, despite drastic improvements over the last 12+ years, their reputation has stuck, and as soon as many DBAs think of a columnstore index, their instant reaction is, “They won’t work for me!” This is simply a mistake.

In the modern world of columnstore indexes, there are still some awkward limitations. Advanced data types, such as `HIERARCHYID`, `GEOGRAPHY`, and `GEOMETRY`, are not supported, nor are large data types such as `(N)VARCHAR(MAX)` or `XML`.

> [!TIP]
>
> We are still limited to having a single columnstore index on a table, but because of the nature of columnstore indexes, this makes sense. We can simply include all relevant columns in the single index.

With that said, however, both clustered and nonclustered columnstore indexes are now supported, and if we create a clustered columnstore index, we can still create nonclustered B-tree indexes on the table as well. This means that they have become genuinely useful for analytical and data warehouse workloads, and we should consider using them when we have large fact tables that are subject to large aggregations.

To see what difference a columnstore index can make, let’s compare the performance of an aggregate query with a traditional B-tree clustered index versus a clustered cloumnstore index. The script in the following listing runs a query that calculates the average `BidPrice` and average `CostPerMille` from the `ImpressionsArchive` table in the `MarketingArchive` database.

Listing 11.18 Running an aggregate query

```sql
SET STATISTICS TIME ON ;

SELECT
      AVG(BidPrice)
    , AVG(CostPerMille)
FROM dbo.ImpressionsArchive ;
```

If we examine the execution statistics in the Messages tab of the query results, we can see how long it took for the query to run. On my test rig, the total elapsed time was 1,067 ms. This is fast, but remember that the table is relatively small, for demonstration purposes. In a production table, we may have billions of wide rows.

> [!TIP]
>
> Remember, if you run the query yourself, the speed will vary, based on multiple factors such as the specification of the computer and any other processes that are consuming resources.

The script in the following listing demonstrates how to create a clustered columnstore index on the `ImpressionsArchive` table of the `MarketingArchive` database.

Listing 11.19 Creating a clustered columnstore index with a nonclustered B-tree index

```sql
CREATE CLUSTERED COLUMNSTORE INDEX ImpressionsArchiveCCSI
ON dbo.ImpressionsArchive ;
GO
```

Now let’s rerun the query in listing 11.18 and review the execution time statistics. On my test rig, the total elapsed time was 45 ms. This means that the query completed in just 4% of the time that it took to execute the query against the traditional B-tree clustered index. If you scale this up to queries against very large production tables, it’s easy to see how massive performance gains can be achieved.

The command in the following listing demonstrates how to create a nonclustered B-tree index, which will support queries that filter by `CampaignID` and `CostCode` but also return `BidPrice` in the `SELECT` list.

Listing 11.20 Creating a nonclustered B-tree index

```sql
CREATE NONCLUSTERED INDEX CampaignIDCountryCodeWithBidPrice
ON dbo.ImpressionsArchive(CampaignID, CountryCode)
INCLUDE (BidPrice) ;
GO
```

## Summary

* A B-tree structure is the structure that indexes are organized into. They have a root level consisting of a single page, zero or more intermediate levels, and a single leaf level.
* The leaf level of a clustered index is the actual data pages of the table whereas the leaf level of a nonclustered index contains pointers to the data pages of the table.
* Internal fragmentation refers to a low page density, which causes more pages than necessary to be read.
* Low page density needs to be traded off against the risk of page splits caused by updates to very dense pages.
* Good page splits occur when pages are allocated at the end of an index.
* Bad page splits occur when pages are allocated in the middle of the index and data needs to be moved to the new page. These page splits cause increased I/O and performance penalties.
* External fragmentation refers to pages being out of order, which can damage performance.
* External fragmentation only causes a performance issue for index scans. Index seeks are not impacted.
* An index seek starts at the root level of the B-tree and traverses all levels until it finds the required row.
* An index scan reads the leaf level of an index until it reaches the end of the data it is searching for.
* An index loop uses a nonclustered index to perform a filter or aggregation and then looks up further data from the clustered index or heap.
* Seek, scan, and lookup ratios can be determined by using the `sys.dm_db_index_usage_stats` dynamic management view (DMV).
* Do not reorganize indexes to fix page density. Reorganizing indexes only fills pages up to the level of `FILLFACTOR`. It does not reduce density to the level of `FILLFACTOR`. We should rebuild indexes instead.
* Avoid aggregating fragmentation statistics from `sys.dm_db_index_physical_stats`. Instead, focus on the leaf-level data.
* Not rebuilding indexes will have an impact on index scans and therefore query performance. If a database is 24/7, use online index rebuilds.
* Do not rebuild all indexes indiscriminately. Only rebuild indexes that require it based on fragmentation statistics.
* Do not update statistics after rebuilding indexes, as this can result in worse statistics.
* In the majority of cases, automatically updating statistics is good enough.
* If you decide to update statistics, consider the tradeoff against plan recompilation.
* Depending on the query performance tradeoff against a maintenance window, consider `MAXDOP` for index rebuilds.
* If you are suffering from last page insert contention, consider using `OPTIMIZE_FOR_SEQUENTIAL_KEY`.
* If you need to perform bulk load operations, consider disabling nonclustered indexes on the target table to improve write performance.
* Do not rely too heavily on tools such as Database Engine Tuning Advisor. You can use them as a guide, but you must layer them with your own business knowledge.
* Columnstore indexes organize pages around columns instead of rows.
* Consider using columnstore indexes on large fact tables to improve the performance of analytical queries.
