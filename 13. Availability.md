# 13 Availability

This chapter covers

* The difference between high availability and disaster recovery
* Availability requirements
* Testing disaster recovery
* When AlwaysOn availability groups are appropriate
* Overloading clusters

High availability (HA) and disaster recovery (DR) are key concepts that every database administrator (DBA) should fully understand to make the correct implementation choices and protect their environments from single points of failure. The topic is both broad and deep, and this chapter does not attempt to focus on the actual implementation of HA and DR. Instead, we will focus on some of the key mistakes that DBAs make when planning their HA and DR strategy. For guidance on how to implement HA and DR in SQL Server, I recommend my book *SQL Server 2019 AlwaysOn*, which can be found at <https://mng.bz/znaQ>.

As SQL Server has matured, the scope of concepts and technologies that DBAs need to understand to implement HA and DR has increased dramatically. For example, to implement AlwaysOn availability groups (which we will now refer to as just availability groups), DBAs need to have an understanding of Windows clustering. Even if the implementation is a collaboration with a Windows team, DBAs must still understand the concepts.

In this chapter, we will first focus on the concepts of HA and DR, as I still find a reasonable number of DBAs get these confused, which can lead to risk and ultimately outage of critical business applications. We will then discuss the implications of not architecting a HA/DR strategy against the business requirements.

We will explore the risks associated with not testing our DR strategy, which is a surprisingly common reality. We will then discuss why availability groups are not always the correct solution, which will prevent a common pitfall. Finally, we will discuss the consequences of overloading a cluster and how this can lead to extended downtime in the event of an incident.

## 13.1 #89 Confusing HA and DR

*Disaster recovery (DR)* refers to the mechanism used to recover a database in the event of a critical incident. This incident may range from a corrupt database to a dead server. It could even be the loss of a whole data center or cloud availability zone. The most basic form of DR is a backup–restore strategy, but many critical applications require a solution that provides faster recovery in the event of a disaster. This is achieved through redundant hardware, usually in a different geographical location, which is kept synchronized with the production system. This is known as a *warm standby*.

*High availability (HA)*, on the other hand, refers to the mechanism used for a database to automatically fail over and recover itself in the event of an issue on the production server. This also requires redundant hardware, but that hardware is often stored in the same location as the production server. The close proximity reduces network latency during the synchronization and also any client-to-server latency after failover has occurred. The redundant infrastructure in this scenario is called *hot standby*.

The mistake that I see some DBAs make is confusing these two concepts. While they are similar and have some overlap, they serve different use cases. Therefore, I have seen DBAs configuring an HA topology and then telling the business that they have implemented DR. As long as they are taking backups of the databases involved in the HA topology and storing them off-site, what they have said is technically true, but it misleads the business.

Why does this matter? The answer is that the business is left with the misconception that it can quickly recover from a far wider range of failures than is actually the case. The perfect example is the loss of a data center. This can happen for many reasons, including loss of power, a bad network change to a core switch, or even a natural disaster.

The diagram in figure 13.1 illustrates an HA topology configured using availability groups. The topology has two servers, and each is located in a different server rack in the same data center. This means that the server, the rack, and the top of rack switch have been removed as single points of failure. This has led the DBA to believe that DR is in place.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F01_Carter.png)<br>
**Figure 13.1 HA topology with availability groups providing automatic failover in a single site**

The trouble with citing this topology as DR is that there are still many single points of failure, and it is not possible to recover from all failure scenarios. For example, imagine that there is a power outage to the data center or that the data center floods. Depending on the data center topology, the loss of a critical networking component, such as a core switch, could even bring down the whole environment, leaving the DBA with a need for a server to be spun up in a different data center or cloud environment, ready to restore from the latest backup.

Now let’s consider the topology illustrated in figure 13.2. This is a DR topology that does not offer HA. In other words, the redundant server is in a second data center and allows us to quickly recover service in the event of a disaster. It is not providing HA, however. The definition of HA is there will be automatic failover, and this design relies on a manual failover process. In this configuration, the cluster is known as a *multisubnet failover cluster* because it will have virtual IPs associated with each subnet, to allow failover between the sites.

> [!TIP]
>
> A subnet is a division within a larger network. Typically, each site will have its own subnet(s). This reduces the amount of traffic sent across the WAN link, because it avoids network traffic such as broadcasts being sent between sites.

So why don’t we simply configure HA across two data centers and kill two birds with one stone? In some environments, where there is a very fast low latency link between the two sites, this may be possible. Generally speaking, however, the challenge is how the redundant infrastructure is synchronized.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F02_Carter.png)<br>
**Figure 13.2 DR topology with availability groups providing manual failover between sites**

Technologies like availability groups replicate data synchronously when used to provide HA. This means that no data can be lost if an unexpected failure occurs. It also means, however, that every write must complete on the secondary server before it completes on the primary server. If the servers are not close to each other, network latency will increase, and this will have a damaging effect on performance.

Availability groups vs. traditional clustering

If traditional clustering is used to provide HA, then instead of synchronizing the data, only one copy of the data is held, usually on a storage area network (SAN). This means there is no network latency involved in synchronizing the data and hence no performance issue.

This also means that the storage introduces more single points of failure into the topology, however—for example, the network connection to the SAN, the storage controller, and the storage itself. These single points of failure are usually on paper only, however. In reality, a cluster will usually have a resilient network path to the SAN. The SAN will have multiple nodes and the storage itself will be configured in a RAID array.

The real benefit of availability groups over traditional clustering is its flexibility. It can be used to implement HA, DR, and even read scaling. A traditional cluster can accommodate HA only, because it is in a single data center. While it is possible to configure a geographically dispersed cluster that relies on SAN replication cross-site; this is often not a supportable solution because of the number of complexities when a failover is required.

Availability groups also allow for a more granular implementation. Clustering is implemented at the instance level, whereas with availability groups, the implementation is at the database level. This means that it is possible to configure just the most important databases with HA to reduce the overhead.

When used to provide DR, technologies like availability groups will instead replicate the data asynchronously. This means each write is completed on the primary server and then replicated to the secondary. Because the primary server is not waiting for confirmation of write completion, we can avoid the performance issues caused by network latency. It also means there is the risk of a small amount of data loss. How much data loss occurs will depend on network performance as well as the performance of the servers. If the servers are undersized, there is the risk of a queue of transactions building up, waiting to be synchronized.

Many critical systems will require both HA and DR to be configured. This can be achieved in a variety of different ways. Technologies can be mixed, or we can simply use availability groups for the whole implementation. For example, consider the topology in figure 13.3. This illustrates a topology where availability groups have been used to implement both HA and DR.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F03_Carter.png)<br>
**Figure 13.3 HA/DR topology using availability groups providing local automatic failover and cross- site manual failover**

It is important that DBAs understand the difference between HA and DR. Failure to understand these concepts can lead to a situation where the business thinks it has a higher level of protection than it actually does.

## 13.2 #90 Failing to architect for the requirements

To determine the correct HA/DR architecture to build for an application, we must understand the availability requirements. The trouble is that if we ask an application owner, “How long can your application be down?” the answer will invariably be “It can’t!”

On the surface, this is fine. In theory, we could build a full HA/DR topology for every single application in our organization. The problem with this approach is that it quickly becomes very expensive. If we are lucky enough to have a solution architect and a business analyst involved in the project, then this is less of an issue because they will be responsible for understanding the requirements and defining the appropriate solution. As DBAs, we will simply implement their design.

Unfortunately, we are not always lucky enough to have access to architectural or analysis resources. This means that DBAs sometimes need to fill this gap. A common mistake I see DBAs make in this scenario is to simply accept what an application owner states as their availability requirements. When we rely on an application owner to make this decision in isolation, however, they often do not understand the implications of their request.

To understand this issue, we need to understand levels of availability. You may have heard people state that they have a requirement for four 9s or five 9s of availability, but what does this mean in practice?

The level of availability refers to the percentage of time that an application is available to users. The number of 9s in the availability requirement refers to the number of 9s in the percentage of availability. For example, four 9s means that an application is available 99.99% of the time, and five 9s means that an application is available 99.999% of the time.

The amount of acceptable downtime for each of these availability levels is detailed in table 13.1.

Table 13.1 Levels of availability

| Level of availability | Downtime per week | Downtime per month |
| --- | --- | --- |
| 99% | 1 hour, 40 minutes, 48 seconds | 7 hours, 18 minutes, 17 seconds |
| 99.9% | 10 minutes, 4 seconds | 43 minutes, 49 seconds |
| 99.99% | 1 minute | 4 minutes, 23 seconds |
| 99.999% | 6 seconds | 26 seconds |

When reviewed in these literal terms, we can see that five 9s of availability is not easy to achieve. A 6-second outage could be caused by something as simple as a network blip. To add some additional context, if we were to spin up a server in a major public cloud platform such as Azure, we would only receive a service level agreement for a stand-alone server of 95% uptime per month at the time of writing.

To achieve a full five 9s of availability, we need to spend a large amount of money on both local and geographically dispersed redundant infrastructure, with redundant private links between sites. This simply isn’t cost effective for the majority of use cases.

To really understand the requirements, we need to understand the cost of downtime for the application. This requires the application owner to understand both the tangible and intangible costs of an application not being available. Tangible costs are quite straightforward to calculate. For example, if we have a sales application, the tangible cost is lost revenue because customers cannot place orders. The hourly cost can be calculated by taking the monthly sales figures, multiplying them by 12, and then dividing that number by 8,760 (the number of hours in a year).

Intangible costs are much more difficult to quantify but can actually be higher. For example, if a customer is unable to place an order, they may place their order with a rival company and never return. Other intangible costs may include a drop in the sales team morale, leading to higher staff turnover or even a loss of company reputation. In some situations, it could even lead to regulatory noncompliance. Because intangible costs can only be estimated, the industry rule of thumb is to multiply the tangible costs by 3.

Once an hourly cost of downtime has been calculated, this number can be expanded out to the full lifecycle of the application. This number can then be compared against the cost of the infrastructure required to meet the requirement for each level of availability. For example, imagine that an application owner has determined that the cost of downtime for an application is $1,000/hour and the application has an expected lifecycle of three years. The total cost of downtime for two 9s of availability would be $87,650. This can be calculated using the following formula.

(Downtime hours per year × Years in application lifecycle) × Downtime cost per hour

We can then compare this against the cost of the infrastructure needed to support the requirement. An example of this can be found in table 13.2. The cost of downtime assumes $1,000/hour.

> [!NOTE]
>
> The cost of the solution just has example costs. This will, of course, be different for every application.

Table 13.2 Comparing the cost of downtime

| Level of availability | Total cost of downtime | Cost of HA/DR solution | Cost of downtime + HA/DR solution |
| --- | --- | --- | --- |
| 99% | $262,810 | $0* | $262,810 |
| 99.9% | $26,290 | $89,193 | $115,483 |
| 99.99% | $2,630 | $100,389 | $103,019 |
| 99.999% | $259 | $301,167 | $301,696 |

* Can be achieved without resilient architecture, provided backups are available

In this example, the cost of achieving five 9s of availability is $301,696. This is more than the cost of downtime with no redundant infrastructure, which is $262,810. Therefore, it is not economical. Achieving four 9s would achieve a cost saving of $159,791 over having no redundant infrastructure ($262,810 – $103,019). Achieving three 9s would only produce a cost saving of $147,327 over having no redundant infrastructure ($262,810 – $115,483). Therefore, achieving four 9s would be the most appropriate level of availability to aim for in this scenario. When we present an application owner with the financial implications of their requirements, they are able to understand why zero downtime is not a reasonable ask.

On premises, the best design for achieving our required level of availability will depend on our organization’s infrastructure, and we should discuss this with our infrastructure team. Usually, however, achieving four 9s of availability will require a cross-site HA/DR topology similar to the design shown in figure 13.3.

In cloud, each cloud provider offers different service level agreements. At the time of writing, however, in Azure, four 9s of availability requires two VMs split across two availability zones for an infrastructure as a service (IaaS) solution or an Azure SQL database configured at the Business Critical or Premium tiers.

Before configuring an HA/DR topology, it is very important we understand the requirements. We need to accept that if we do not have access to a solution architect or business analyst, we will need to support the business in understanding what the requirements are for a given application. We should always encourage application owners to base their requirements on cost versus benefit.

## 13.3 #91 Not testing the DR strategy

A very common mistake that accidental DBAs make is to never test their DR strategy. The rationale is that they can see the data is synchronized. What could possibly go wrong? Unfortunately, the answer to this question is that there is plenty that can go wrong, and it is always better to find this out during a controlled test, rather than at 2 a.m. when it needs to happen in a real recovery scenario.

Let’s start by thinking about some of the everyday issues that can happen during failover. The most common of these is caused by the performance of the secondary node. Imagine that we fail over an availability group from a primary replica to a secondary replica in a different data center. The failover works, but users instantly start complaining that performance is so poor that they cannot work. This can be caused by a number of factors, ranging from network latency to the secondary site to the secondary replica simply being too low-spec to handle the user throughput.

Another common reason for an inability to fail over is permissions. For failover to be successful, the `SYSTEM` account must have the `VIEW SERVER STATE`, `CONNECT SQL`, and `ALTER ANY AVAILABILITY GROUP` permissions on the secondary replica. I have seen a couple of examples where security-conscious DBAs have removed permissions from the `SYSTEM` account, unaware of the implications for availability groups.

The third most common issue I have seen involves encryption. If `FORCE PROTOCOL ENCRYPTION` is configured on the secondary replica but the secondary replica is not configured for encryption, health monitoring on the secondary replica will be prevented from connecting to the local replica, which is part of the failover process.

This is just a small sample of issues that can occur when we attempt to failover availability groups, and of course, other HA/DR technologies, such as clustering and log shipping, all have their own unique set of issues that can occur. In chapter 12, we mentioned the adage that “You don’t have a backup until you have restored it.” There is a similar philosophy that “You don’t have DR until you have failed over.”

Therefore, it is important that we test our DR strategy on a regular basis. Depending on the number of applications we support and the policy of our organization, the frequency of this test will vary. It is typical, however, to assume that every application has a DR test once a year. Usually, it is sensible to run this as a rolling task, testing a small number of applications every week or month. This allows us to build it into our standard business-as-usual routines.

Organizing a DR test involves arranging a downtime window with the application owner. We should be sure to arrange a window that is longer than we are expecting the failover to take, in case there are any problems that need to be addressed. It is also important to ensure that we have one or more users available to test that they can connect to the application and that it is functioning with tolerable performance during the test. Depending on the technologies we use and the skills within the DBA team, we may also need to arrange for other teams to be on hand for support. For example, if we do not have Windows skills in the team, and we use either availability groups or failover clustering as our HA/DR technology, then we may want to have a Windows engineer on hand for support. Alternatively, if we have a geo-cluster, then we will likely need a SAN engineer on hand to perform the storage failover, as well as provide support with any required troubleshooting.

Performing DR testing may feel like a chore, both to DBAs and to the business, but it is vitally important. It is much better to discover that we cannot fail over an application during a controlled test with scheduled downtime than it is to discover we cannot fail over in the middle of a P1 incident.

## 13.4 #92 Assuming availability groups are always the right answer

Since their introduction, availability groups have become the go-to HA/DR technology for all DBAs. This leads to a common misconception, however, that availability groups are always the right choice of technology for all data-tier applications. This is a mistake that can cause issues if we do not understand the limitations of availability groups and the alternatives available.

Imagine that we have a large data warehouse that processes a very large volume of data every night. The reporting the data warehouse supports is business critical, and it is vital that we have high availability in place. We configure availability groups in synchronous mode with automatic failover. Unfortunately, the application team complains that the extract, transform, and load is no longer completing within the window.

After investigation, we realize that the synchronous commit between the replicas is likely the cause of the issue. The very large volume of transactions that happens during the ETL run is causing a queue of commits to build up on the secondary replica, and the primary replica cannot commit a transaction until it has been notified that the commit has succeeded on the secondary. This is the process that ensures no data loss can occur. So to resolve the issue, we change the configuration of the availability group to use asynchronous commit. Of course, this means that we must also change to manual failover. This does not meet the requirement for automatic failover, but we hope that it will be a middle ground that will be good enough.

Unfortunately, the application team reports that, while the ETL performance is significantly better, it is still not completing within the ETL window, so we have to go back and investigate what else could be causing the problem.

After further investigation, we realize that, to implement availability groups, we have had to change the recovery model of the data warehouse database from `SIMPLE` to `FULL`. This is causing significantly more I/O, because every transaction is fully logged. There is no way around this. Either we accept that the ETL process will run for significantly longer, which does not meet the needs of the business, or we accept that we don’t have an HA or even a DR solution, which also does not meet the business requirements.

This is a great example of where availability groups are not the correct HA/DR technology for a database, and it is an example that I have come across on more than one occasion. In this scenario, to meet the business requirements, failover clustering is the best technology to use. This is because it only stores the data once, and in the event of a failover, it detaches the storage from the failed node and attaches it to a redundant node. This is illustrated in figure 13.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F04_Carter.png)<br>
**Figure 13.4 HA with failover clustering providing automatic failover in a single site**

Failover clustering is not fashionable in the industry, but if you have on-premises data warehouses or other applications with a significant write volume, it may still be the best option instead of availability groups. Because the data does not need to be synchronized, there is no need to use `FULL` recovery model, and therefore we can avoid fully logging the transaction.

> [!TIP]
>
> Further discussion around the choice of recovery model can be found in chapter 12.

If we have a data warehouse that we plan to host in cloud and we need HA, a good candidate may be a dedicated SQL pool in Azure Synapse. This technology, which was previously called Parallel Data Warehouse, uses massively parallel processing technology to implement distributed queries. In this architecture, queries are run against a control node, which then distributes the query to a compute node. The data is stored in Azure storage, and there is a data-movement service that coordinates data movement between compute nodes when this is required to satisfy a query. This is illustrated in figure 13.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F05_Carter.png)<br>
**Figure 13.5 Azure Synapse dedicated SQL pool**

Should we ever use log shipping or replication?

Log shipping is an old method for achieving DR, which works on the premise of backing up transaction logs, shipping them to a secondary server, and restoring them to a database. Just like availability groups, it requires a database to be in `FULL` recovery model. It has very few of the benefits, however. It can only be used for DR, and the secondary database cannot be brought online until the point of failover. This means that it cannot be used for read scaling or offloading administrative tasks. Additionally, after a failover event, if we do not wish to fail back, then we have to reconfigure log shipping in the other direction. After failover, applications must also be repointed to the secondary instance, as the solution does not use clustering.

There is only one use case where it is still appropriate to use log shipping, although I have not done so myself for some time. This is a scenario where the business wants a delay before the data is applied at the secondary database so that it has a window to undo an accidental action by a user.

This use case cannot be catered for in availability groups, but in log shipping it is as easy as increasing the frequency at which the logs are reapplied at the secondary database.

Replication is a technology that can be used to disperse data. There are multiple different types of replication, such as transitional, merge, and snapshot. Many years ago, before technologies like availability groups were introduced, some people used it as a DR tool to get around some of the limitations of log shipping. Not only is this not what replication was designed for, but the tool is highly complex and often has a high operational overhead to keep it reliable. In the modern era, there are no use cases where replication is a suitable tool for DR.

When we implement an HA/DR solution, it is important that we choose the right technology for the job. There is no doubt about it: availability groups are usually the right technology. This is not true in all scenarios, however, and we must understand their limitations and when we should steer away from them. If we have a large data warehouse, or even an OLTP application with a massive volume of writes, then availability groups may not be the right choice—especially if HA is required. In this scenario, it is still appropriate to consider failover clustering. We may also decide to move to an appropriate cloud technology, such as Azure Synapse dedicated SQL pool.

## 13.5 #93 Overloading a cluster

Most organizations are cost conscious, and redundant hardware costs money. Therefore, it is a common practice to use an active/active cluster for failover clustering topologies and availability group topologies alike. In an active/active cluster, all nodes or replicas host production databases. If any node in the cluster fails, then the databases hosted on that node fail over to one of the other nodes. This is illustrated in figure 13.6, which depicts a two-node cluster in which each replica hosts a production availability group.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH13_F06_Carter.png)<br>
**Figure 13.6 Active/active cluster using availability groups providing automatic failover in a single site**

Active/active is not load balancing

It is important to understand that an active/active cluster is not a load-balancing solution. Only one copy of any given database can be active for writes at the same time. The only technology that allows us to use any form of load balancing (for write activity) is Azure Synapse dedicated SQL pool. This implements a massively parallel processing technology to coordinate transactions and synchronize data.

You may also hear that a technology called peer-to-peer replication can be used as a load-balancing solution, but this is not quite correct. Peer-to-peer replication is built on transactional replication, and multiple versions of the database can be updated.

There is no cluster or controller in front of the databases, however; an application must connect directly to one of the nodes. Additionally, conflict handling is rudimentary at best. If a conflict occurs, then the distribution agent will stop and an alert will be raised. A DBA must then either reinitialize the replica or manually synchronize the changes. In reality, this means that the technology can only practically be used for updating different areas of the database in most cases.

The design in figure 13.6 allows for the load to be spread across multiple nodes in the cluster. The result is that all infrastructure is utilized while still maintaining the ability to fail over. This comes with an accepted risk that performance may be impeded in the event of a failover event but that it will only be a short-term issue, as a DBA will fail the databases back to their original node as soon as the issue is addressed. It provides a good tradeoff of reliability versus cost optimization.

The mistake that I sometimes see is that the load in this topology then grows. Sometimes it is because of organic growth in an application, and other times it is because more availability groups or failover clustered instances are added to each node.

This can result in a situation where, in the event of a failover, the node that is now the primary node for all databases simply doesn’t have enough capacity to deal with all the requests. This can result in poor performance, timeouts, and loss of monitoring; I even saw one instance with a failover cluster where Windows was simply not able to bring the failed-over instance online at all.

As DBAs, it is critical that we avoid this scenario, and there are a couple of methods that can work together to achieve this. The first method is simply performing capacity planning for data-tier applications. We should do this before an application goes live, but then we should also proactively monitor trends to ensure that we have enough capacity on a single node to be able to run all applications—even if they are running hot. You can find further discussion of capacity planning in chapter 9.

The second method is ensuring that DR tests are regularly carried out. This gives us the opportunity to rehearse a failover event and ensure that performance is still acceptable to the business. This is discussed in more detail in section 13.3.

## Summary

* High availability (HA) refers to the automatic failover of a database in the event of a failure.
* Disaster recovery (DR) refers to the ability to recover a database in the event of a major incident.
* HA is usually configured between servers that are physically close and have low latency.
* DR is usually configured between servers that are geographically dispersed so that an application can withstand the loss of a data center.
* An application’s level of availability refers to the percentage of time that it is available for use. This is often shortened to be reflected as a “number of 9s.”
* Ensure that an HA/DR solution is cost effective. The cost of the redundant solution should never be more than the cost of the application downtime.
* Always test a DR strategy. Just because a database is synchronized does not mean that we can actually fail over.
* It is common practice to test the DR strategy for every application on an annual basis. This is best served through a rolling schedule of DR tests throughout the year.
* When you test DR, involve users so that they can test that they have acceptable performance after failing over.
* Availability groups are an excellent, highly flexible technology, but they are not always the correct answer.
* Failover clustering is unfashionable in today’s world, but do not be afraid to use it if it is the correct technology for a use case.
* Availability groups may not be the correct technology when a database has a very high volume of writes and HA is a requirement.
* Availability groups also may not be suitable for large data warehouses, even with asynchronous replication.
* Avoid overloading a cluster, as this can lead to issues when you fail over. You can avoid this by ensuring you undertake capacity planning and DR testing.
