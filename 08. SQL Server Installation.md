# 8 SQL Server installation

This chapter covers

* Meaningful instance names
* Using the correct OS and SQL Server edition
* Automated installation
* SQL Server’s footprint
* Installation considerations in cloud

In this chapter, we will discuss the large array of options that we have for installing SQL Server. If we go back a decade, planning a SQL Server deployment was comparatively straightforward. We had to choose the most appropriate edition of SQL Server to install, and we had to think about how we would install it (GUI vs. script). We needed to consider the features required for the instance and give it an appropriate name, but that was about all.

Fast forward to the present day and the possibilities for SQL Server installation are endless. This can lead to a raft of mistakes, many of which stem from performing traditional installations without considering more optimal alternatives.

In this chapter, we will install a series of SQL Server instances for MagicChoc and explore some of the pitfalls that can await the accidental database administrator (DBA). This will cover multiple aspects of planning and executing deployments, ranging from instance naming to selecting the appropriate operating system environment and SQL Server edition. We will also explore the challenges with manual installations, installing features that are not required, and overconsolidating instances.

## 8.1 #33 Using obscure instance names

In chapter 2, we discussed naming standards and the consequences that can arise from using object names that have not been well thought out. We can have similar issues with SQL Server instances if we do not think through the instance names. To explore this, let’s imagine that MagicChoc has asked us to build four SQL Server instances, split across two servers, which will host databases used by the following applications:

* Choc Maker, which is used by manufacturing
* Magic Sales, a sales team application
* Temperature Smart, an application used in the manufacturing process
* HR Manager, a tool used by the human resources team

There are four applications and four instances, so we decide to create four instances across the two servers and give them names that are aligned to the application names:

* MagicServer1\ChocMaker
* MagicServer1\MagicSales
* MagicServer2\TempSmart
* MagicServer2\HRManager

At the time, this sounds perfectly sensible. A few months later, however, we realize it was a mistake. The database lead asks us to consolidate the Choc Maker and Temperature Smart applications into a single instance.

We now have three options. We can either have Choc Maker running in an instance called `TempSmart`, which will likely cause a degree of confusion in the future; we can have Temperature Smart running in an instance called `ChocMaker`, which will cause an equal amount of confusion; or we can rename the SQL Server instance as `Manufacturing`, which can generate a heap of extra work, depending on how applications are accessing the instance and the features being used.

So am I recommending naming SQL Server instances by department rather than application? No, I’m certainly not. This could come with its own set of challenges, such as having multiple instances with the same name on different servers—or worse, needing to have two instances with the same name on the same server (which isn’t possible). There is also the risk of a department changing its name as part of a restructure, of course. For example, the Sales department could split into two departments, called Sales and Marketing, respectively.

How we should name instances is specific to our organization and the way the SQL Server estate is structured. For example, as well as naming an instance after an application or department, it could also be named after a business service.

It may also be a requirement to have a naming convention, which includes various aspects within the name. As an example of this, consider the instance name `MAPRDENTLON01`. This instance name reflects the department with a two-letter code, `MA` = Manufacturing. It reflects the ecosystem with a three-letter code—in this case, `PRD` = Production. The next three letters specify the license applied to the instance. In this example, `ENT` = Enterprise Edition. The next three characters in our name identify the location on the instance: `LON` = London. Finally, there is a two-digit incrementing identifier.

In short, my recommendation is simply to give due consideration to instance names and name them appropriately and in a way that will not cause confusion in the future. The naming standard should be consistent across the estate.

## 8.2 #34 Using Windows indiscriminately

Until 2017, SQL Server could only be run on Windows. This makes a lot of sense in the context of both products being developed by Microsoft. In SQL Server 2017, however, there was a dramatic shift, with Microsoft introducing support for SQL Server to run on Ubuntu, SUSE, and Red Hat flavors of Linux. It is even supported to run in Docker containers. But doesn’t this seem a little odd? Surely, we would only ever want to run SQL Server on Windows, right? Well, let’s explore a scenario with MagicChoc.

In chapter 4, we discussed how MagicChoc felt that its processes were too fragmented, so it commissioned a new application to combine its non-internet sales and procurement functions into a single interface with a single backend. What we didn’t discuss is that this application will be written in Go and will be hosted on VMs running Ubuntu. Performance and stability are of paramount importance for the application.

The application team had originally planned to use MySQL, but MagicChoc’s DBA team was not experienced in MySQL. They also wanted to use advanced SQL Server features, such as AlwaysOn, and the application team realized that they could benefit from other advanced T-SQL features, such as `HIERARCHYID` data type and windowing functions.

Therefore, it was decided that the DBA team would build SQL Server instances on VMs running Windows Server 2022. But for this scenario, that was a mistake. The clues as to why this was a mistake are all in the case study.

First, the DBA team did not want to support MySQL due to a lack of skills in that area, but at the same time they expected Linux-based developers to switch to a Windows environment for the Database Engine. The barrier to entry would have been lower if SQL Server were hosted on Linux.

The second reason is that the application was being built on Ubuntu. This is a license-free distribution of Linux, which implies that cost is a big factor. Many enterprise applications are built on Red Hat or SUSE distributions, as these have a license cost but also external support. If SQL Server were installed on Ubuntu rather than Windows, then the Windows Server license cost could have been avoided.

The final reason is the requirement for the application to be fast and stable. While SQL Server boasts both of these requirements on any operating system in its own right, they are somewhat enhanced in Linux due to the nature of the operating system.

Linux is a very lightweight operating system. This means that fewer things can go wrong. Even though Windows stability has drastically increased over the last decade, Linux distributions still lead the way in this regard. The lightweight nature can also offer performance advantages. In 2021, official benchmarks rated Red Hat Linux as the most performant operating system for SQL Server.

Of course, all these benefits in this scenario do not mean that Linux should be a default choice for SQL Server installation. It does mean that when architecting the platform environment for the data tier of an application that uses SQL Server, Linux should be considered. There are occasions where Linux is the optimal choice, but in many situations, Windows will still be a better option.

For example, if the application is a Windows-based application, then we would only be adding complexity by introducing a Linux server. We may also be reducing security by having to enable SQL Server authentication, instead of relying solely on Windows authentication.

> [!NOTE]
>
> It is possible to enroll Linux servers in Active Directory (AD), but this is an atypical configuration.

Depending on our organization, we may be lacking Linux skills. If this is the case, then we are unlikely to want to install a complex product like SQL Server on an OS we will struggle to support.

There are also a few feature limitations in Linux. For example, SQL Server Reporting Services is not supported. Other graphical tools, such as SQL Server Management Studio (SSMS), are not supported on the Linux server, but we can install that on a Windows device and connect to the instance hosted on Linux like we could for any other instance.

A possible decision tree that can be worked through is shown in figure 8.1.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH08_F01_Carter.png)<br>
**Figure 8.1 Windows vs. Linux decision tree**

In brief, SQL Server’s hosting environment should be carefully considered when planning an installation. We should consider the nature and requirements of the application tier, the technical requirements for SQL Server, operating system license costs, the support model, and the skills of the team before making a decision. Installation on Linux should always be in the back of our mind, however.

> [!TIP]
>
> The installation of SQL Server on Linux is beyond the scope of this book. Microsoft has a walk-through at <https://mng.bz/aVO7>. Alternatively, you can find guidance on this as well as a walk-through in my book *Pro SQL Server 2022 Administration*, which can be found at <https://mng.bz/gA9V>.

## 8.3 #35 Forgetting how useful containers can be

When I first started working with SQL Server, it was always installed on a physical server, or “bare metal.” Over time, VMs became increasingly popular, and although SQL Server was a little late to the party, it eventually became the norm to install SQL Server on VMs. In more recent years, containers have become ever more popular, and SQL Server has been supported on containers since SQL Server 2017.

Unlike a VM, which virtualizes hardware, a container virtualizes the kernel of the operating system. This makes containers portable, isolated, and lightweight. Containers bring many advantages, such as their use in microservice applications, and make it very simple to deploy applications, especially in DevOps scenarios.

Despite Microsoft arriving at the container party in surprisingly good time, a constant mistake I see among DBAs is a failure to adopt them. This is often caused by a lack of understanding and a failure to see their value for SQL Server. With that in mind, let’s take a look at a MagicChoc scenario.

MagicChoc has just signed a deal with a consultancy to build a new, integrated application for managing the production process. It’s a large project that is forecast to take over a year to complete, and the project team will consist of 15 developers and 5 testers. Each member of the project team needs a dedicated SQL Server instance.

Some members of the project team use Windows laptops, but most of them use Macs. Therefore, they can’t just install SQL Server on their laptops. One of the developers mutters something about using containers, but the DBAs shut that conversation down, because containers are stateless, so the developers would not be able to save their work. Instead, the DBA asked the Windows team to spin up 15 VMs. Unfortunately, there was not enough capacity left on the VMware cluster to do this. Instead, they spun up 1 large VM, and the DBA installed 20 instances of SQL Server.

This scenario is fairly typical of conversations that happen in the IT departments of large organizations, but unfortunately, it is riddled with misconceptions and mistakes. The most optimal answer to this scenario would have been to use containers. Let’s explore the misconceptions in a little more detail.

The DBA team was reluctant to use containers because they are stateless, and therefore, they thought that the developers would not be able to save their work. While it is absolutely true that containers are stateless, the assumption that the developers could not have saved their work is not. When using SQL Server in a container, it is imperative that data is persisted. Otherwise, when the container is removed, everything inside the container, including the databases and code, will be lost.

To persist data outside of a container, we will use a *Docker volume,* which is a Docker-managed resource that points to a file system location outside of the container. The built-in driver is `local`. Simply, this means that the SQL Server instance will be hosted inside the container, but the Docker volume exists on the host. We can create a Docker volume called `sqldata` using the command in the following listing.

Listing 8.1 Creating a Docker volume

```sql
sudo docker volume create sqldata
```

> [!TIP]
>
> Other driver plugins are available for Docker. For example, Flocker enables us to create Docker volumes on external storage such as Elastic Block Storage in AWS.

Once the Docker volume has been created, we can then use the `-v` switch when creating the container to map the internal file system to the external volume, as shown in listing 8.2. This will create a container called `production` and map the folder where the system databases are stored to the `sqldata` Docker volume.

> [!NOTE]
>
> This example assumes that Docker is installed and running and that you have downloaded the default SQL Server 2022 container image.

Listing 8.2 Creating a SQL Server container using a Docker volume

```sql
sudo docker run -e "ACCEPT_EULA=Y" \
-e "MSSQL_SA_PASSWORD=Pa$$w0rd" \
-p 1433:1433 \
--name production \
--hostname production \
-v sqldata:/var/opt/mssql \
-d mcr.microsoft.com/mssql/server:2022-latest
```

The alternative is to directly mount a host directory as a data volume inside the container. Using Docker volumes has several advantages, however. When we use a Docker volume, then we can manage it using the Docker CLI. This makes volumes easier to migrate or share between containers. Unlike mounting a host directory, Docker volumes have the additional advantage of not being dependent on the directory structure of the host.

Containers certainly have a place in the life of any modern DBA. They provide process and user isolation for applications and can dramatically reduce the deployment and management overhead of development environments for large teams. Data can be persisted by being stored outside of the container, despite the SQL Server engine running inside the container. This means that the data can survive a container being removed.

## 8.4 #36 Using Desktop Experience unnecessarily

Previously in this chapter, we discussed why we should consider Linux as a potential hosting platform when planning an installation of SQL Server. At the same time, however, if we decide that Windows is definitely required, then we should consider if we really need a GUI. Many times, the answer will be no, and if this is the case, then SQL Server can be installed on Windows Server Core.

To explore this, let’s look at another MagicChoc requirement. In chapter 6, we created a `Marketing` database. We also built an SSIS package that would populate the `Impressions` data within this database. The business intelligence developers have no experience with Linux and are reluctant to provide application support on this platform. Therefore, the DBA team has decided that the SQL Server instance should be built on Windows Server 2022.

A VM is built, and they use the installation wizard to install a default instance of SQL Server. They made this decision because SSIS packages are developed in a GUI using Visual Studio. This was a mistake, but why?

Windows Server Core is Windows Server but without the GUI. Instead of a GUI, the server is administered using PowerShell. Removing the GUI removes much of the bloat of Windows, making it a more lightweight, secure operating system. It consumes less disk space, processor, and memory than the Desktop Experience, meaning it is more efficient, and because there are fewer components and services running, the attack surface is reduced. In other words, there are fewer features for a bad actor to attack. These characteristics make it a great operating system for running SQL Server.

But the developers will be using Visual Studio to develop the packages, and Visual Studio can’t run on Server Core, so surely we need a GUI in this scenario? Well, that is not correct; this also highlights another “mini mistake.” Specifically, quite a few administrators think that developers require Visual Studio to be installed on servers that run SSIS or other elements of the business intelligence stack in production environments. In fact, this should be avoided. There is no technical justification for it. Integrated development environments (IDEs) should ideally run on developers’ workstations or in rare cases on a development server.

IDEs on development servers

The ideal scenario is for developers to use IDEs exclusively on their workstations and connect the IDEs to the databases hosted on the development server. I have encountered scenarios where this is not practical, however.

One specific scenario that I encountered was when I was developing SSIS packages for a very large data warehouse. I had to develop against a full dataset, because some of the transformations required multiple months of data to produce meaningful results, and the business analysts were not able to construct a sample dataset that reflected the business rules.

The data was so vast that my workstation simply didn’t have enough RAM to run the packages efficiently. Additionally, I was frequently working remotely, and pulling that amount of data over the VPN with a slow internet connection was not working out well.

Therefore, to develop the packages in a timely manner, I had to use Remote Desktop Protocol to connect to the development server and run the packages locally. This is very much the exception rather than the rule, however.

Allowing developers to have an IDE installed on a production server increases the attack surface of the production environment, but it also gives developers a back door into production. This back door leaves administrators with the risk of developers performing actions, or even releasing code, without the administrator’s knowledge, which puts the service at risk and makes the server impossible to support. Organizations may also find that they fall foul of regulatory requirements that ensure proper separation of duties.

With this mini-mistake in mind, there is actually no requirement for an operating system in our scenario, and administrators should have built the SQL Server instance on a server running Windows Server Core.

**Figure 8.2 expands on figure 8.1 to add Windows Server Core into the decision tree.**

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH08_F02_Carter.png)<br>
**Figure 8.2 Decision tree including Server Core**

Is installing SQL Server on Windows Server Desktop Experience always a mistake? No; it is the right choice in some scenarios. The mistake is installing SQL Server on Windows Server with Desktop Experience because it’s “what we have always done.” Installing SQL Server on Windows Server Core should always be considered and only disregarded if there are legitimate reasons. Those legitimate reasons are a requirement for graphical features, such as when there is a requirement for SQL Server Data Quality Services (DQS) or Master Data Services.

## 8.5 #37 Using Enterprise Edition indiscriminately

SQL Server is expensive. To make SQL Server more accessible and reduce the cost to customers, Microsoft offers multiple editions of the product, each with different features and license constraints. For example, Express edition is free, but it has significant technical restrictions such as a maximum 10 GB database size and a maximum compute capacity of one socket or four cores. There are also many feature restrictions. Developer edition, on the other hand, is also free, but it has the full set of features supported by Enterprise edition. The only restriction is that the license does not permit use in a production environment.

MagicChoc is trying to determine the correct edition of SQL Server to use to host the data-tier portion of their Marketing application. It has decided to host it on Enterprise edition, because it is a large 2 TB database, it needs to be hosted on an availability group with a synchronous replica, and it is running SSIS. This was a mistake. To understand why, let’s break down the requirements.

First, if we consider the size of the database, Standard edition supports databases up to 524 PB, the same as Enterprise edition. The thinking may have been that, because it is a large database, it will require a large amount of processor capacity and RAM, and these are restricted on Standard edition.

This is true, but the requirements make no mention of capacity planning. It is simply an assumption. Given that Standard edition supports 128 GB of RAM and 24 cores (or 4 sockets) of processor, this assumption should be tested with capacity planning. Otherwise, there is a good chance that our organization will be spending a lot of extra money. At the time of writing, the list price for a two-core pack of Enterprise licenses is $15,123. This is compared to a two-core pack of Standard licenses with a list price of $3,945. If capacity planning shows that the data-tier application requires 16 cores, using Standard edition would reflect a cost savings of $89,424.

CAPEX vs. OPEX

If we build VMs in a public cloud environment, such as Azure or AWS, we can include the cost of the SQL Server license in the hourly cost of the EC2 instance. This has some advantages, such as being able to stop paying for the license when it is no longer needed and not paying for the license during hours when the instance is turned off. This conversion of capital expenditure (CAPEX) to operational expenditure (OPEX) is also in line with the cloud financial model, as opposed to the data center–hosting model.

On the flip side, however, if we know that our application will have longevity, then over time, we will end up paying more than we would for a perpetual license. This should be a consideration in the financial model of our business case for new applications or applications that we are looking to migrate to cloud.

Let’s also think about the feature requirements. MagicChoc has stated that the application should be hosted on availability groups, and this is a common reason I hear as a requirement for Enterprise edition. But MagicChoc has requested a single, synchronous replica, and Standard edition supports a feature called *Basic Availability Groups*. This feature is a cut-down version of availability groups that offers a single replica (either synchronous or asynchronous). The replica must be inactive until the point of failover. This means that it does not support advanced functionality such as readable secondaries, backup on secondary, or integrity checks on secondary. It does, however, meet MagicChoc’s requirements, so there is no reason to use Enterprise edition. We will discuss availability groups more in chapter 13.

The final reason for Enterprise edition was that SSIS is required. This is a common misconception that even some quite experienced DBAs make. In reality, SSIS is supported on Standard edition, with only a few restrictions. These restrictions include the ability to operate as a scale-out master and some advanced control flow tasks and data source transformations. For example, there are restrictions on change data capture (CDC) tasks and transformations involving text matching and extraction. There are also restrictions on advanced sources and destinations, such as Oracle, Teradata, SAP, and SSAS.

One of the most common reasons that I hear for brownfield applications requiring Enterprise edition is “It might need some database-level Enterprise features, but I don’t know and I don’t want to risk it.” This argument can be easily countered, however, as it is easily checked. The script in the following listing creates a database with a table that uses table partitioning, which is a feature that is only available in Enterprise edition.

Listing 8.3 Creating a database that requires Enterprise edition

```sql
CREATE DATABASE EnterpriseDatabase ;
GO

USE EnterpriseDatabase ;
GO

CREATE PARTITION FUNCTION PartFunc (INT)
    AS RANGE LEFT FOR VALUES (100, 200, 300) ;

CREATE PARTITION SCHEME PartScheme
    AS PARTITION PartFunc
    ALL TO ('PRIMARY') ;

CREATE TABLE VeryLargeTable (
    KeyColumn      INT    PRIMARY KEY    IDENTITY,
    OtherColumn    VARCHAR(50)
) ON PartScheme(KeyColumn) ;
```

We can now simply query the `sys.dm_db_persisted_sku_features` dynamic management view (DMV) to return a list of enterprise features. As demonstrated in listing 8.4, if we query this DMV from the database created previously, it will return a single row for partitioning. If the database was not using any Enterprise-only features, then it would return an empty result set.

Listing 8.4 Discovering Enterprise-only features

```sql
USE EnterpriseDatabase ;
GO

SELECT feature_name
FROM sys.dm_db_persisted_sku_features ;
```

To summarize, we should never use Enterprise edition as a default option. It is far more expensive than Standard edition and should be reserved for occasions where Enterprise-only features are required. In development environments, Developer edition should be used, as it is a fully featured edition, with the only limitation being that we cannot use it in production.

## 8.6 #38 Installing an instance when DBaaS or PaaS will suffice

The three major public cloud providers—GCP, AWS, and Azure—all offer SQL Server Database as a Service (DBaaS) functionality and Azure also offers SQL Server instances as a Platform as a Service (PaaS). These products can offer a large number of business benefits for some (but not all) workloads.

Let’s think about MagicChoc and a new application that it is building in cloud, which will provide process automation to reduce clerical overhead. The application is to be built in Azure and will use various cloud-native components, including *Azure Event Grid*, which is a cloud-native publisher/subscriber-based message distribution service; *Azure Functions*, which is a serverless option to execute event-driven code; and *Azure Kubernetes Service*, which is a managed container service. Because the application is greenfield, nobody really knows how much compute capacity is required to run the database.

MagicChoc’s DBA team is stretched. They are a small team, and in addition to the large number of projects happening at MagicChoc at the moment, they have multiple operational issues to fix, predominantly around performance tuning. The lead DBA can see the benefits of having an entirely cloud-native application, but instead of recommending using Azure SQL Database, they recommend a SQL Server VM be built in Azure. This is a mistake, and still a fairly common one. The trouble is that many DBAs are still reluctant to embrace cloud offerings. I have noticed two common reasons for this stance. The first is a lack, or perceived lack, of cloud skills, combined with a reluctance to learn a whole new technology stack. The second reason is a fear that SQL cloud offerings will make them redundant.

The first of these fears is somewhat down to the individual. Learning a new technology stack is always a daunting task, but cloud is not going anywhere and as database professionals, we can choose to adapt or we can follow the dodo into fables. The second of these fears, however, is certainly not true. Cloud will change the skill set required of the DBA, but it will certainly not make the DBA redundant.

> [!TIP]
>
> DBAs should also be reassured that despite SQL Azure being quite the leap from an instance running in a data center, the fundamental Database Engine is very similar to the database engine that they are familiar with.

In our specific scenario, the DBA team is under a lot of pressure. They have high-value, high-skilled work to attend to, such as diagnosing and resolving performance issues and helping the business with projects that require new databases. By avoiding DBaaS, the DBAs have simply given themselves yet another instance to manage, configure, resolve failed patches, organize downtime, and all of the other mundane parts of the job. If they had chosen a DBaaS offering instead, then they would have had more time to focus on the higher-value work.

An additional benefit of choosing Azure SQL Database in this scenario is autoscaling. The compute capacity in our scenario is not known. Provided we build our Azure SQL Database using the serverless compute tier, we can implement autoscaling. This will free up even more time, as we will have far less concern about right-sizing. We can avoid costs getting out of control by specifying the maximum number of vCores to use. We can also avoid the database scaling in too far, with a minimum number of vCores. The script in listing 8.5 uses PowerShell to create a new Azure SQL Database that will automatically scale between 2 and 16 vCores. The code uses a technique called *splatting*, which makes the code more readable by passing parameters to a command from a hash table.

Listing 8.5 Creating an autoscaling Azure SQL Database

```sql
$sqlAdminName = 'MagicChocAdmin'
$sqlAdminPassword = 'Passw0rd!'

$credentials = $(New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $sqlAdminName,
$(ConvertTo-SecureString -String $sqlAdminPassword -AsPlainText -Force))

$serverParameters = @{
    ServerName = 'processautomationsql'
    Location = 'eastus2'
    SqlAdministratorCredentials = $credentials
    ResourceGroupName = 'MagicChocApps'
}

New-AzSqlServer @serverParameters

$databaseParameters = @{
    ResourceGroupName = 'MagicChocApps'
    ServerName = 'processautomationsql'
    DatabaseName = 'AutoPro'
    Edition = 'GeneralPurpose'
    ComputeModel = 'Serverless'
    ComputeGeneration = 'Gen5'
    MinimumCapacity = 2
    Vcore = 16
}

New-AzSqlDatabase @databaseParameters
```

While DBaaS is not suitable for every database requirement, it is a powerful tool in a DBA’s arsenal. It should not be dismissed on the grounds of fear, and database professionals who plan to stay in the profession over the coming years and decades should take the time to learn and embrace cloud-native offerings.

## 8.7 #39 Installing all features

When installing SQL Server through the GUI, it is very easy and very tempting to perform what I call a “next, next, install” installation. This is where someone clicks next on every page of the installation wizard, accepting the default settings (we will talk more about this in the next section), and then selects all features on the Features to Install page.

When we are installing SQL Server from the command line, it is also very easy and very tempting to specify the `/ROLE` parameter as `AllFeatures_WithDefaults`. This will have the same effect as selecting all features from the list in a GUI installation.

Of course, I have not come across many people who perform a “next, next, install” installation in a professional setting, but I have encountered a lot of teams whose standard is to install all features. Let’s take a look at why this might be.

A development team at MagicChoc reaches out to the DBA team and asks for a new VM with SQL Server installed. The DBA team asks what features they require, and the developers say they are not sure. “Can we have everything to start with, as we have not designed the whole system yet?”

A few months later, the project is finishing up, and the team asks for a production server to be built. “We would like it exactly the same as the development server, because we know that works.”

It is typical for a DBA team to follow the path of least resistance and just wave this kind of request through. After all, what harm can it do, right? In some cases, DBA teams simply build a SQL Server instance with all features in all cases as their corporate standard. This is because it reduces effort if any new features are required in the future. This approach is a mistake, however—especially in production.

The SQL Server product stack is a large suite of applications, with each of these applications running as its own service. A service is an application that runs in the background. If we install all features, then we end up with the following services installed:

* SQL Server Database Engine
* SQL Server Agent
* PolyBase Engine
* PolyBase Data Movement
* Analysis Services
* SQL Browser
* SQL Server Full Text
* Integration Services

We will also have the Master Data Services and Data Quality Client applications installed. If we are installing older versions of SQL Server, we will also be installing all of the tools suite (2019 and below) and maybe even the Reporting Services Service (2016 and below).

That’s a lot of services and applications. This poses two problems. The first problem is that services consume resources when they are running, even if they are not being used. Therefore, by running services that are not required we are consuming resources that could be used by the features that we are using.

> [!TIP]
>
> We can, of course, work around this issue, by keeping the services that are not required in a stopped state, but why would we want the extra work? It also doesn’t resolve the issue of using additional disk capacity. Although this isn’t a massive amount of space and isn’t usually a big problem, it could, in some cases, make the difference between needing to resize a disk or not.

The second, much bigger issue with running this number of services is related to security. The more services that are installed on a server, the higher the attack surface of the server. If a bad actor finds a way to exploit a service that is running on our server, they may be able to gain lateral access to other servers. Therefore, it is always good practice to only install features that we require.

> [!TIP]
>
> An additional consideration relating to security is patching. If you install additional SQL Server features that run under their own service when they are not required, then they will still need to be patched. This adds additional work to support services that are not required.

A full list of features that can be installed from the SQL Server 2022 setup wizard can be found in table 8.1.

Table 8.1 List of SQL Server features

| Feature | Parent feature | Feature type | Description |
| --- | --- | --- | --- |
| Database Engine Services | N/A | Instance level | Provides core relational database functionality |
| SQL Server Replication | Database Engine Services | Instance level | Allows data to be disbursed between disparate instances using a publisher/subscriber model |
| Machine Learning Services and Language Extensions | Database Engine Services | Instance level | Supports distributed machine learning solutions with support for R and Python |
| Full-Text and Semantic Extractions for Search | Database Engine Services | Instance level | Provides advanced searching and matching functionality inside binary data, as well as textual columns with features such as fuzzy matching and thesaurus |
| Data Quality Services | Database Engine Services | Instance level | Core DQS functionality, including data quality functionality and storage |
| PolyBase Query Service for External Data | Database Engine Services | Instance level | Allows for multiple heterogeneous data sources to be queried |
| Analysis Services | N/A | Instance level | Provides multidimensional and tabular data modeling and querying functionality |
| Data Quality Client | N/A | Shared between all instances on server | A standalone application used to interact with DQS Services and both prepare and clean data |
| Integration Services | N/A | Shared between all instances on server | An extract, transform, and load tool used to orchestrate data ingestion and transformation, as well as exports to other systems. Further details can be found in chapter 6. |
| Scale Out Master | Integration Services | Shared between all instances on server | SSIS Scale Out can reduce bottlenecks by horizontally scaling the execution of packages. The Scale Out Master is responsible for the management of this scale out. |
| Scale Out Worker | Integration Services | Shared between all instances on server | The Scale Out Workers execute tasks that they have retrieved from the Scale Out Master. |
| Master Data Services | N/A | Shared between all instances on server | Allows you to create models that allow your data to be mastered across the enterprise. This is very useful in sprawling organizations, where different departments have different terminology for the same data item, as it allows you to map these terms to a single source of truth. |

Those of you familiar with extreme programming philosophy will be familiar with the *YAGNI principle*. YAGNI is an acronym for “You Aren’t Going to Need It” and is designed to remove inefficiencies in development. The same principle should be applied to SQL Server installations.

## 8.8 #40 Not scripting SQL Server installation

Installing SQL Server manually and then configuring the instance to meet best-practice guidelines is tedious. It is also prone to human error. Even if the DBA team has a detailed runbook of how to install and configure SQL Server instances, if there are multiple people manually installing multiple instances, mistakes will happen over time and our estate will end up inconsistent, which makes it harder to look after.

Let’s take MagicChoc as an example. Its DBA team manages 120 SQL Server instances, split across 100 VMs. Some of these are on-premises, and others are in the cloud. They have a build standard and a detailed build runbook, but each instance is built and configured manually. The high-level aspects of the build standard are as follows:

* Install a default instance using an appropriate edition, but the default instance can be overridden with an instance name if the server is to have multiple instances.
* Harden the instance.
* Install only the Database Engine unless other features are specifically required.

> [!TIP]
>
> In your environment, I would strongly recommend that you also consider performance optimizations. These will be at both the operating system and SQL Server levels. We will discuss many of these options in chapter 10.

Unfortunately, a recent audit of instances shows that none of these areas have been consistently applied across all servers. This is because, over the years, multiple DBAs have failed to follow the runbook thoroughly when building instances in a rush. The mistake was expecting DBAs to install and configure instances manually.

Instead of installing and configuring an instance manually, the DBA team could have applied a scripted approach. Using this methodology, a script, often written in PowerShell, will install and configure instances. This means that when a new SQL Server instance is required, a DBA can simply run a reusable script, and that’s it. Job done. This not only improves consistency, but it also removes much of the time and effort of building instances, freeing up DBAs for higher-value work.

Gold images

Some people still use the “gold image” approach to SQL installation. In this approach, a Windows Server image will be created with SQL Server preinstalled. The scripted approach is much more flexible, however.

Gold images certainly improve consistency but maybe a little too much. They often lead to all SQL instances being installed as Enterprise edition and all instances being configured for online transaction processing (OLTP), even when a company has many data warehouses.

The workaround for this is to have multiple gold images, but that generates a lot of work—when an update is required to an image, for example, to upgrade it to use the latest service pack.

Therefore, I recommend favoring favor a scripted approach over a gold image. It is much easier to get the right balance of consistency and flexibility by using parameters that can be passed to your script.

Let’s see how we could improve the process by creating a PowerShell script that will install a SQL Server instance, run some basic smoke tests, and then configure the instance. First, though, let’s think about what our build standards might be to ensure that our instances are hardened:

* Disable remote access
* Rename the `sa` account

WARNING In this example, we will make just two hardening configurations. This is for the sake of a clear and concise example. In your environment, there are many hardening configurations that you should consider. I recommend consulting the SQL Server 2022 Center for Internet Security (CIS) benchmark, which is available from CIS at <https://mng.bz/eV0Q>.

In addition to hardening the instance, we will also need to think about the edition that we require. We will want to install Developer edition for development machines, and we will want to be able to choose between Enterprise and Standard edition for production instances.

So let’s first think about the parameters that we will want to pass to our script. Given our configuration options, we will want to be able to pass the username and passwords of the service accounts that we will use to run the Database Engine and SQL Server Agent. We will want to pass the edition, and we will also want to pass our chosen name and password for the `sa` account. Therefore, we will need a `param` block at the top of our script, so that it knows what parameters to expect. We also need to accept a parameter for instance name, but because the standard behavior, in line with our requirements, should be to install a default instance and a named instance should be by exception, we will give the parameter a default value of `MSSQLSERVER`. This means that a default instance will be installed unless we specifically override this value by manually supplying a value for the parameter at execution time.

The first task that our script will perform is to install the `sqlserver` PowerShell module. This will enable us to run `Invoke-SqlCmd` later in the script. We can run this with the `Install-Module` cmdlet.

Our second task is to determine the value of the `$pid` variable based on the edition parameter. The SQL Server installation determines which edition of SQL Server to install based on a product key. Therefore, a product key must be passed to the `/PID` parameter. If no product key is passed, the installation will default to Evaluation edition, which will stop working after the trial period if it isn’t upgraded.

WARNING In a real environment, product keys, also known as product IDs (PIDs) should ideally be stored in a password vault and read out of the vault by the script. This avoids company-specific details being stored in source control.

Many organizations have volume license keys. This means that they will use the same key for all of their SQL Server installations. Therefore, we can simplify the installation by mapping a product key to an edition. Then, for each install, DBAs can simply pass a value for the edition that they want to install, rather than having to dig out a volume license key each time.

WARNING The product key 00000-00000-00000-00000-00000 used in the sample script for Enterprise and Standard editions is actually the product key for Evaluation edition. The expectation is that you will replace this key with your organization’s appropriate product keys before execution.

Next, we will install SQL Server by calling `setup.exe`. The script assumes you have downloaded the installation media and that it is available and that the script is being called from the folder where the installation media is stored. This command takes most of the parameters that we pass to our scripts: the instance name, the service account names, and passwords as well as the edition. We also pass the `/IACCEPTSQLSERVERLICENSETERMS` and `/Q` parameters, which are required to accept the license terms and to perform a quiet installation. Both are required for an unattended installation. The command also uses the `$pid` parameter, which we calculated from the `$edition` parameter, and we will pass the value `SQL` to the `/SECURITYMODE` parameter to enable mixed-mode authentication. This means that we also have to pass the password for the `sa` account.

> [!NOTE]
>
> Immediately after the installation, the `sa` account will have its default name. We will change it later in the script.

Once the instance is installed, we should run some simple smoke tests to validate the installation. Finally, we will use the `Invoke-SqlCmd` tool to configure the hardening of our instance. The first `sqlcmd` command disables remote access. This disables the deprecated feature that allows local stored procedures to be run from remote servers over a linked server and vice versa. The second command changes the name of the `sa` account to a name that we will pass to the script as a parameter. The rationale behind this is that `sa` is a well-known name for the administrator account. Because `sa` is used during second-tier authentication, changing it to a novel name reduces the risk of a bad actor performing a brute-force attack against the instance to gain administrative privileges.

The script in listing 8.6 brings all of these components together into a single script that the DBA team can execute every time they need to install and configure a new instance of SQL Server.

> [!TIP]
>
> Don’t forget to change the PIDs to reflect your organization’s product keys.

Listing 8.6 Creating a scripted installation of SQL Server

```sql
[CmdletBinding()]
param(
[string] $SQLServiceAccount,
[string] $SQLServiceAccountPassword,
[string] $AgentServiceAccount,
[string] $AgentServiceAccountPassword,
[string] $SaName,
[string] $SaPassword,
[string] $edition,
[string] $InstanceName = 'MSSQLSERVER'
)

if ($edition -eq 'Developer') {
    $sqlPid = '22222-00000-00000-00000-00000'
} elseif ($edition -eq 'Standard') {
    $sqlPid = '00000-00000-00000-00000-00000'
} elseif ($edition -eq 'Enterprise') {
    $sqlPid = '00000-00000-00000-00000-00000'
}

If(-not(Get-Module NuGet -ErrorAction silentlycontinue)){
    Install-Module NuGet -Confirm:$False -Force
}                                                                  ①

If(-not(Get-Module SQLServer -ErrorAction silentlycontinue)){
    Install-Module SQLServer -Confirm:$False -Force -AllowClobber
}                                                                  ②

./setup.exe /ACTION=Install /FEATURES=SQLENGINE /INSTANCENAME=$InstanceName /SQLSVCACCOUNT=$SQLServiceAccount /SQLSVCPASSWORD=$SQLServiceAccountPassword /AGTSVCACCOUNT=$AgentServiceAccount /AGTSVCPASSWORD=$AgentServiceAccountPassword /PID=$sqlPid /IACCEPTSQLSERVERLICENSETERMS /Q /SECURITYMODE=SQL /SAPWD=$SaPassword
/SQLSYSADMINACCOUNTS=Administrator                                 ③

if ($InstanceName -ne 'MSSQLSERVER') {                             ④
    $ServiceName = 'MSSQL$' + $InstanceName
    $ServerInstance = 'localhost\' + $InstanceName
} else {
    $ServiceName = $InstanceName
    $ServerInstance = 'localhost'
}

$ServiceStatus = Get-Service -servicename $ServiceName

if ($ServiceStatus.Status -eq 'Running') {
    $output = "The service for SQL Server instance: {0} is Running" -f
$InstanceName
    Write-Output $output
} else {
    $output = "The service for SQL Server instance: {0} is NOT Running.
Script terminating" -f $InstanceName
    Write-Output $output
    exit
}

$ServerName = Invoke-Sqlcmd -ServerInstance $ServerInstance -query 'SELECT
@@SERVERNAME ;' -Username 'sa' -Password $SaPassword

if ($ServerName -ne $null) {
    $output = "The SQL Server instance: {0} is accessible" -f $InstanceName
    Write-Output $output
} else {
    $output = "The SQL Server instance: {0} is NOT accessible. Script
terminating" -f $InstanceName
    Write-Output $output
    exit
}                                                                   ⑤

Invoke-Sqlcmd -ServerInstance $ServerInstance -Query "EXEC sp_configure 'show advanced options', 1 ; RECONFIGURE ; EXEC sp_configure 'remote
access', 0 ; RECONFIGURE ; EXEC sp_configure 'show advanced options', 0 ;
RECONFIGURE ;" -Username 'sa' -Password $SaPassword                 ⑥

$Query = "ALTER LOGIN sa WITH NAME = {0}" -f $SaName

Invoke-Sqlcmd -ServerInstance $ServerInstance -Query $Query -Username 'sa'
-Password $SaPassword                                               ⑦
```

① Silently installs NuGet, a prerequisite to install the SQL Server module

② Silently installs the SQL Server PowerShell module

③ Installs SQL Server instance

④ Start of smoke tests

⑤ End of smoke tests

⑥ Disables remote access

⑦ Renames sa account

If the script is saved as `InstallSQLServer.ps1` into the root of the folder that contains the installation media, then it can be executed using the command in listing 8.7.

> [!TIP]
>
> The script will need to be run as Administrator.

Listing 8.7 Executing the scripted installation of SQL Server

```sql
./InstallSQLServer.ps1 -SqlServiceAccount 'sqlServiceAccount' –
SqlServiceAccountPassword 'Pa$$w0rd' -AgentServiceAccount
'sqlServiceAccount' -AgentServiceAccountPassword 'Pa$$w0rd' -SaName
'SQLAdmin' -SaPassword 'Pa££w0rd' -Edition 'Developer'
```

Automating SQL Server installation should be at the top of any DBA’s to-do list. It reduces manual effort and promotes standardization. It also brings other advantages, such as being able to store the build in GitHub, providing version control and a simple change mechanism.

## 8.9 #41 Thinking configuration management doesn’t apply to SQL Server

One of the main reasons for automating SQL Server installation is to promote consistency. This is important to make our enterprise supportable and to ensure that our security baseline is in place. The trouble is that, after our build has finished, anyone with an appropriate level of access can simply change it.

For example, imagine that we have just built a SQL Server instance. It is correctly configured and adheres to CIS level 1. Unfortunately, a few days after building the instance, a junior DBA enables `xp_cmdshell`. We will discuss this extended stored procedure in more detail in chapter 14, but for now you only need to know two things about it. First, it allows operating system commands to be executed from within a SQL Server instance. Second, it is a big security hole and should not be enabled.

So just a few days after being built, the instance is no longer in the desired state and there is a live security vulnerability. We could deal with this issue, in part, by correctly limiting permissions and by auditing activity. Neither of these methods fully resolves the problem, however.

A full solution is to apply a technique called *configuration management*. This is a technique where configuration is fully applied when a server is first built, but it is then periodically evaluated. If an evaluation determines that drift has occurred, then the drift is automatically fixed. Therefore, if configuration management evaluated the state of our server every 30 minutes, then a misconfiguration such as a security vulnerability would be automatically resolved between 0 and 30 minutes after it occurred.

Configuration management has been popular on Linux for more than a decade and is key to many DevOps environments. Its usage has been increasing in Windows environments for a number of years, but it is still not as commonly used as it is in Linux environments. There are multiple configuration management tools available, with some of the most popular including Puppet, Ansible, and PowerShell Desired State Configuration (DSC).

At the most basic level, these tools use the following concepts:

* A resource is a unit of code that evaluates and configures the current state of a specific server configuration. This configuration could be anything from a registry key to a folder, or even the installation of software. Each resource will have the following methods:

  + A test method, which determines if the current configuration is correct. A test method is usually executed every time a resource is evaluated.
  + A set method, which updates the configuration if it is not in the desired state. This method is usually only run if a test method discovers drift.
* A manifest, which lays out the resources that must be run to configure the server, along with the values that should be passed to the resources.

The diagram in figure 8.3 demonstrates this. It illustrates a single DSC configuration being applied to multiple servers that host SQL Server instances. It shows the process that happens on each of these servers to enforce best practices for disabling `xp_cmdshell`. Every 30 minutes, the DSC test function runs and checks if `xp_cmdshell` is enabled. If the desired state (disabled) is discovered, then it exits the process and moves on to the next configuration in the manifest. If the configuration is not in the desired state, however, it runs a set function to disable the feature.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH08_F03_Carter.png)<br>
**Figure 8.3 Configuration management process**

Configuration management is a great approach to installing and configuring SQL Server instances. The trouble is, however, that many DBAs are either not familiar with it or fail to use it because they are not familiar with the concepts. This is a mistake. Using configuration management can not only reduce effort and make the enterprise easier and more consistent to manage, but it can also help with facets such as compliance and security.

While a full discussion of PowerShell DSC is beyond the scope of this book, as it is worthy of a book in its own right, let’s explore how we could create a simple *DSC Managed Object Format (MOF)* document, which is the DSC implementation of a manifest. The MOF we create will ensure that a SQL instance is created. It will then disable remote access and create a SQL login for a local Windows user called `Pete`. We will then explore how to compile and apply the configuration.

Before we get started, however, let’s get set up. We will need to install the `SqlServerDsc` PowerShell module, which we can achieve using the script in the following listing.

Listing 8.8 Installing DSC PowerShell modules

```sql
Install-Module SqlServerDsc
```

> [!TIP]
>
> If we want to configure the operating system, we should also install the `PSDscResources` PowerShell module.

The configuration that we will create is called `InstallSql` and accepts parameters for the instance name (which defaults to the default instance) and the edition that we want to install. The first statement in the configuration imports the `SqlServerDsc` module. When creating configurations, it is considered good practice to import the modules that will be used. We then determine the PID that should be used based on the edition. This uses the same logic as we used in the previous section.

We will then define a node. This is the name of the server on which we want to run the configuration. In our case, we have only defined a node for localhost, but in more complex examples we could create a central DSC push service, which will push the configuration to multiple servers. In that scenario, we can define multiple nodes.

Inside the node definition, we will define three DSC resources. The first of these resources ensures that the SQL Server instance has been created. If the instance exists, then the resource will do nothing, but if the instance is missing, the resource will create it.

The following two resources disable remote access and create a login for `Pete`, respectively. If these resources were executed before the instance was created, then they would, of course, throw an error, as the resources would not be able to connect to the instance. Therefore, the `DependsOn` property is used to ensure that they are not executed before the instance exists. The configuration can be reviewed in the following listing.

Listing 8.9 Creating a DSC configuration

```sql
Configuration InstallSql {
    param (
        $SqlInstanceName = 'MSSQLSERVER',
        $Edition
    )

    Import-DscResource -ModuleName SqlServerDsc

    if ($Edition -eq 'Developer') {
        $ProductKey = '22222-00000-00000-00000-00000'
    } elseif ($edition -eq 'Standard') {
        $ProductKey = '00000-00000-00000-00000-00000'
    } elseif ($edition -eq 'Enterprise') {
        $ProductKey = '00000-00000-00000-00000-00000'
    }

    node localhost {
          SqlSetup 'InstallInstance' {
               InstanceName        = $SqlInstanceName
               Features            = 'SQLENGINE'
               SourcePath          = 'C:\SQL Media'
               SQLSysAdminAccounts = @('Administrator')
               ProductKey          = $ProductKey
          }

          SqlConfiguration 'RemoteAccess' {
              InstanceName = $SqlInstanceName
              OptionName   = 'remote access'
              OptionValue  = 1

              DependsOn    = '[SqlSetup]InstallInstance'
          }

          SqlLogin 'AddSqlAdmin' {
              Ensure       = 'Present'
              Name         = 'Pete'
              InstanceName = $SqlInstanceName
              LoginType    = 'WindowsUser'

              DependsOn    = '[SqlSetup]InstallInstance'
          }
     }
}
```

Now that we have defined the configuration, we need to compile it, which will create the MOF file. To compile the configuration, we will first need to dot source the file, which loads it within the current context. We will then run the `InstallSql` configuration, passing the parameter values as we would for a cmdlet. Therefore, assuming we save our configuration as `SqlInstallDsc.ps1` in the folder `c:\DSC`, the script in listing 8.10 will compile the MOF.

> [!TIP]
>
> If you have been following along with the examples in this chapter and have already created a default instance of SQL Server, you may wish to amend the code samples in this section to create a named instance. This will avoid either uninstalling the existing instance or using a different VM. You will, of course, need to supply your own file paths.

Listing 8.10 Compiling the configuration

```sql
. c:\DSC\SqlInstallDsc.ps1

InstallSql -Edition Developer
```

A folder named after the configuration will be created in the working directory. Inside this folder we can find the compiled MOF file. The MOF file created from our compilation is

```sql
/*
@TargetNode='localhost'
@GeneratedBy=Administrator
@GenerationDate=11/05/2023 11:49:28
@GenerationHost=EC2AMAZ-43B3PBE
*/

instance of DSC_SqlSetup as $DSC_SqlSetup1ref
{
SourcePath = "C:\\SQL Media";
 InstanceName = "MSSQLSERVER";
 ProductKey = "22222-00000-00000-00000-00000";
 SourceInfo = "C:\\dsc\\SqlInstallDsc.ps1::18::11::SqlSetup";
 ResourceID = "[SqlSetup]InstallInstance";
 ModuleName = "SqlServerDsc";
 SQLSysAdminAccounts = {
    "Administrator"
};
 ModuleVersion = "16.5.0";
 Features = "SQLENGINE";

 ConfigurationName = "InstallSql";

};
instance of DSC_SqlConfiguration as $DSC_SqlConfiguration1ref
{
ResourceID = "[SqlConfiguration]RemoteAccess";
 InstanceName = "MSSQLSERVER";
 SourceInfo = "C:\\dsc\\SqlInstallDsc2.ps1::26::11::SqlConfiguration";
 OptionValue = 1;
 ModuleName = "SqlServerDsc";
 OptionName = "remote access";
 ModuleVersion = "16.5.0";

DependsOn = {

    "[SqlSetup]InstallInstance"};

 ConfigurationName = "InstallSql";

};
instance of DSC_SqlLogin as $DSC_SqlLogin1ref
{
ResourceID = "[SqlLogin]AddSqlAdmin";
 InstanceName = "MSSQLSERVER";
 Ensure = "Present";
 SourceInfo = "C:\\dsc\\SqlInstallDsc2.ps1::34::11::SqlLogin";
 Name = "Pete";
 ModuleName = "SqlServerDsc";
 LoginType = "WindowsUser";
 ModuleVersion = "16.5.0";

DependsOn = {

    "[SqlSetup]InstallInstance"};

 ConfigurationName = "InstallSql";

};
instance of OMI_ConfigurationDocument

                    {
 Version="2.0.0";

                        MinimumCompatibleVersion = "1.0.0";

                        CompatibleVersionAdditionalProperties=
{"Omi_BaseResource:ConfigurationName"};

                        Author="Administrator";

                        GenerationDate="11/05/2023 11:49:28";

                        GenerationHost="EC2AMAZ-42B3QBE";

                        Name="InstallSql";

                    };
```

The catalog can be applied to the server by using the command in listing 8.11. This command can then be added to a scheduled task that will run on a schedule and ensure that the server is always configured as required.

Listing 8.11 Applying the configuration

```sql
Start-DscConfiguration -Path 'C:\dsc\InstallSql' -Verbose
```

> [!TIP]
>
> We are not limited to the DSC resources available in the `SqlServerDsc` module. DSC is fully extensible, and we can create our own, custom DSC resources. While beyond the scope of this book, it is something that I encourage you to explore. As a starting point, there is a chapter dedicated to DSC in the book *Windows PowerShell in Action*, 3rd ed., which can be found at <https://mng.bz/px5z>, or see a very basic walk-through provided by Microsoft, which can be found at <https://mng.bz/OmNE>.

Configuration management is a powerful tool, and DBAs should consider it as a means of reducing administrative overhead by ensuring that the SQL estate is consistent. It is the modern approach to automation and is fully extensible, regardless of the framework that we adopt.

> [!TIP]
>
> If your organization uses Azure, then DSC can be applied to Azure Arc–enabled servers using Azure Automanage Machine Configuration (formally known as Azure Policy Guest Configuration).

## 8.10 #42 Using SQL Server cloud images without modifying them

All major cloud providers supply images with SQL Server preinstalled. This makes spinning up SQL Server VMs in cloud very easy. Using cloud-provided images is also important if we want to use the license-included model of paying for SQL Server. In this model, instead of bringing our own SQL Server license, the license cost is included in the hourly rate of the VM. To use the license-included model, we must use a cloud provider’s SQL Server image, as this is what triggers the license inclusion.

It is very common for organizations to use this image, as provided, to simply spin up a new VM in their chosen cloud whenever a new SQL Server VM is required. This, however, is a mistake.

To explain why this is a mistake, let’s think about some of the things we have already discussed in this chapter. For example, we have discussed why it is not a good idea to install all SQL Server features but to limit the installation to just those features that are required. We have also discussed the use of configuration management to keep the desired configuration of the server and SQL Server instance. In chapter 10, we will also discuss why we should avoid using trace flags 1117 and 1118.

Despite these good practices, if we were to build a new SQL Server EC2 instance in AWS, we would notice that all features have been installed. This includes SSMS, which should really be used as a client tool rather than being installed on a server. The image in figure 8.4 is taken from a freshly built EC2 instance using the AWS-provided SQL Server AMI.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH08_F04_Carter.png)<br>
**Figure 8.4 Fresh EC2 instance with SQL Server AMI**

In Azure, the situation is even worse. Not only will all features and tools be installed, but trace flags T1117 and T1118 will also be configured on the SQL Server instance. The image in figure 8.5 was taken from a freshly built Azure VM using the Azure-provided virtual machine image. The image shows SQL Server configuration manager with all SQL services running and the startup parameters of the Database Engine service expanded to show the configured trace flags.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH08_F05_Carter.png)<br>
**Figure 8.5 Freshly built Azure VM with SQL Server image**

Why do cloud providers do this? The answer is that it’s an attempt to make everything easy for all customers regardless of their requirements. This comes with the drawbacks that we discussed earlier in this chapter, however. What’s more, because the SQL Server instance is baked into the image, it is fully subject to drift.

This means that using a cloud provider’s SQL Server image as is can be considered a mistake—and a very common one. The way to avoid this mistake while still remaining license compliant is to create our own custom image but to base that image on the cloud provider’s SQL Server image.

To do this, we can create a new VM based on the cloud provider’s SQL Server image and then customize it to meet our requirements. For example, we might uninstall components such as Analysis Services and Integration Services if we do not need them. We might also configure the instance to be CIS compliant. We could even create a DSC MOF to ensure the instance remains configured to our requirements.

Once we have finished modifying the VM, we can create a new image based on our VM. The process for doing this varies between cloud providers but is always quite straightforward. For example, in Azure, we can navigate to the VM in Azure Portal and select the Capture option from the top menu. This will cause the Create an Image page to be displayed, where we can configure properties for the image, such as the resource group, and decide if we want the image to be shared to the Azure compute gallery, which will make them public, or if we want to keep it as a (private) managed image (which is almost certainly the option you will select).

Alternatively, to create an image from an AWS EC2 instance, we can simply navigate to EC2 instances in the AWS Console. From there, right-click on the EC2 instance and select Image and Templates > Create Image from the context menu. This will cause the Create Image window to be displayed. Here we can give the image a name and tags. We can also add additional volumes and configure volume sizes.

WARNING If you play with cloud image creation, be mindful that SQL Server is expensive, and when it’s included in the hourly cost of a VM, prices can add up. Make sure you keep instances shut down when not in use and terminate/delete them when you no longer need them.

When building SQL Server VMs in the cloud, we should not use the cloud provider’s images as is. Instead, we should modify a server to meet our requirements and then create our own image.

## Summary

* When building SQL Server instances, it is important to use meaningful instance names to keep environments easy to identify and avoid confusion.
* Naming instances appropriately can be something of an art over science.
* When planning a deployment, consider Linux as a viable operating system.
* SQL Server 2022 is supported on Red Hat, SUSE, and Ubuntu.
* Consider SQL Server containers, rather than assuming that SQL Server should always be installed on a VM or bare metal—especially for large development environments.
* When using SQL Server inside a container, use a Docker volume to store that data outside of the container. This allows data to be persisted even after a container is removed.
* SQL Server is fully supported on Linux containers. It is not currently supported on Windows containers, but Windows containers can still be used for SQL Server in nonproduction environments if we create our own container image.
* Server Core should be considered when planning an installation of SQL Server. Installing SQL Server on Windows Server with Desktop Experience should not be the default. Server Core is a good choice, unless there is a specific reason that the GUI is required.
* With no GUI, Server Core can be more performant and more secure than Windows Server with Desktop Experience.
* SQL Server Enterprise edition is expensive and should be reserved for cases where Enterprise-level features are required in production.
* Use Standard edition where appropriate in production.
* Use Developer edition in nonproduction environments.
* When planning SQL Server deployments, DBaaS offerings from cloud providers should be considered. They reduce overhead, as we do not need to manage the operating system or SQL Server instance.
* While cloud-native SQL Server offerings have the potential to change the required skillset of a DBA, they do not have the potential to remove the need for DBAs. Therefore, DBAs should not fear cloud.
* Only install the features that are required, rather than all features. This will reduce the security footprint and avoid unnecessary consumption of resources.
* Installing SQL Server manually is time-consuming and leads to human error.
* Automating SQL Server installation saves administrative overhead and leads to an estate that is easier to manage.
* Scripting a deployment only ensures consistency and good practice at build time. Drift can occur after the deployment.
* Using configuration management techniques allows us to keep a server and SQL Server instance correctly configured for the lifecycle of the application.
* SQL Server can be configured with configuration management tools such as PowerShell DSC and Puppet.
* Using configuration management helps us remain compliant with security requirements and reduces administrative overhead of the SQL Server estate.
* Configuration management tools include Ansible and Puppet. Microsoft also offers configuration management inside PowerShell. This is called PowerShell DSC.
* PowerShell DSC is fully extensible, and custom DSC resources can be created if required.
* Cloud images that are preloaded with SQL Server have all features installed, which consumes additional resources and increases the attack surface of the SQL Server instance.
* Create custom cloud images based on a vendor-supplied image and use the custom image instead of the vendor-supplied image.
* Custom images must be based on a vendor-supplied image when the “license included” licensing model is required. This ensures that the cost of the SQL Server license is included in the hourly cost of the VM.
