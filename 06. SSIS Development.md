# 6 SSIS development

This chapter covers

* An introduction to SSIS and SSIS development mistakes
* Losing bad data
* Not optimizing data loads
* Using SSIS as a T-SQL orchestration tool
* Always extracting all the data from a source table

SQL Server Integration Services, commonly known as SSIS, is a tool that ships with SQL Server Enterprise and Standard editions, although there are feature limitations in the Standard edition. It is an *extract, transform, and load* (*ETL*) tool that allows developers to build data movement and transformation pipelines within a drag-and-drop GUI.

NOTE Examples in this chapter will use SQL Server Data Tools, which can be downloaded from <https://mng.bz/RNB0>. The Integration Services extension also needs to be installed and is available in Extensions | Manage Extensions and then by searching for SQL Server Integration Services Projects 2022 from the marketplace.

An SSIS package always consists of a single *control flow*, which orchestrates the tasks that the package will run. Within the control flow, we can create zero or more *data flows*, which are used to import, export, and transform data within memory buffers.

Tasks on the control flow are joined by precedence constraints. This allows us to design our packages to run tasks serially, instead of all tasks running in parallel. Tasks following a precedence constraint are always run after the task(s) preceding the constraint.

Constraints

SSIS has three types of constraint: success, failure, and completion. These allow us to control the flow of our package and create custom error-handling logic. Tasks connected to constraints have the following behavior:

* A task connected to a success constraint will be executed following the successful completion of the preceding task. If the preceding task fails, it will not be executed.
* A task connected to a failure constraint will be executed following the failure of the preceding task. If the preceding task succeeds, it will not be executed.
* A task connected to a completion constraint will run following the execution of the preceding task. It will be executed regardless of the success or failure of the preceding task.

In the SSIS control flow, a success constraint is shown in green, a failure constraint in red, and a completion constraint in blue.

Precedence constraints within the control flow should not be confused with data paths within the data flow. In the data flow, a blue data path indicates that rows that successfully pass through the preceding component are passed to the subsequent component. A red data path indicates that rows that fail the preceding component will be passed to the subsequent component. There is no completion data path.

When we create a project from the Integration Services Project template, we will see an empty control flow in front of us, with the SSIS Toolbox in the left-hand pane. This toolbox is where we can drag out tasks onto the control flow or our components onto a data flow. The toolbox is context sensitive, so if we are in a data flow, we will see data flow components instead of control flow tasks.

Each data flow is actually a task on our control flow, so we can create a data flow by dragging a data flow task onto the control flow. We can then view the data flow by either double-clicking the task or switching to the Data Flow tab at the top of our design surface.

Connection managers are used to create connections to data sources, such as databases and flat files. Out-of-the-box connection managers include OLE DB connections, ADO.NET connections, Files, Flat Files, and Analysis Services. For performance, when connecting to SQL Server instances within data flows, OLE DB is often the optimal choice. ADO.NET connection managers are often used for connecting to SQL Server for Execute T-SQL Statement tasks on the control flow.

OLE DB configuration

Some years ago, OLE DB was deprecated by Microsoft. It was then undeprecated in 2018 and a new OLE DB driver was released. SQL Native Client 11 is still deprecated, however, and does not ship with SQL Server 2022. Therefore, to use SQL Server OLE DB with applications that connect to SQL Server, including SSIS, you will need to download and install the Microsoft OLE DB Driver 19 for SQL Server, which can be found at <https://mng.bz/AagK>.

The Microsoft Visual C++ Redistributable is a prerequisite for this driver, however, so this will need to be installed first. This redistributable can be found at <https://mng.bz/ZVXO>.

Once these packages are installed, the correct OLE DB driver will appear in the Provider drop-down. By default, however, the connection manager will set the Use Encryption For Data property to `Mandatory`. Unless you have certificates configured for your SQL Server instance, you will need to change this to `Optional`. The property can be found by using the Data Links button in the connection manager dialog box and navigating to the All tab.

Developers can make various mistakes when developing SSIS packages. In this chapter, we will explore a mistake that can result in data loss. We will explore the results of not optimizing data loads. We will also look at the effects of using SSIS purely as an orchestration tool and not taking advantage of data pipelines. Finally, we will look at the mistake of not filtering our data extraction.

To explore some of the mistakes that are commonly made when using SSIS, we will continue with the MagicChoc example and use the following scenario. The marketing department is going to start advertising online and wants to track the impressions (views of online advertising banners) that are seen by potential customers. The impressions data will be received in a CSV file, where it will be picked up by an SSIS package, which will be scheduled to run once per day. The package will put the data into a table in the `staging` schema of the `Marketing` database. It will then transform the data into a table in the `marketing` schema. Finally, it will roll the data up and insert it into an aggregated table in the `reporting` schema, before truncating the staging table.

If you wish to follow along with the examples in this chapter, see the sample CSV file in the code repository for this book. It is called `impressions.csv`. You will also need to create the `Marketing` database using the script in the following listing.

Listing 6.1 Creating the `Marketing` database

```sql
CREATE DATABASE Marketing ;
GO

USE Marketing ;
GO

CREATE SCHEMA staging ;
GO

CREATE SCHEMA marketing ;
GO

CREATE SCHEMA reporting ;
GO

CREATE TABLE staging.ImpressionsStage (
    ImpressionUID        VARCHAR(MAX)    NULL,
    ReferralURL          VARCHAR(MAX)    NULL,
    CookieID             VARCHAR(MAX)    NULL,
    CampaignID           VARCHAR(MAX)    NULL,
    RenderingID          VARCHAR(MAX)    NULL,
    CountryCode          VARCHAR(MAX)    NULL,
    StateID              VARCHAR(MAX)    NULL,
    BrowserVersion       VARCHAR(MAX)    NULL,
    OperatingSystemID    VARCHAR(MAX)    NULL,
    CostPerMille         VARCHAR(MAX)    NULL,
    EventTime            VARCHAR(MAX)    NULL,
    BidPrice             VARCHAR(MAX)    NULL
) ;

CREATE TABLE marketing.Impressions (
    ImpressionID       BIGINT            NOT NULL    PRIMARY KEY  IDENTITY,
    ImpressionUID      UNIQUEIDENTIFIER  NOT NULL,
    ReferralURL        VARCHAR(512)      NOT NULL,
    CookieID           UNIQUEIDENTIFIER  NOT NULL,
    CampaignID         BIGINT            NOT NULL,
    RenderingID        BIGINT            NOT NULL,
    CountryCode        TINYINT           NULL,
    StateID            TINYINT           NULL,
    BrowserVersion     BIGINT            NOT NULL,
    OperatingSystemID  BIGINT            NOT NULL,
    BidPrice           MONEY             NOT NULL,
    CostPerMille       MONEY             NOT NULL,
    EventTime          DATETIME          NOT NULL
) ;

CREATE TABLE reporting.ImpressionAggregates (
    ImpressionAggregateID    BIGINT    NOT NULL    PRIMARY KEY  IDENTITY,
    CampaignID               BIGINT    NOT NULL,
    CountryCode              TINYINT   NOT NULL,
    EventDate                DATE      NOT NULL,
    AvgBidPrice              MONEY     NOT NULL,
    AvgCostPerMille          MONEY     NOT NULL
) ;
```

In Visual Studio, we will create a new project called `ImpressionsLoad`, which will use the Integration Services project type (which needs to be installed). The solution should have the same name, and the project can be created in the same directory as the solution.

Online marketing terminology

For some context, see the following definitions of some terms used within this data domain:

* Cost per mille (CPM) is the actual cost charged for 1,000 views, following an automated auction of the banner space.
* The bid price is the maximum cost per 1,000 views that the advertiser is willing to pay in the automated auction.
* The referral URL is the website on which the view occurred.
* The cookie ID is the unique ID of the tracking cookie. Therefore, it is synonymous with a user.

It may also be interesting to note that “views” is synonymous with “impressions,” also known in the industry as “eyeballs.”

## 6.1 #22 Throwing away bad data

One of the most common mistakes that I see SSIS developers make is throwing away rows that they can’t load. Often, misconceptions around how SSIS works means that developers do not even know they are doing this. To explore this, let’s create a simple SSIS package with a data flow that will load the impression data from our `impressions.csv` file (which I have placed in the root of `c:\`) into our `staging.impressions` table.

Let’s begin by dragging a data flow task onto the control flow and renaming it Load Impressions Staging. We should also create two connection managers: one for the CSV file and the other for the SQL Server instance.

To create the connection manager for the CSV file, we should choose a flat file connection. On the General Page of the dialog box, we can configure the basic details of the connection, such as the name of the file, file encoding, and type of file (delimited, fixed width, or ragged right). In the Columns page of the dialog box (figure 6.1), we should ensure that the column delimiter is configured as a comma. Other possible delimiters include tab, semicolon, and vertical bar, among others. The Advanced page allows us to specify the properties of individual columns, such as data type. The Preview page displays the first 100 rows within the file.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F01_Carter.png)<br>
**Figure 6.1 Flat file connection manager—Columns page**

In the Advanced page of the dialog box, we should set our data types. SSIS can suggest data types for us, based on the first 100 rows, but unless we are sure that the first 100 rows adequately cover the possibilities within our data, we should select the data types ourselves. Because we are loading into a staging table with expansive data types, however, we will just leave the data types as 50-character-length strings for now.

Our second connection manager will be an OLE DB connection manager, which points to the SQL Server instance that hosts the marketing database. In this connection manager, we will first choose the OLE DB provider that we want to use from the drop-down. In our case, we will select the Native OLE DB\Microsoft OLE DB Driver 19 for SQL Server. We will then specify the name of the SQL Server instance and the name of the database (initial catalog) that we will be connecting to. Additionally, we can select how we want to authenticate to the instance. If we use Windows integrated security, then the package will authenticate as the identity that runs the package. If we choose a SQL Server username and password, then the package will authenticate using second-tier authentication, also known as SQL authentication.

Back in the data flow, we can now create a flat file source, which will use our flat file connection manager, and an OLE DB destination, which will use our OLE DB connection manager.

The Columns page of the data flow source allows us to map *external columns*, which are columns coming from the data source, to *output columns*, which are the columns that are passed from our component to the next component in the data flow. We can remove external columns by deselecting them in the top window, and we can change the names of the output columns. For our use case specifically, we will map the `ImpressionUID` external column to an output column called `ImpressionID`.

On the Error Output page of the flat file source, we can define the behavior of the component if there is a failure of any given row that passes through it. We can differentiate between and specify different behaviors of the component based on general errors or data truncations. For now, we will leave the default values in place, which means that if a row fails to be passed through the source into the data flow, it will cause the component to fail.

To complete the simple data flow, we will also need an OLE DB destination. In the destination, we can choose if we want to load data into a named object or an object that will be pulled from a variable. For each of these options, we can also choose if we want to see the fast load options. These options will allow us to optimize the performance of the load; we will talk more about this in the next section of the chapter. In our case, we will use the table or view fast load data access mode and select the `staging.ImpressionStage` as the destination, as shown in figure 6.2.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F02_Carter.png)<br>
**Figure 6.2 OLE DB destination editor—Connection Manager page**

NOTE Leaving the default settings for the fast load options is a mistake. We should always spend time configuring these options. This forms part of the mistake of not optimizing the load, and we will discuss this in the next section.

On the Mappings page of the OLE DB destination, any input columns (columns coming from the previous component) will automatically be mapped to the destination columns (in the destination table) if their names are the same. The `ImpressionID` column is not mapped, however, because the name is different. Therefore, we must select it from the drop-down list. We could also modify the automappings if we needed to.

The Error Output page of the destination defines the behavior of the component should a row fail to be inserted. Just like the error output on the source, however, we will leave it with the default Fail Component. The columns in our destination table are so expansive that there is little chance of a recoverable row failing.

Now let’s execute the package in debug mode by clicking the Start button in the toolbar and see what happens. The results are shown in figure 6.3. You will notice that we have successfully loaded 954,912 rows into the staging table, but then there was an error at the Impressions CSV source. This is indicated by the red cross.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F03_Carter.png)<br>
**Figure 6.3 Failed data flow**

If we move over to the Progress tab, we can see that there was a data truncation error on row 999,969, as shown in figure 6.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F04_Carter.png)<br>
**Figure 6.4 Error in the Progress tab**

How annoying! There are only 1 million rows in the file! We don’t want to lose the whole file because of one bad row, do we? So let’s go back into the Error Output page of the flat file source editor and change the behavior of the component so that if any errors are encountered, we will ignore the row(s) with the error and continue loading the data. This is shown in figure 6.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F05_Carter.png)<br>
**Figure 6.5 Flat file source editor—Error Output page set to ignore failure**

> [!TIP]
>
> Click the Stop Debugging button before modifying the package.

> [!TIP]
>
> Before running the package again, truncate the staging data to avoid double-loading the data. We can do this with a `TRUNCATE TABLE Staging.ImpressionsStage` command.

Now if we run the package again, we will see that the data flow was successful. This will be denoted by each component having a green tick. So that’s great, right? Well, not really. In fact, it’s a mistake. It may have solved the short-term problem, but what if the next file has five bad rows? Or 50? What if it has 5,000 bad rows? The cookie ID is a significant column, which we need, so we should not be throwing away this bad data. But we also don’t want our package to fail every time. So what should we do?

The answer is to redirect bad rows to a different destination so that they can be examined by the application support team to decide if they can safely be thrown away or if we can fix the data. It might even be the case that the application owner needs to reach out to the vendor and request a new data extract. Data coming from untrusted sources is often dirty, and we should account for this in our code.

To fix the problem, let’s first create a new table, which will hold any rows that fail the task. We can achieve this using the script in the following listing.

Listing 6.2 Creating an error table

```sql
CREATE TABLE staging.ImpressionLoadFailures (
    FlatFileSourceErrorOutputColumn    VARCHAR(MAX)    NOT NULL,
    ErrorCode                          VARCHAR(MAX)    NOT NULL,
    ErrorColumn                        VARCHAR(MAX)    NOT NULL
) ;
```

Next, we will create a new OLE DB destination and name it Failed Rows. Pull the red connector from the data source onto the new data flow destination. With our current configuration, this will immediately cause the Configure Error output dialog box to be displayed. This dialog box is identical to the Error Output page of the data source editor. We should configure this page to redirect rows with errors and truncations.

Let’s now configure the new destination. On the Connection Manager page, we will point to our new `ImpressionLoadFailures` table as the destination table. On the Mappings page, we will need to manually map the Flat File Source Error Output Column, as our table does not have spaces in the column name, so automapping will not work. Our data flow will now look like figure 6.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F06_Carter.png)<br>
**Figure 6.6 Data flow with error output**

If we now run the package again, the failed row will be inserted into our `ImpressionLoadFailures` table. We can examine that row:

```sql
91C27789-2805-41EB-8E06-A4F5AD147817;jouktjkatujr.com,
7D522130-5A00-4C4B-83D5-EA0A7E1777CF,
54879,4433265,220,NULL,128,142,3.3635,1.2668,02/07/2023 07:56
```

We know that we are expecting the file to be comma separated, but we can see after the first Globally Unique Identifier (GUID) that there is a semicolon instead of a comma. This will cause the row to fail because of a truncation error on the Cookie ID column. This is because we have used the suggested data type of a 50-character string, and the length of the column (until we hit the first comma) is 53 characters.

> [!TIP]
>
> In some scenarios, we would also want to add a failure constraint to the data source destination. In our scenario, however, we have configured the staging table, so that all columns are `VARCHAR(MAX)`, and that is as expansive as we can be. Therefore, there is no need for a failure source, as we could not make its columns any more expansive.

We should always avoid throwing away bad data. The ignore-row behavior may be convenient, but if the data is important enough to load, it should be important enough to keep, even if a later review of the data reveals that we can’t repair it. If a particular column is not needed, then we should avoid loading it. It increases our chances of the package failing, and it makes the rows larger than they need be, which could also carry a performance penalty. This is discussed later in the chapter.

## 6.2 #23 Not optimizing data loads

In the previous section, while building our data flow, you may have noticed an additional mistake that we made—namely, not optimizing our data load. Without this optimization, our package will never perform optimally.

To optimize the load, we should configure properties on the data flow, which control the size of the data buffers that the data will be loaded into when they are read out of the CSV file. There are then fast load options on the OLE DB destination that can be used to optimize the bulk load into SQL Server.

Real-world package optimization

There have been numerous occasions where optimizing the load has been absolutely essential. An example I have seen repeatedly is when quarterly and annual financial transaction data has been loaded from financial systems into reporting tools and must be completed in a certain time window. Another great example, however, is when I have been working with the same data domain that we are discussing in this chapter.

In this chapter, we have a single flat file, with only 1 million rows in it. I remember, however, working for a large advertising group. We had to load impressions data, click data, and event data on a nightly basis, and then perform a complex click-path analysis. There were multiple files from multiple cookie providers. There were hundreds of millions of impressions, tens of millions of clicks, and millions of events that had to be processed every night.

Because of the size of our sample file, we will be looking to save seconds. In the real-world scenario, however, optimizing the data loads saved many minutes. This was crucial, as the files were often not delivered until after midnight and they had to be ready for the business by the start of the working day. The ETL processes took many hours to complete, and without the load optimizations, they would have overrun their windows, causing a business impact on a daily basis.

The properties on the data flow that control its size are `DefaultBufferMaxRows`, which controls the maximum number of rows within the buffer, and `DefaultBufferSize`, which controls the maximum size of the data buffer. The default maximum number of rows is 10,000, and the default maximum size is 1MB. The buffer size will be controlled by the limit that is reached first.

This can be confusing, so I recommend setting the `AutoAdjustBufferSize` property to `True`. This will result in not having to worry about the `BufferSize` property, as it will automatically be configured to match our chosen maximum number of rows within the buffer.

The fast load options within the OLE DB destination allow us to specify if an identity specification should be disabled on a column, if `NULL` values should be kept, if check constraints should be disabled, and if a table lock should be used. It also allows us to specify the maximum number of rows per batch and the maximum commit size of the insert.

Because we are loading data into a staging table, which is a flat heap that has no keys, constraints, or `NULL` constraints, we can ignore these settings for our scenario. If we did have these constraints, however, then there would be a performance penalty for having them enabled. We will ensure that the table lock option is checked, however, as this will avoid contention and reduce the overhead of lock escalation.

The maximum batch size and commit size are important values to configure. By default, data will be loaded into the table in a single transaction. This is likely to cause performance issues, due to excessive use of `TempDB` and filling up the transaction log. We suspect that reading data from the flat file will be slower than inserting data into the staging table, so we will keep the maximum batch and commit size in line with the number of rows in our data flow source.

Unfortunately, there is no magic number for the number of rows that should be in each buffer or in each batch. The optimum values will be very specific to your unique environment, resource profile, workload profile, and data profile. The way to find the optimum value is to test the data flow with multiple size options.

TIP When doing this, I usually start with a small value and increase the size in increments. This usually results in incremental performance improvements, which start to tail off. When execution times start to increase, I know that I have hit the optimum size.

Table 6.1 contains the results of various tests I performed on the data flow. The first column details the maximum number of rows in the buffer, which is aligned with the maximum number of rows per batch. The second column shows the execution time, in seconds, for the package. These execution times can be pulled from the Progress tab before stopping execution or from the Execution Results tab after execution stops.

Table 6.1 Buffer size performance tests

| Buffer/batch size | Execution time (seconds) |
| --- | --- |
| 1,000 | 49.032 |
| 5,000 | 36.828 |
| 10,000 | 35.813 |
| 20,000 | 34.875 |
| 50,000 | 34.328 |
| 75,000 | 35.125 |

TIP If you want to run the tests on your own environment, remember to truncate the staging table each time to keep it a fair test. Also remember that your mileage will vary, depending on the specifications of your computer and other processes that may be running.

In my environment, it is clear to see that 50,000 rows is the optimal size of the buffer. We should also check out the hypothesis that the flat file source will be the limiting factor, however. Therefore, let’s run some more tests. This time, we will keep the `MaximumRowsPerBuffer` at 50,000 for each test but tweak the batch size. The results of these tests can be found in table 6.2. Once again, remember that the results will be specific to your unique environment, so your mileage may vary.

Table 6.2 Batch size performance tests

| Batch size | Execution time (seconds) |
| --- | --- |
| 25,000 | 35.125 |
| 75,000 | 34.962 |

As expected, the results show that reading the flat file was the limiting factor and we cannot gain a performance improvement by removing the alignment of the batch size from the buffer size.

We should always optimize our load performance. Suboptimal performance can cause packages to overshoot their ETL windows. This could have an effect on the business or on maintenance processes such as backups.

## 6.3 #24 Using SSIS as a T-SQL orchestration tool

A common mistake I see made by developers who are new to SSIS is to avoid using data flows and to simply perform loads between tables or transformations using Execute T-SQL tasks on the control flow.

This approach results in developers not harnessing the power of SSIS. It means that we are just running standard T-SQL. The ETL will not become loosely coupled from our data sources, ETL developers who come after us will not have a graphical representation of the logic, and we will not be able to take advantage of SSIS error handling and logging. A bad row will result in a whole query failing, rather than being able to redirect an individual bad row. Additionally, we will lose the ability to explore where our data is within a pipeline, making it harder to debug performance issues.

SSIS performance

It is worth noting that if the source and destination tables are both in the same server, then using native T-SQL will likely be more performant than transforming data using data flow pipelines, because it performs better at operations like joining and merging data. Therefore, for some large loads, where a single database server is involved, native T-SQL might be the correct approach, especially if we have a narrow ETL window. We should use the right tool for the job.

Arguably, however, in this scenario, there may be little benefit in the overhead of running SSIS at all, and we may be better off running a SQL Server Agent job and using job steps to orchestrate the workflow. When loading and transforming data between servers, SSIS can normally outperform a linked server.

It is also worth weighing up performance versus functionality. If we do not have any constraints around the length of the ETL window, we may consider that the error handling, logging, and debugging tools of SSIS are more important considerations than speed.

To explore the effects of using SSIS as a pure orchestration tool, let’s expand our package to transform data from our staging table into our core marketing table and then on into the reporting table. Finally, our package should truncate the staging table. The following section will discuss how these tasks may be created using the control flow as an orchestration tool for Execute T-SQL Statement tasks.

### 6.3.1 Creating an Execute T-SQL Statement orchestration

Our first step in enhancing the package will be to create a new ADO connection manager. This connection manager will be used by our Execute T-SQL Statement tasks to connect to our SQL Server instance.

Once we have a connection manager, we can now create our tasks that will execute our SQL statements. We should configure the first new Execute T-SQL Statement task to execute the query in listing 6.3. This query performs an `INSERT` statement, converting data where SQL Server is not able to perform an implicit conversion. The `SET DATEFORMAT` statement at the start is required, so that SQL Server recognizes the dates in the `EventTime` column. Without this, the task would fail with data type overflow errors.

Listing 6.3 Loading data into the `Impressions` table

```sql
SET DATEFORMAT DMY
INSERT INTO marketing.Impressions (
    ImpressionUID,
    ReferralURL,
    CookieID,
    CampaignID,
    RenderingID,
    CountryCode,
    StateID,
    BrowserVersion,
    OperatingSystemID,
    BidPrice,
    CostPerMille,
    EventTime
)
SELECT
      ImpressionUID
    , ReferralURL
    , CookieID
    , CampaignID
    , RenderingID
    , CASE
        WHEN CountryCode = 'NULL' THEN NULL
        ELSE CountryCode
      END CountryCode
    , CASE
        WHEN StateID = 'NULL' THEN NULL
        ELSE StateID
      END StateID
    , BrowserVersion
    , OperatingSystemID
    , CAST(BidPrice AS MONEY)
    , CAST(CostPerMille AS MONEY)
    , EventTime
FROM staging.ImpressionsStage ;
```

The next Execute T-SQL Statement task we will create is called Merge Aggregates, which is used to populate the `reporting.ImpressionAggregates` table. For this task, we should use the query in listing 6.4. This query uses a `MERGE` statement to roll the data up by `CampaignID`, `CountryCode`, and `EventDate` before loading the aggregated table. If a row already exists for the campaign, country, and date, then the average bid price and average CPM will be updated. Otherwise, a row will be inserted.

Listing 6.4 Loading the `Aggregates` table

```sql
MERGE INTO reporting.ImpressionAggregates AS Target
USING (
    SELECT
          CampaignID
        , CountryCode
        , AVG(BidPrice) AS AvgBidPrice
        , AVG(CostPerMille) AS AvgCostPerMille
        , CAST(EventTime as DATE) AS EventDate
    FROM marketing.Impressions
    GROUP BY
          CampaignID
        , CountryCode
        , CAST(EventTime as DATE)
) AS source
ON (
    Source.CampaignID = Target.CampaignID
        AND Source.CountryCode = Target.CountryCode
        AND Source.EventDate = Target.EventDate
)
WHEN MATCHED THEN
    UPDATE SET
          AvgBidPrice = Source.AvgBidPrice
        , AvgCostPerMille = Source.AvgCostPerMille
WHEN NOT MATCHED THEN
    INSERT (
           CampaignID,
           CountryCode,
           EventDate,
           AvgBidPrice,
           AvgCostPerMille
           )
    VALUES (
           Source.CampaignID,
           Source.CountryCode,
           Source.EventDate,
           Source.AvgBidPrice,
           Source.AvgCostPerMille
           ) ;
```

Our final task will truncate the staging table at the end of the load using the statement in listing 6.5. Truncating a table is a perfectly acceptable usage of an Execute T-SQL Statement task. This task cannot be performed as part of the data flow. We will keep this task, even when we optimize the package to replace the previous Execute T-SQL Statement tasks with data flows.

Listing 6.5 Truncating the `Staging` table

```sql
TRUNCATE TABLE staging.ImpressionsStage ;
```

All our tasks should be joined together by success constraints. This will force the tasks to run sequentially. It also means that if a task fails, the subsequent tasks will not run. Figure 6.7 illustrates how our control flow will look at this point.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F07_Carter.png)<br>
**Figure 6.7 Package used to orchestrate T-SQL tasks**

The package is now complete, but it is less than ideal for loading data, as in this case, the data comes from an untrusted source and is likely to be dirty. Let’s imagine that we have loaded a dirty row into our staging data. Because of the expansive columns in our staging table, we have managed to get the data this far, but when we try to load the `marketing.Impressions` table, the row fails because of a failure to convert between data types. In this scenario, the whole task will fail. It will also be tricky to track down the reason for the failure, as SQL Server will not report the specific row that failed.

So how could we create a better package? The answer is to use data flow tasks. To do this, let’s delete the Load Marketing Impressions and Merge Aggregate tasks. The following section will discuss how to replace them with data flows.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F08_Carter.png)<br>
**Figure 6.8 Load Marketing Impressions data flow**

### 6.3.2 Converting Execute T-SQL Statement tasks to data flows

The Load Marketing Impressions data flow will look similar to the flow illustrated in figure 6.8. You will see that there is a source, which is reading the data from the staging table. There is then a derived column transformation, which converts the columns to the correct data types, before a data flow destination inserts the rows into the `marketing.Impressions` table.

The source and destination are straightforward components. It is the derived column transformation where the magic happens. In figure 6.9, you will notice that we have used *SSIS expressions,* which are a combination of columns, functions, operators, and literals, to create new output columns that contain the converted values. The Derived Column Name field is free text and allows us to specify a name for the output column. The Derived Column field is a drop-down, allowing us to select the name of the column we wish to replace or to add the output as a new column. The Expression field can be built entirely freeform. Alternatively, column, variable, and parameter names can be dragged from the top-left area of the dialog box. The top-right area of the dialog box allows us to drag functions into the expression field. The Data Type field is automatically populated based on the expression. There are also other fields to the right, which are automatically populated based on the output data type. These fields include Length, Precision, Scale, and Code Page. If there is an error in an expression, then it will turn red, and hovering over the expression will display the details of the parse error.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F09_Carter.png)<br>
**Figure 6.9 Derived column transformation**

The first two expressions are converting GUID values. For SSIS to recognize the values as GUIDs, they must be enclosed in curly braces. Therefore, we explicitly cast the input column to a 36-character Unicode string and add the curly braces before and after. We then convert the entire value to the `DT_GUID` data type.

The expression to convert the `EventTime` column first explicitly converts the input to a 26-character string, replacing the `/` characters with `–` characters. This is because SSIS can’t recognize the data format used in the staging table. Finally, we cast the results to the `DB_TIMESTAMP` data type.

The expression to convert the URL simply performs an explicit cast to a string with the 1252 code page. This code page is synonymous with the Windows 1252 code page.

The expression to convert the `StateID` column is arguably the most interesting of the expressions. We are using this expression not just to change the data type but also to replace the string value of `"NULL"` with a `NULL` value. To those of you with a .NET background, the syntax may be somewhat familiar, but to understand this expression, let’s break it down into parts. The first part is to the right of the `:` and passes `StateID` as an input column, after explicitly casting it as a 50-character string. On the left side of the `:` we are checking if the value within `StateID` (which, again, has to be explicitly cast) is equal to `"NULL"`. If it is, then we replace this value with a `NULL` value. There is another cast here, because in SSIS there is a different `NULL` value type for every data type. For example, a `NULL(DT_WSTR,50)` is a different type to a `NULL(DT_I4)`. Finally, outside of the brackets, on the far left, we convert the final value to a `DT_UI1`, which is a single-byte unsigned integer.

The expressions used in this transformation are detailed in the following listing.

Listing 6.6 SSIS expressions

```sql
(DT_GUID)("{" + (DT_WSTR,36)(ImpressionUID) + "}")          ①
(DT_GUID)("{" + (DT_WSTR,36)(CookieID) + "}")               ②
(DT_DBTIMESTAMP)(REPLACE((DT_WSTR,26)EventTime,"/","-"))    ③
(DT_TEXT,1252)ReferralURL                                   ④
(DT_UI1)((DT_WSTR,50)StateID == "NULL" ? NULL(DT_WSTR,50) : ⑤
(DT_WSTR,50)StateID)                                        ⑤
```

① ConvertedImpressionUID

② ConvertedCookieUID

③ ConvertedDate

④ ConvertedURL

⑤ ConvertedStateID

The biggest advantage of this data flow over the corresponding Execute T-SQL Statement task is the ability to handle errors. As discussed earlier in this chapter, we could enhance this data flow to include error paths so that failed rows can be redirected to a data errors table. This would allow an application support team to deal with bad rows that cannot be converted into the more restrictive data types as part of business-as-usual operations. With the Execute T-SQL Statement task, the entire insert would fail and need to be debugged, and the data fixed, before any data from the load is available to the business.

The Merge Aggregates data flow will look similar to figure 6.10. This flow starts with a data flow source, which reads the impressions from the `marketing.Impressions` table. There is then a derived column transformation, which rolls up the date and time data to rounded dates. The aggregate transformation then calculates the average bid price and CPM. Finally, there is an upsert destination, which inserts rows into the `reporting.ImpressionAggregates` table if the key combination does not already exist or updates the aggregated values if the key combination does already exist.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F10_Carter.png)<br>
**Figure 6.10 Merge Aggregates data flow**

Transformational data flows such as this are often far more complex than this example. For complex transformations, a data flow can have many advantages over an Execute T-SQL Statement task. Not only can failed rows be redirected individually, but we also have a graphical view of our logic. This can help make the business logic clear to other developers. We also have the ability to watch our flow execute in real time and understand where errors occur or where performance is slow. This can assist the development process.

The Read Marketing Impressions source is a simple read from the base table. The Rollup DateTime To Date transformation is a derived column transformation, which generates a new output column using the SSIS expression `(DT_DBTIMESTAMP)(DT_DBDATE)EventTime`. This expression first converts the value to `DT_DBDATE`, which removes the time component. For ease of data type matching, it then immediately converts the value back to `DT_DBTIMESTAMP`, which adds a time component back in, but as the time information is now missing, it will be set to `00:00:00.000`.

The Aggregate Impression Costs transformation uses the aggregate transformation to calculate the average values of `BidPrice` and `CostPerMille`, grouped by the `CampaignID`, `CountryCode`, and `EventTime` columns. Figure 6.11 shows the Aggregations page of the Aggregate Transformation Editor. You will notice that in the top half of the dialog box we have selected the appropriate input columns for the transformation, and in the lower half, we have specified the appropriate aggregation that we want to apply. For the measures, we have selected Average, and we have selected Group By for the key columns. This will generate different average cost data for each unique combination of `CampaignID`, `CountryCode`, and `EventTime`.

> [!TIP]
>
> Other aggregate functions available are Count, Count Distinct, Sum, Minimum, and Maximum.

Blocking transformations

It is worth noting that SSIS transformations can either be blocking or non-blocking. *Non-blocking* transformations, such as the derived column transformation, will output rows within batches as soon as those batches have been processed. Other transformations, however, do not pass any outputs until all rows have been processed. These are known as *blocking* transformations, as downstream components cannot begin until the blocking transformation has completely finished its processing. The aggregate transformation is a blocking transformation. This is because it cannot confirm aggregate values until all rows are received. Otherwise, it could be processing and passing out incorrect aggregations, as the aggregate values would likely change if more rows with the same group by keys were received.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F11_Carter.png)<br>
**Figure 6.11 Aggregate Transformation Editor**

The upsert destination is not an out-of-the-box SSIS component. While there are various methods to achieving the result with native SSIS components, they are deemed a poor practice, as they often exhibit very poor performance. There are various upsert destinations available, including offerings from COZYROC and SentryOne, but for this chapter, we will use the destination from ZappySys, which is part of their SSIS PowerPack offering. It can be purchased from <https://mng.bz/2gVd>.

In the Settings page of the Upsert Destination dialog box, we will choose the operation that we want to perform. In our scenario, that is an upsert, but the component also supports sync (upsert + delete), bulk update, and bulk delete. We will then select the data source we want to use and the table that will be our destination. We can also tweak our batch size for performance.

In the Column Mappings tab of the Upsert Destination dialog box, shown in figure 6.12, we will map all of our source columns and check the Key option against `CampaignID`, `CountryCode`, and `EventTime`. The key columns designate the business keys that will be used to match the rows when deciding if a row should be inserted or if an update should be made.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F12_Carter.png)<br>
**Figure 6.12 Upsert destination—Column Mappings tab**

In the Advanced page of the dialog box, we have the ability to configure commands that will be executed before and after the insert. This can be useful for creating and removing indexes that will support the load. It can also be used for specifying hints, such as imposing a table lock.

When performing ETL operations between servers, using data flows is invariably a good idea as opposed to using SSIS as an orchestration tool for executing T-SQL scripts. When performing ETL operations between tables on the same server, SSIS can perform slower than a T-SQL script; however, it may still be a better option if we need the out-of-the-box logging, error handling, and debugging features, as well as the ability to fail individual rows.

## 6.4 #25 Extracting all data when we only need a subset

When building a data flow, it can be tempting to always configure our data flow source to read all data from the source table. Extracting a whole table is the default option. Choose our table in the drop-down box and away we go.

In some scenarios, we really will need to extract and process all data from a table, and if this is the case, then there is nothing wrong with using this option. If we only need a subset of the data from a table, however, then doing this can have a drastic impact on performance. Let’s take our Merge Aggregates data flow, for example. In the previous section, we configured the data flow source to simply pull all data from the `marketing.Impressions` table, as shown in figure 6.13.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F13_Carter.png)<br>
**Figure 6.13 Original data source configuration**

If I execute the data flow task on my test rig, it takes 16.403 seconds to complete.

If we examine the data flow, however, we will notice that the only columns we need in the flow are `CampaignID`, `CountryCode`, `EventTime`, `BidPrice`, and `CostPerMille`. If we removed the other columns, which are unnecessary, then the buffers would be smaller and more performant.

Let’s also imagine that there is a business rule, which means that backdated impression data is only supplied for the last 30 days. Therefore, we know that any data older than 30 days won’t change, so the aggregates do not need to be reprocessed. We could reduce the number of data buffers that the data flow needs to process by filtering the data at source.

To achieve this, let’s reconfigure our data flow source to load data using a command rather than extracting all data from the table. The reconfigured data flow is shown in figure 6.14.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH06_F14_Carter.png)<br>
**Figure 6.14 New data flow configuration**

The query used in this data flow source can be found in listing 6.7. The query pulls only the required columns and filters the data, so that only impressions with an `EventTime` inside the last 30 days are pulled into the flow.

Listing 6.7 Loading impression data

```sql
SELECT
    CampaignID,
    CountryCode,
    BidPrice,
    CostPerMille,
    EventTime
FROM marketing.Impressions
WHERE (EventTime >= GETDATE() - 30) ;
```

When I executed the data flow on my test rig with the new configuration, it completed in 2.937 seconds. If we have a tight ETL window, performance enhancements like this, implemented throughout our packages, could make a huge difference.

> [!TIP]
>
> Because the filter is based on date, if you are following along with this example in the future, it is likely that you will not return any rows. You can resolve this by either updating the event times in your base table or altering the `WHERE` clause in the data flow source query.

If we do not need all of the data from our source, then we should filter it at source to reduce the size and number of data buffers required within our flow.

## Summary

* When loading data from an external source, we should always try to keep as much data as possible. Consider using staging tables with expansive column types, and use the Redirect Rows feature to pull bad rows into an error-handling table.
* Tweaking data flow settings, such as the `MaximumRowsPerBuffer`, can have a significant impact on performance. This can be important, especially when we have a tight ETL window.
* When loading data into SQL Server, use OLE DB sources and destinations where possible for performance. Always tweak OLE DB destination fast load options to optimize performance for your environment.
* There is no magic bullet configuration. Test various configurations to see which performs best in a particular environment.
* Avoid using SSIS as an orchestration tool for Execute T-SQL tasks. Doing this loses many of the benefits of using SSIS, such as out-of-the-box logging and error handling.
* Avoid extracting all the data from a table unless you really need to. We can get good performance improvements in our package by limiting the number of columns and rows returned to match what is required by our data flow. If you don’t need all the data in the table, use a data flow source with a SQL command and filter the data at source.
