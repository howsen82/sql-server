# 5 T-SQL development

This chapter covers

* Mistakes that can cause unexpected results
* Mistakes that can lead to performance problems
* Avoiding looping with cursors in T-SQL
* Deleting large numbers of rows

SQL is an ANSI and ISO standard language that allows database developers to interrogate and manipulate data within a relational database. T-SQL is SQL Server’s dialect of the SQL language and is used to interact with SQL Server instances and the databases hosted within.

In chapter 4, we designed and created the tables for a new MagicChoc database. In this chapter, we will explore some of the common mistakes that can be made in T-SQL by developers who are less experienced with the language. For examples of T-SQL development, we will look to MagicChoc, which wants us to develop the logic that will be used by its frontend applications. We will use this as an opportunity to start exploring some of the common mistakes that can be made in T-SQL by developers who are less experienced with the language.

Getting to grips with SQL can be a challenge for developers who are more familiar with writing application code, using languages such as C# or Visual Basic, because of the large conceptual difference between how the languages work. For example, looping in .NET languages is perfectly acceptable, but in the set-based world of SQL, it can cause serious performance problems.

Most development mistakes in T-SQL cause performance issues, and that will be the main focus of this chapter. The first two sections, however, will focus on mistakes leading to unexpected results. Finally, we will look at a common mistake that is made when deleting large numbers of rows.

## 5.1 #14 Dealing incorrectly with NULL values

MagicChoc has asked us to look at the data and confirm how many product subcategories do not have a description. Therefore, we run the query in the following listing.

Listing 5.1 Incorrectly counting `NULL` values

```sql
SELECT COUNT(*)
FROM dbo.ProductSubcategories
WHERE ProductSubcategoryDescription = NULL ;
```

`0` is returned as the result. Fantastic. Every product subcategory has a description, right? We know there are 16 subcategories in total, so let’s just double-check our result by flipping the query to count the number of rows where the description isn’t `NULL`, using the query in the following listing.

Listing 5.2 Incorrectly counting non-`NULL` values

```sql
SELECT COUNT(*)
FROM dbo.ProductSubcategories
WHERE ProductSubcategoryDescription <> NULL ;
```

Wait a minute! This query is returning `0` as the result as well. What is going on here? What developers new to SQL sometimes don’t realize is that a `NULL` is an unknown value. Therefore, in comparison, a `NULL` is not equal to another `NULL`.

To understand this, think about the following analogy. How many stars are there in our galaxy? Personally, I do not know the answer to this. How many grains of sand are there in the world? Again, I personally have no idea. Does that mean that the number of stars in the galaxy is equal to the number of grains of sand in the world? No, of course, it doesn’t. It might be; it might not be. I have no idea. Therefore, just as I cannot say that two values that I don’t know are the same or different, SQL Server cannot tell us if two values it does not know are the same or different.

To deal with this problem, we just need to tweak our syntax when dealing with `NULL` values to use `IS` or `IS NOT` instead of `=` and `<>`. For example, the script in the following listing successfully returns a count of product subcategories that do not have a description, followed by a count of product subcategories that do have a description.

Listing 5.3 Successfully returning a count of `NULL` and non-`NULL` values

```sql
SELECT COUNT(*)
FROM dbo.ProductSubcategories
WHERE ProductSubcategoryDescription IS NULL ;

SELECT COUNT(*)
FROM dbo.ProductSubcategories
WHERE ProductSubcategoryDescription IS NOT NULL ;
```

Another aspect of dealing with `NULL` values that can confuse people who are new to SQL is the use of `IS NULL` versus the `ISNULL()` function. As we have just seen, `IS NULL` is used in a `WHERE` clause to filter a result set so that it returns only rows where the column contains `NULL` values.

The `ISNULL()` function, on the other hand, is used in a `SELECT` list, `JOIN` clause, or the `SET` clause of an `UPDATE` statement to replace a `NULL` value with a value that is not `NULL`. For example, the query in the following listing will return a complete list of product subcategories, but descriptions that are `NULL` will be replaced with the value `No description available`.

Listing 5.4 Using the `ISNULL()` function to replace a `NULL` value

```sql
SELECT
      ProductSubcategoryName
    , ISNULL(ProductSubcategoryDescription, 'No description available')
FROM dbo.ProductSubcategories ;
```

Always take care when dealing with `NULL` values. Remember that a `NULL` value is not equal to another `NULL` value. Also remember that the `IS NULL` syntax is used to filter a query to return `NULL` values, while the `ISNULL()` function is used to replace a `NULL` value with a non-`NULL` value.

## 5.2 #15 Using NOLOCK as a performance tweak

MagicChoc’s sales application has a drop-down box filled with addresses related to a customer, which allows the salesperson to select the delivery address they want an order to be shipped to. When the delivery address screen loads, however, the drop-down list is slow to populate, and we have been asked to improve the performance of the query.

We have heard that locking and blocking can cause performance issues in SQL Server, and someone has mentioned that there is a query hint, called `NOLOCK`, which can improve performance. We therefore amend the query that populates the delivery address drop-down, so that it reads as per the following listing.

Listing 5.5 Adding the `NOLOCK` query hint

```sql
DECLARE @CustomerID INT ;
SET @CustomerID = 2 ;

SELECT
      Street
    , Area
    , City
    , ZipCode
FROM dbo.Addresses a WITH(NOLOCK)
INNER JOIN dbo.Customers c
    ON a.AddressID = c.DeliveryAddressID
WHERE CustomerID = @CustomerID ;
```

Everything is fine for a while, but one day, an order is delivered to an incorrect address, and we are asked to investigate how this could have happened.

Imagine the following scenario. A salesperson is processing an order for Cooking Schmooking. At the same time, an administrator is speaking to another member of the Cooking Schmooking team about updating various details, including a change of address. The administrator realizes that they have input the incorrect address and cancels the customer update before it completes. The administrator then continues on to input the correct address. Let’s examine the exact sequence of events in figure 5.1.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F01_Carter.png)<br>
**Figure 5.1 Sequence of events**

SQL Server uses locking to make sure that a transaction cannot read data that is currently being modified by a different transaction. By using the `NOLOCK` query hint, we have prevented our `SELECT` statement from taking out any locks. The result is that the salesperson reads the address from the table while the transaction performing the update is in flight. The transaction that was performing the update is then rolled back. This results in the salesperson reading a delivery address that was never actually committed and, therefore, never really existed in the database.

There are many ways to optimize SQL Server performance, and tuning locking through the use of transaction isolation levels, which affect how locks are maintained, is one option. We will be discussing this in chapter 10. I would always recommend, however, avoiding the use of `NOLOCK` against a query, as it is an opaque optimization that usually adds more risk than reward.

## 5.3 #16 Using SELECT * as standard

Let’s now take a look at mistakes that can lead to performance issues. Let’s start by considering a scenario where we are developing a procedure that returns information regarding sales areas. We can’t remember the columns in the table, or what the data looks like; therefore, we run the following ad hoc query:

```sql
SELECT *
FROM SalesAreas
```

A colleague then asks us, “Do we store best-before dates for products?” We are not sure, so we run the following ad hoc query, so that we can answer our colleague:

```sql
SELECT *
FROM Products
```

Both of these examples are a perfectly valid usage of `SELECT *`. The mistake that I want to discuss is when developers use `SELECT *` in code that they plan to release and maintain.

Let’s imagine that our application is going to perform some analysis on how long orders take to be delivered and how many of them are late. To meet this requirement, we need to return the following columns to the frontend application:

* `SalesOrderDate`
* `SalesOrderDeliveryDueDate`
* `SalesOrderDeliveryActualDate`

Instead of selecting these three specific columns, however, we decide to use `SELECT *`. There are three reasons why this practice is a mistake: performance, code maintenance, and code readability. Let’s first discuss performance.

There are two aspects of performance that we should consider when we think of `SELECT *`. The first is sending data to the application tier. In this case, our application needs us to send it three columns, totaling 9 bytes. If we were to send all columns to the application, then the size per row would be up to 61 bytes per row. That is an extra 51 bytes per row. Imagine that there are 2.5 million sales orders in the table. That’s an extra 121MB of data that we are sending across the network for no reason.

Now imagine that we used the same technique for a table that had several `NVARCHAR(MAX)` columns, which can store up to 2 GB each. What if multiple users were running the same query at the same time? You can see how this can easily become an issue.

The second aspect of performance we should consider relates to indexes. To explain this problem, let’s imagine that the exact requirement is to return the three columns, filtered by `SalesOrderDate`. To satisfy this query in the most efficient way, we could create what is known as a *covering index*—that is, an index that includes all columns that we want to return.

For example, listing 5.6 creates an index built on the `SalesOrderDate` column but then includes the `SalesOrderDeliveryDueDate` and `SalesOrderDeliveryActualDate`. When you use the `INCLUDE` syntax, SQL Server generates the index on the main column(s) but then includes the values of the included columns at the leaf level. This is especially useful for covering queries, where you need to filter or join on a given column but then also wish to return a small set of other columns in the results. This is because it minimizes the size of the index while avoiding a lookup operation to the clustered index to retrieve the other columns.

Listing 5.6 Creating a covering index

```sql
CREATE NONCLUSTERED INDEX [OrderDate-Including-DueDate-ActualDate]
    ON dbo.SalesOrderHeaders (SalesOrderDate)
INCLUDE(SalesOrderDeliveryDueDate,SalesOrderDeliveryActualDate) ;
```

Unfortunately, if you use `SELECT *`, then indexes like this will almost certainly not be used, as they do not include all the columns that you are returning. It would, of course, be possible to include every column in the table, but this would make a very wide, inefficient index. Then, if we were to add another column to the table, the index would stop working unless we remembered to update the index definition as well.

That brings us nicely to the second problem with the `SELECT *` approach, which is code maintenance. Imagine an application that acts as a middleware layer. It pulls order dates from the `SalesOrderHeaders` table using a query such as

```sql
SELECT *
FROM dbo.SalesOrderHeaders
WHERE SalesOrderDate = '20230616' ;
```

The data is then passed into a stored procedure on another instance, which performs the analytics. Because we are passing all columns from the table, the table type on the analytics instance is created as follows:

```sql
CREATE TYPE SalesOrdersForAnalysis AS TABLE
(
    SalesOrderNumber                NCHAR(12)       NOT NULL,
    SalesOrderDate                  DATE            NOT NULL,
    SalesPersonID                   INT             NOT NULL,
    SalesAreaID                     INT             NOT NULL,
    CustomerID                      INT             NOT NULL,
    SalesOrderDeliveryDueDate       DATE            NOT NULL,
    SalesOrderDeliveryActualDate    DATE            NULL,
    CurrierUsedforDelivery          NVARCHAR(32)    NOT NULL
) ;
```

The stored procedure is then declared in the following way:

```sql
CREATE PROCEDURE dbo.AsyncAnalysis
    @DatesForAnalysis SalesOrdersForAnalysis READONLY
AS
BEGIN
    SELECT *
    FROM @DatesForAnalysis ;

    --Analysis logic here...
END
```

In this scenario, if we add a column to the `SalesOrderHeaders` table, we will need to then perform the following steps:

1. Drop the `AsyncAnalysis` stored procedure, because it depends on the `SalesOrdersForAnalysis` type.
2. Drop the `SalesOrderForAnalysis` type.
3. Recreate the `SalesOrderForAnalysis` type.
4. Recreate the `AsyncAnalysis` stored procedure.

In short, the code update would have been far simpler if we had just used the columns that we needed, as opposed to all columns in the table. There have been multiple times in my career where I have come across problems like this, and it exponentially increases the time it takes to perform a simple change.

The final reason why we should avoid the `SELECT *` approach relates to code readability. In chapter 2, we discussed the benefits of having self-documenting code. Using `SELECT *` breaks the self-documenting code model and makes your code more opaque and harder for you and other developers to maintain in the future, as what you are trying to achieve is less obvious.

I recommend always avoiding `SELECT *` in code that you need to release and maintain. It can have a negative impact on performance due to increasing network load and forcing the use of less efficient index operations to retrieve the data. It makes your code harder to maintain in cases where there are downstream dependencies. It also makes your code harder to read and breaks the self-documentation model.

## 5.4 #17 Unnecessarily ordering data

We have been asked to perform some reporting against our customer contacts, and we have decided that the data might be more useful if we order it by email addresses. Before we examine this, however, let’s generate some data for the `CustomerContacts` table, using the script in the following listing, which will generate 3.2 million rows of data.

Listing 5.7 Generating data for the `CustomerContacts` table

```sql
DECLARE @FirstName TABLE (FirstName NVARCHAR(32)) ;

DECLARE @LastName TABLE (LastName NVARCHAR(32)) ;

DECLARE @domain TABLE (Domain NVARCHAR(250)) ;

DECLARE @topleveldomain TABLE (TLD NVARCHAR(6)) ;

DECLARE @email TABLE (Email NVARCHAR(256)) ;

INSERT INTO @FirstName
VALUES
    ('Rachel'),
    ('Seth'),
    ('Tony'),
    ('Angel'),
    ('Isabell'),
    ('Robert'),
    ('Adelaide'),
    ('Jessie'),
    ('Paxton'),
    ('London'),
    ('Jadyn'),
    ('Corey'),
    ('Maximo'),
    ('Johan'),
    ('Mariah'),
    ('Raven'),
    ('Hamza'),
    ('Cristofer'),
    ('Molly'),
    ('Malcolm') ;

INSERT INTO @LastName
VALUES
    ('Hill'),
    ('Acosta'),
    ('Oconnell'),
    ('Jefferson'),
    ('Cross'),
    ('Patel'),
    ('House'),
    ('Price'),
    ('Morales'),
    ('Reeves'),
    ('Rice'),
    ('Drake'),
    ('Briggs'),
    ('Henry'),
    ('Aguilar'),
    ('Holloway'),
    ('Burnett'),
    ('Aguilar'),
    ('Simon'),
    ('Barry') ;

INSERT INTO @domain
SELECT
    CONCAT(FirstName, LastName)
FROM @FirstName
CROSS JOIN @LastName ;

 INSERT INTO @topleveldomain
 VALUES
    ('.net'),
    ('.com'),
    ('.co.uk'),
    ('.eu'),
    ('.ru'),
    ('.edu'),
    ('.gov'),
    ('.ninja'),
    ('.io'),
    ('.co'),
    ('.ai'),
    ('.ca'),
    ('.me'),
    ('.de'),
    ('.fr'),
    ('.ac'),
    ('.am'),
    ('.ax'),
    ('.ba'),
    ('.ch') ;

INSERT INTO @email
SELECT
    CONCAT(Domain, TLD)
FROM @domain
CROSS JOIN @topleveldomain ;

INSERT INTO dbo.CustomerContacts(
    CustomerContactFirstName,
    CustomerContactLastName,
    CustomerContactEmail
)
SELECT
      FirstName
    , LastName
    , Email
FROM @FirstName
CROSS JOIN @LastName
CROSS JOIN @email ;
```

Now that we have some data, let’s meet our brief and write a query that returns the `CustomerContactFirstName`, `CustomerContactLastName`, and `CustomerContactEmail` columns from the `CustomerContacts` table. Before running this query, the script in the following listing runs the command `SET STATISTICS TIME ON`, which will return execution time statistics in the messages window of SSMS.

Listing 5.8 Returning required data from the `CustomerContacts` table

```sql
SET STATISTICS TIME ON ;

SELECT
      CustomerContactFirstName
    , CustomerContactLastName
    , CustomerContactEmail
FROM dbo.CustomerContacts ;
```

On my test rig, that query took 20,679 ms to run. So now let’s try that again, but this time we will order the data by `CustomerContactEmailAddress`, as shown in the following listing.

Listing 5.9 Returning data from `CustomerContacts` ordered by email

```sql
SET STATISTICS TIME ON ;

SELECT
      CustomerContactFirstName
    , CustomerContactLastName
    , CustomerContactEmail
FROM dbo.CustomerContacts
ORDER BY CustomerContactEmail ;
```

TIP If you are following along with measuring performance, it is important to note that your mileage may vary depending on the performance of your hardware, as well as other processes that may be running. I also advise against capturing execution plans at the same time as testing performance, as this will affect the result of the time collection.

On the same test rig, this query took 27,415 ms to execute. That is 25% slower than the unordered version of the query. The reason for this is that relational databases are built on the branch of mathematics known as *set theory*, where we consider a *set*, which is a distinct group of objects, and a *bag (or multiset)*, which is a collection of objects that may contain duplicates. In both mathematical concepts, the order of the results is unimportant. Therefore, ordering data is not a set-based operation. Instead, it is a presentational operation only. Therefore, you should order data only if you really need to.

We can see this represented in the relative cost of the sort operation if we look at the execution plan, which is the steps, or operators, that the query optimizer has decided to take to satisfy the query. You can access execution plans in multiple ways, including via Query Store, which we will discuss in chapter 10, or metadata, which we will also discuss in chapter 10, or simply by pressing the “Include Actual Execution Plan” button on the toolbar in SQL Server Management Studio prior to executing the query.

The execution plan generated for the query in listing 5.9 is illustrated in figure 5.2. You will notice that the sort operation is 91% of the estimated cost of the query.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F02_Carter.png)<br>
**Figure 5.2 Execution plan sort operation cost**

Execution plan of sort operation cost

In the context of an execution plan, there are a couple of things to bear in mind. First, cost is estimated when the query is compiled, and SQL Server does not update the costs post-execution. Therefore, even when displaying the actual execution plan (as opposed to the estimated execution plan—which can be viewed prior to query execution)—you will see estimated costs. These costs can be incorrect due to factors such as inaccurate statistics.

Second, cost is not a direct measurement of performance. It is a weighted value, calculated based on a proprietary algorithm that estimates a relative cost for processor and IO and then sums these values to derive a total cost for the operator, known as a *subtree cost*.

If a situation arises where you simply must order your data for presentational reasons, then using indexes can help. For this query, the ideal index to cover the query would be built on `CustomerContactEmail` as the index key and include the `CustomerContactFirstName` and `CustomerContactLastName` columns at leaf level. This index, which can be created using the command in listing 5.10, is ordered by the `CustomerContactEmail` column, so no sort operation is required. Because the other required columns are included at the leaf level, there is not even any need to perform a lookup operation to the clustered index.

Listing 5.10 Creating a covering index to improve performance

```sql
CREATE NONCLUSTERED INDEX
    [NI-CustomerContactEmail-Include-FirstName-LastName]
        ON dbo.CustomerContacts(CustomerContactEmail)
        INCLUDE(CustomerContactFirstName, CustomerContactLastName) ;
```

Now that we have a covering index, let’s run the query in listing 5.9 again and see what that does to the execution plan, which is illustrated in figure 5.3, and also the execution time. You can see that, this time, the optimizer has chosen to perform a nonclustered index scan, and on my test rig, the query took 20,904 ms to execute, which is roughly the same as the original, unordered query.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F03_Carter.png)<br>
**Figure 5.3 Execution plan using a nonclustered index scan**

While creating the index has worked well for this specific query, we need to remember that nothing is ever free. Although the index has resolved the performance of the query that has ordered the data by email address, the existence of the index will decrease the performance of `INSERT`, `UPDATE`, and `DELETE` operations made against the table, as SQL Server will need to also update the nonclustered index.

Therefore, it is always better to avoid ordering data unless absolutely necessary, as performance will decrease. If you do have to order data, for presentation reasons, consider your index strategy, but bear in mind that this will have an impact on the performance of write operations to the table.

## 5.5 #18 Using DISTINCT without good reason

We have been asked to return a list of unique suppliers. I have seen less experienced developers use the `DISTINCT` keyword, just to make sure that the results are unique. To examine this, we could achieve the same results using either of the queries in listing 5.11. Because (unless we have a serious data quality issue) the same supplier will not be listed twice in our table, the use of the `DISTINCT` keyword is redundant.

Listing 5.11 Return suppliers with and without using `DISTINCT`

```sql
SELECT SupplierName
FROM dbo.Suppliers ;

SELECT DISTINCT SupplierName
FROM dbo.Suppliers ;
```

Even though we don’t need the `DISTINCT` keyword, does it make any difference if we use it? To answer that, let’s examine the execution plan in figure 5.4. If both queries had the same cost, then they would each have a query cost, relative to batch, of 50%. In this case, however, you can see that the query with the `DISTINCT` keyword has a relative cost of 82%, meaning it was far less efficient. You can see this reflected in the Distinct Sort operator.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F04_Carter.png)<br>
**Figure 5.4 Execution plans with and without `DISTINCT`**

There may be some occasions where we simply must uniquify our results. For example, imagine that we have been asked to return a unique list of the suppliers from which MagicChoc purchased large head sprockets in June 2023. We could return the list of suppliers using the query in the following listing.

Listing 5.12 Returning a list of suppliers of large head sprockets

```sql
SELECT DISTINCT s.SupplierName
FROM dbo.Suppliers s
INNER JOIN dbo.PurchaseOrderHeaders poh
    ON poh.SupplierID = s.SupplierID
INNER JOIN dbo.PurchaseOrderDetails pod
    ON pod.PurchaseOrderNumber = poh.PurchaseOrderNumber
WHERE MONTH(poh.PurchaseOrderDate) = 6
    AND YEAR(poh.PurchaseOrderDate) = 2023
AND pod.ProductID = 4 ;
```

The trouble is that, because we have purchased the item twice from Unknown Engineering within this time period, this query returns two results for the Supplier. We could, of course, use the `DISTINCT` keyword, but we know that it will degrade performance. Are there any other options?

The three queries in listing 5.13 are all functionally equivalent. The first query uses the `DISTINCT` keyword to uniquify the results, the second uses the `GROUP BY` clause, and the third uses the `ROW_NUMBER()` windowing function.

Listing 5.13 Using the `DISTINCT` key word

```sql
SELECT DISTINCT s.SupplierName
FROM dbo.Suppliers s
INNER JOIN dbo.PurchaseOrderHeaders poh
    ON poh.SupplierID = s.SupplierID
INNER JOIN dbo.PurchaseOrderDetails pod
    ON pod.PurchaseOrderNumber = poh.PurchaseOrderNumber
WHERE MONTH(poh.PurchaseOrderDate) = 6
    AND YEAR(poh.PurchaseOrderDate) = 2023
AND pod.ProductID = 4 ;

SELECT s.SupplierName
FROM dbo.Suppliers s
INNER JOIN dbo.PurchaseOrderHeaders poh
    ON poh.SupplierID = s.SupplierID
INNER JOIN dbo.PurchaseOrderDetails pod
    ON pod.PurchaseOrderNumber = poh.PurchaseOrderNumber
WHERE MONTH(poh.PurchaseOrderDate) = 6
    AND YEAR(poh.PurchaseOrderDate) = 2023
AND pod.ProductID = 4
GROUP BY s.SupplierName ;

SELECT SupplierName FROM (
    SELECT s.SupplierName, ROW_NUMBER() OVER(ORDER BY s.SupplierName) AS rn
    FROM dbo.Suppliers s
    INNER JOIN dbo.PurchaseOrderHeaders poh
        ON poh.SupplierID = s.SupplierID
    INNER JOIN dbo.PurchaseOrderDetails pod
        ON pod.PurchaseOrderNumber = poh.PurchaseOrderNumber
    WHERE MONTH(poh.PurchaseOrderDate) = 6
        AND YEAR(poh.PurchaseOrderDate) = 2023
    AND pod.ProductID = 4
) a WHERE rn = 1 ;
```

In our specific case, because the number of rows in question is tiny and because my test rig is not under load, there was no performance difference between the three, and the versions with `DISTINCT` and `GROUP BY` both generated the same execution plan.

In a production environment, however, with complex queries and large amounts of data, you may find that you have three entirely different plans and that some are significantly more efficient than others. Therefore, if you find that you have a performance issue using the `DISTINCT` keyword, it is worth trying the other two approaches to see if you can improve performance.

TIP I could make the `ROW_NUMBER()` query outperform the `DISTINCT` and `GROUP BY` queries in the preceding example simply by loading more data into the table. This example demonstrates, however, the importance of performance testing based on realistic data. We will explore this further in chapter 7.

We should never use `DISTINCT` for its own sake. If there is a genuine requirement to uniquify the results of a query, however, and if you experience performance issues when using `DISTINCT`, then you can explore other approaches to improve performance. If `DISTINCT` is really required and there are no performance issues observed, then I would recommend sticking with this approach, as it is the least opaque of the three options. It is immediately apparent what you are doing, and that helps make the code self-documenting.

> [!TIP]
>
> A genuine need to use `DISTINCT` may be because of an underlying issue with the database schema. Please see chapter 4 for more details.

## 5.6 #19 Using UNION unnecessarily

Just like unnecessarily ordering data and unnecessarily removing duplicates, developers who are new to SQL Server also make a similar mistake with the `UNION` clause. A union is a way of horizontally joining two sets of results, and there are two different ways to produce this union. Specifically, you can use a `UNION` clause or a `UNION ALL` clause.

The difference between `UNION` and `UNION ALL` is that `UNION ALL` will return all results from both queries. `UNION`, however, will remove duplicate results. For example, imagine that we have been asked to compile a list of MagicChoc’s contacts from both the `CustomerContacts` table and the `SupplierContacts` table. The first query in listing 5.14 uses `UNION` to create a distinct list of contacts. The second query uses `UNION ALL` to create a list that may include duplicates.

> [!TIP]
>
> There are additional horizontal join operators called `INTERSECT` and `EXCEPT`. `INTERSECT` will return the results from query 1, which also appear in the results from query 2. `EXCEPT` will return the results from query 1 that are not present in query 2.

Listing 5.14 Creating contacts lists with and without duplicates

```sql
SELECT SupplierContactFirstName, SupplierContactLastName
FROM dbo.SupplierContacts
UNION
SELECT CustomerContactFirstName, CustomerContactLastName
FROM dbo.CustomerContacts ;

SELECT SupplierContactFirstName, SupplierContactLastName
FROM dbo.SupplierContacts
UNION ALL
SELECT CustomerContactFirstName, CustomerContactLastName
FROM dbo.CustomerContacts ;
```

We can see from the execution plans in figure 5.5 that the cost of deduplicating the list was significantly higher, so it would be worth checking the requirement. Do we really need to deduplicate the list? If we don’t, then we should not deduplicate just for the sake of it.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F05_Carter.png)<br>
**Figure 5.5 Execution plans for horizontal joins**

There will be times where we need to use `UNION`, but we should only use it when there is a genuine requirement to do so. If duplicates are unimportant, or cannot occur because of business logic, then use `UNION ALL` instead.

## 5.7 #20 Using cursors

The head of procurement at MagicChoc would like us to produce a report that shows how many products we have in stock grouped by each product category. Rather than in a vertical format, however, we must produce the reports in a horizontal format, where the column names are the product categories, with a single row detailing the quantity of products in stock relating to each category.

The mistake that a lot of developers make at this point is to use a cursor. A cursor is a mechanism within T-SQL for looping over a set of rows, processing them one row at a time. Cursors can be used for many purposes, including pivoting data and generating and executing dynamic T-SQL scripts such as running a command against every table. They can also be used to find a value within any column of a table or to rank data.

The problem is that cursors are a dreadfully inefficient way to process relational data. Each iteration of a cursor has the same overhead as running the command in a stand-alone manner. For example, if you have a cursor that iterates through 1 million rows, it would have the same overhead as running 1 million statements against the table. Furthermore, the language enhancements to T-SQL over the past 25 years have nullified any requirement to use cursors. I cannot think of a single situation where a cursor is required to achieve the desired outcome, and you will most likely (at least hopefully) have a coding standard in place that disallows you from using cursors anyway.

> [!TIP]
>
> Even DBAs who formerly used cursors to iterate over a number of objects within a database have no reason to do so. We will discuss this more in chapter 9.

So, moving back to our scenario, if we went ahead and used a cursor to produce our pivoted report, we could use a script like the one in listing 5.15. The script first creates a temporary table with the final structure of our report. We then insert a holding row containing zero for each column. This will give us the base that we can update. When declaring our variable, we also declare a cursor to include the full result set that we will iterate. In our scenario, that is a list of product categories and quantities in a vertical tabular format. We then open our cursor and use a `FETCH` statement to pull in the first row. The `WHILE` loop then tells our cursor the actions we want to perform—in our case, updating the appropriate column in the temporary table based on the values inside the cursor. At the end of the `WHILE` loop, we pull the next row into the cursor. The `WHILE` loop exits when `@@FETCH_STATUS = 0`. This system variable lets us know when there are no rows left to fetch. Finally, we simply run a select statement from the temporary table before cleaning up the temporary objects so they don’t clutter the memory.

Listing 5.15 Using a cursor to pivot data

```sql
CREATE TABLE #Categories (
    [Raw Ingredients]               INT,
    [Machine Parts]                 INT,
    [Misc]                          INT,
    [Confectionary Products]        INT,
    [Non-confectionary Products]    INT
  ) ;

INSERT INTO #Categories
VALUES (0,0,0,0,0) ;

DECLARE @Category as varchar(32) ;
DECLARE @Stock as varchar(32) ;

DECLARE product_cursor CURSOR FOR
SELECT
      pc.ProductCategoryName
    , SUM(ISNULL(p.ProductStockLevel,0)) Stock
FROM dbo.ProductCategories pc
INNER JOIN dbo.ProductSubcategories ps
    ON pc.ProductCategoryID = ps.ProductCategoryID
LEFT JOIN dbo.Products p
    ON ps.ProductSubcategoryID = p.ProductSubcategoryID
GROUP BY ProductCategoryName ;

OPEN product_cursor  ;

FETCH NEXT FROM product_cursor INTO @Category, @Stock  ;

WHILE @@FETCH_STATUS = 0
BEGIN
    IF @Category = 'Raw Ingredients'
        UPDATE #Categories
        SET [Raw Ingredients] =
            [Raw Ingredients] + @Stock
    ELSE IF @Category = 'Machine Parts'
        UPDATE #Categories
        SET [Machine Parts] =
            [Machine Parts] + @Stock
    ELSE IF @Category = 'Misc'
        UPDATE #Categories
        SET [Misc] = [Misc] + @Stock
    ELSE IF @Category = 'Confectionary Products'
        UPDATE #Categories
        SET [Confectionary Products] =
            [Confectionary Products] + @Stock
    ELSE IF @Category = 'Non-confectionary Products'
        UPDATE #Categories
        SET [Non-confectionary Products] =
            [Non-confectionary Products] + @Stock ;

    FETCH NEXT FROM product_cursor INTO @Category, @Stock  ;
END

SELECT
      [Raw Ingredients]
    , [Machine Parts], [Misc]
    , [Confectionary Products]
    , [Non-confectionary Products]
FROM #Categories ;

CLOSE product_cursor ;

DEALLOCATE product_cursor ;

DROP TABLE #Categories ;
```

Instead of using a costly cursor, we could have used the `PIVOT` operator. This operator will do the hard work for us and do it as a set-based operation, which will be far more efficient than a cursor.

Real-world example

About 10 years ago, I was working with one of the world’s largest advertising companies, which worked with very large datasets that were harvested from search engines and cookie service providers. They asked me to look at a performance problem they had on an ETL job that pivoted a large dataset.

Their original implementation used a cursor. I rewrote the process to use a `PIVOT` statement instead of a cursor. Doing so reduced the execution time from over 3 hours to 48 seconds!

The single query in listing 5.16 achieves the same result as the cursor. The outer query specifies the columns that we want to return from the query. Here we can use `*`, or we can specify a column list. If we use a column list, then there is no obligation to select all the pivoted columns. The subquery pulls a flat list of categories and stock levels. Finally, the `PIVOT` operator defines the final result set by specifying the aggregation that we want to use against the stock column and the values within the `ProductCategoryName` column that we want as our pivoted columns.

Listing 5.16 Pivoting data using the `PIVOT` operator

```sql
SELECT
      [Raw Ingredients]
    , [Machine Parts]
    , [Misc]
    , [Confectionary Products]
    , [Non-confectionary Products]
FROM (
    SELECT
          pc.ProductCategoryName
        , ISNULL(p.ProductStockLevel,0) Stock
    FROM dbo.ProductCategories pc
    INNER JOIN dbo.ProductSubcategories ps
        ON pc.ProductCategoryID = ps.ProductCategoryID
    LEFT JOIN dbo.Products p
        ON ps.ProductSubcategoryID = p.ProductSubcategoryID
) AS WorkingTable
PIVOT
(
    SUM(Stock)
    FOR ProductCategoryName IN (
        [Raw Ingredients],
        [Machine Parts],
        [Misc],
        [Confectionary Products],
        [Non-confectionary Products]
    )
) AS PivotTable ;
```

To demonstrate the difference in query cost, we can copy the `PIVOT` query into the same query window as our cursor operation. If we then look at the execution plan for this statement, we can see that it is only 9% of the cost of the whole batch, meaning that the combined total cost of the statements we had to execute for the cursor-based approach was estimated as more than 10× less efficient than using the `PIVOT` operator. The relevant section of the execution plan is shown in figure 5.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F06_Carter.png)<br>
**Figure 5.6 Relative cost of a query using `PIVOT` against an equivalent cursor**

## 5.8 #21 Deleting many rows in a single transaction

It is not uncommon for SQL developers to be asked to remove old data from tables. This is often followed by a database shrink operation, which will reclaim space. We will be discussing database shrink operations again in chapters 9 and 11.

The most efficient way to remove a large amount of data from a table is to use a command called `TRUNCATE TABLE`. This command removes data from a table while leaving the table structure intact by deallocating data pages. The trouble here is that there is no `WHERE` clause with a truncate operation. It’s all or nothing, or in other words, you have to remove every single row.

Even if you do wish to remove every single row from a table, there are other limitations with table truncation. For example, you cannot perform the operation if a column in the table is referenced by a foreign key constraint or *edge constraints*, which enforce semantics and ensure the integrity of *edge tables*, which represent relationships within a graph database. Additionally, truncation cannot be used if the table is a base table within an index view, participates in either transactional or merge replication, or is a *system-versioned temporal table*, which is a table used to track the full data change history of another table to allow for point-in-time analysis.

These limitations often leave developers having to use `DELETE` statements to remove large quantities of rows from tables. The mistake I have seen here, many times, is that developers will try to delete all rows from a table in a single transaction. For example, consider a table that has many millions (or even billions) of rows.

The script in listing 5.17 creates a table and populates it with a very large number of rows. The number of rows it creates will vary depending on the tables and columns within your database, but on my test rig, it will generate just under 3.5 billion rows.

WARNING This script will take a long time to run.

Listing 5.17 Creating a very large table

```sql
CREATE TABLE dbo.VeryLargeTable (
    ID         BIGINT            IDENTITY    PRIMARY KEY,
    TextCol    NVARCHAR(4000)
) ;

DECLARE @LoopCounter INT = 0 ;

WHILE @LoopCounter < 2000
BEGIN
    INSERT INTO dbo.VeryLargeTable (TextCol)
    SELECT 'Yet another row in a very, very, very large table. In fact,
this table is going to take a very long time to create, and you will not be
able to delete all rows in one go!'
    FROM sys.columns c1
    CROSS APPLY sys.columns c2 ;

    SET @LoopCounter = @LoopCounter + 1 ;
END
```

Let’s try to delete all rows from this table in a single transaction using the query in listing 5.18. Remember that if we do not start an explicit transaction, then each statement will run inside an autocommit transaction. In other words, the lowest granularity of a transaction is a single statement.

Listing 5.18 Deleting rows in a single transaction

```sql
DELETE FROM dbo.VeryLargeTable ;
```

This statement will result in a very large transaction. While a transaction is open, the transaction log cannot truncate itself to free up space. This is true even in the `SIMPLE` recovery model (we will discuss recovery models in chapter 12). The transaction will become so large, in fact, that we will run out of space in the transaction log, and the query will roll back, causing a 9002 error to be thrown, as illustrated in figure 5.7.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH05_F07_Carter.png)<br>
**Figure 5.7 9002 error thrown when the transaction log becomes full**

Instead, we need to break the `DELETE` operation down into multiple statements and hence multiple transactions. Assuming the database is in the `SIMPLE` recovery model, this will prevent the transaction log from becoming full, as it will be able to truncate itself between statement executions. This is one of the very few scenarios where I would consider using a `WHILE` loop in a production environment, and that is only because this is an ad hoc script. I would always avoid putting a `WHILE` loop into code that I intend to deploy, for the same reasons I avoid the use of cursors. In listing 5.19, we have set the script to delete rows in batches of 250,000. You can tweak this number to optimize performance depending on the characteristics of your environment.

WARning This script will take a long time to run.

Listing 5.19 Deleting rows in batches

```sql
DECLARE @RowCounter BIGINT ;

SET @RowCounter = 1 ;

WHILE @RowCounter > 0
BEGIN
    DELETE TOP(250000)
    FROM dbo.VeryLargeTable ;

    SET @RowCounter = (SELECT COUNT(*) FROM dbo.VeryLargeTable) ;
    PRINT @RowCounter ;
END
```

Operations that modify data, such as `DELETE` operations, can cause the transaction log to become full when they are run against large tables. To avoid this issue, break the operation into multiple statements and iterate over them.

## Summary

* Remember that a `NULL` is an unknown value and is therefore not equal to another `NULL` value.
* Avoid using the `NOLOCK` query hint as a performance optimization as it can lead to unexpected results, which return data that never existed in the database.
* Avoid `SELECT *` in anything other than ad hoc queries, as it can cause issues with performance and code maintenance, and it’s an antipattern for self-documenting code.
* Ordering data is a presentation feature rather than a set-based feature. Only order data if it is absolutely necessary.
* Do not uniquify results unless you really need to. If `DISTINCT` causes performance issues, consider other techniques such as `GROUP BY` or `ROW_NUMBER().`
* `UNION` is more expensive than `UNION ALL` because it removes duplicates. Therefore, if duplicates are either unimportant or not possible, use `UNION ALL` instead of `UNION`.
* Cursors should be avoided. They are very expensive, and in modern versions of SQL Server, there are no operations that can’t be performed via other methods.
* Deleting many rows from a table in a single transaction can cause the transaction log to become full. Avoid this by splitting the deletion into multiple batches.
