# 10 Optimization

This chapter covers

* Instance-level optimizations
* Query optimizations
* Table optimizations
* Transaction isolation levels
* Throwing hardware at performance issues

In this chapter, we will dive into mistakes and misconceptions that relate to the optimization of SQL Server performance. We will start by looking at instance-level optimizations, where we will discuss the use of deprecated trace flags, misconceptions around instant file initialization, and memory configuration mistakes. Mistakes can come in two broad flavors: failure to optimize and incorrect optimization.

Failure to optimize

I witnessed a failure to optimize recently. I was asked to look at an extract, transform, and load process that was taking a significant time to load data into a data warehouse. After examination of the process, it was clear that the team had not undertaken capacity planning (see chapter 9). While the disk had space remaining, the data files had not been sized to account for the large amount of data that was pumped into the database each night. That was causing the data file to expand in small increments, multiple times per night. Instant file initialization had not been enabled, which meant that the additional file space was being zeroed out every time, which is discussed in the second mistake in this chapter. Enabling instant file initialization immediately resolved the issue, but I also sent the team away to plan their capacity and resize the disk accordingly.

We will then move on to mistakes that are made when optimizing queries. The first of these mistakes is working against the optimizer when we should be working with it. We will then discuss query-processing feedback, which is often overlooked by database administrators (DBAs).

We will move on to look at table optimizations. Here we will discuss the common mistake of overlooking useful performance optimization techniques, including partitioning tables and compressing tables.

Next we will look at transaction isolation levels, where we will explore the mistakes and misconceptions that I see time and time again. Finally, we will discuss what happens when we decide to throw more hardware at a performance issue.

To discuss the mistakes in this chapter, we will use the `Marketing` database that we created in chapter 6 and the `MarketingArchive` database that we created in chapter 9. If you have followed along with the examples in chapter 8 and have not yet repaired the `MarketingArchive` database, please run the script in listing 10.1 before continuing.

WARNING In normal operations, we should not use `REPAIR_ALLOW_DATA_LOSS` unless we have no other recovery options, such as restore from backup, etc., as this option can result in data loss.

Listing 10.1 Fixing the `MarketingArchive` database

```sql
USE master ;
GO

ALTER DATABASE MarketingArchive SET SINGLE_USER ;
GO

DBCC CHECKDB (MarketingArchive, REPAIR_ALLOW_DATA_LOSS) ;
GO

ALTER DATABASE MarketingArchive SET MULTI_USER ;
GO
```

For the examples that make changes to an instance, any instance will suffice, but the instance on which you are hosting the `MarketingArchive` database would be a good choice.

## 10.1 #54 Turning on TF1117 and TF1118

Trace flags are toggle options that allow administrators to change specific configurations. Each flag has a three- or four-digit number, and while many are either undocumented or relate to older versions of SQL Server, there are certainly some useful trace flags. For example, trace flag 3226 can be used to suppress successful backup messages. This reduces the amount of “noise” in the SQL Server logs.

Depending on the nature of the trace flag, it will be possible to apply the configuration to either the local session or globally, meaning all sessions. For example, the aforementioned 3226 flag can only be turned on globally.

Trace flags can be toggled on and off using the `DBCC TRACEON` and `DBCC TRACEOFF` commands. The trace flag number is passed to the command for session configuration. For global configuration, a second parameter of `-1` is passed. For example, the following script demonstrates how to turn on trace flag 1224 within the session and then globally. This flag disables lock escalation based on the number of locks:

```sql
DBCC TRACEON (1224) ;
DBCC TRACEON (1224,-1) ;
```

The challenge with global trace flags is that they do not persist a restart of the Database Engine service. Therefore, if we would like a configuration to persist a restart, we must add it as a startup parameter on the SQL Server service.

Some time ago, it was common to use two of these trace flags to help with performance issues in data warehouse–style applications and in some circumstances `TempDB` on OLTP-style workloads. Specifically, T1117 (a common abbreviation for trace flag 1117), which was used to grow all files within a filegroup whenever any file reaches its autogrow threshold. SQL Server uses a proportional fill algorithm for splitting data between files in a filegroup. Therefore, growing all of the files at the same time prevents SQL Server from “favoring” the larger file and subsequently not being able to get the full benefit from reading data from files in parallel.

T1118 was used to enforce uniform extents. This means that multiple objects cannot allocate pages to the same extent. By default, mixed extents were possible for small objects, which were less than 64 KB in size. Enforcing uniform extents helps to prevent contention in system pages such as the Global Allocation Map (GAM), Shared Global Allocation Map , and Page Free Space system pages, which reside in every data file. Uniform versus mixed extents are illustrated in figure 10.1.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F01_Carter.png)<br>
**Figure 10.1 Uniform vs. mixed extents**

Both T1117 and T1118 were deprecated in SQL Server 2016, and their functionality was moved to lower levels, meaning that they can be configured more granularly. This is a much better implementation, because the configurations can be applied to the specific databases that would benefit from it, as opposed to the trace flags, which applied the configuration to all databases.

I still see people setting T1117 and T1118 on the Database Engine service, and even the Azure SQL Server image has these flags configured. This is a mistake, however, because since SQL Server 2016, these trace flags have had no effect. Administrators who are unaware of this change think that they have configured these settings when in fact they haven’t.

Fortunately, equal file growth is now the default behavior for `TempDB` but should still be configured for data warehouses. Uniform extents are now the default behavior for `TempDB` and user databases. But that just flips the issue around. A DBA who is unaware of these changes but wishes to use mixed extents and/or equal file growth will do nothing, unaware that the undesired behavior is default.

So what should we do instead of configuring these two trace flags if we would like to change the behavior for specific databases? The script in listing 10.2 turns on equal file growth and turns off uniform extents for the `MarketingArchive` database. The first command in the script uses `ALTER DATABASE..MODIFY FILEGROUP` to turn on `AUTOGROW_ALL_FILES,` which overrides the default of `AUTOGROW_SINGLE_FILE`. In the second statement, we use an `ALTER DATABASE SET` option to enable mixed extents.

Listing 10.2 Turning off equal file growth and uniform extents

```sql
ALTER DATABASE MarketingArchive
    MODIFY FILEGROUP [PRIMARY] AUTOGROW_ALL_FILES ;

ALTER DATABASE MarketingArchive
    SET MIXED_PAGE_ALLOCATION ON ;
```

> [!TIP]
>
> These commands could also be performed through Desired State Configuration (see chapter 8).

## 10.2 #55 Not using instant file initialization

When SQL Server creates a file or grows a file, then it has to zero out the file, which literally means filling any blank space with `0`s. As a performance optimization, it is possible to skip this process for data files, and sometimes for transaction log files, by using instant file initialization. To do this, we simply have to give the service account that runs the Database Engine service the Perform Volume Maintenance Tasks user rights assignment (`SeManageVolumePrivilege`). SQL Server will then automatically skip the zero-out on file creation or growth. Microsoft has made it even easier for us in recent versions of SQL Server by adding this as an option during installation. It has also added it as an option that can be configured on the Advanced tab of the Database Engine service in SQL Server Configuration Manager, as shown in figure 10.2.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F02_Carter.png)<br>
**Figure 10.2 Configuring instant file initialization**

So it seems like a no-brainer to enable this, right? Well, not quite, and the mistake I have seen some DBAs making is to choose not to implement it. To understand why some DBAs make this mistake, we need to understand the security implications.

Imagine that we have a database containing very sensitive information. The database is migrated to a new server and is dropped from the original server. The original server is then used to host a new database.

When data is deleted from a disk, the physical data is not deleted—only the pointers to that data. When the new data files are created in the same physical location on disk, the files are zeroed out, and hence the original data is overwritten.

In a scenario where we have used instant file initialization, however, the data is not overwritten until SQL Server allocates extents within the same physical location and writes pages to it. Therefore, there is a theoretical risk that a bad actor could retrieve the original data.

If there is a potential security risk, why am I suggesting that it is a mistake not to use instant file initialization? The answer is simply because the risk is too small to outweigh the advantages of using the feature.

For the security risk to be realized, the following criteria would all have to be met:

* Sensitive data previously existed on the disks.
* An attacker has elevated permissions to the server.
* An attacker has the specialist software and skills required to find and retrieve the data.
* An attacker has time to find and retrieve the data before

  + An administrator notices the attack.
  + The data is overwritten by new data.

Therefore, unless you have been storing the nuclear armament codes and the truth of what really happened at Roswell (and decided not to shred the disks afterward), the risk is probably too small to worry about in the context of the performance gains.

## 10.3 #56 Failing to leave enough memory for other applications

I imagine that many readers will have performed a double take when reading the title of this section. Surely I should be writing about how hosting other applications on the same server as a SQL Server instance is a mistake, rather than talking about memory allocation for other applications?

Well, yes, having other applications installed on the same server as SQL Server is a horrible practice, for many reasons, including security, mixing workload profiles, and troubleshooting. Therefore, I would never advocate for it. What you may not realize, however, is that other features within the SQL Server stack, such as SQL Server Integration Services (SSIS) and SQL Server Analysis Services (SSAS), are also, essentially, other applications. They have their own services, run in their own processes, and have their own memory allocations. Therefore, if we are using any of these features, then we should consider their memory requirements separately from the Database Engine.

On top of this, we should also consider agents that may be used by other teams, such as an antivirus agent, a monitoring agent, or an inventory agent, to name but a few. While they are not business facing, they are very important tools for the wider business and are often mandated to be on all servers. Of course, these tools and agents should be lightweight, but I have seen some of them use far more memory than you would expect.

We also need to consider the amount of memory that we leave available to the operating system. Microsoft recommends leaving 25% of memory available for Windows. This 25% is also used by some SQL Server components, such as extended stored procedures and executables.

By default, when you install SQL Server, the instance will allocate as much memory as it needs for the *buffer pool*, the area of memory where data and index pages are cached. A common mistake is that DBAs leave this default configuration in place on the basis that SQL Server is the only application installed on the server; therefore, it should use as much memory as it wants. This, of course, does not consider the requirements discussed earlier. The mistake is even bigger if Lock Pages In Memory has been configured, as this will stop the working set from being trimmed, as discussed in the next section.

Therefore, we should configure the maximum amount of memory that can be used by the buffer pool, using the following equation:

Max Memory = (Server Memory – Memory Required by Other Processes) / 100) × 75

We can configure the maximum memory using the `sp_configure` stored procedure. The script in listing 10.3 configures the maximum server memory to be 48 GB, which would be appropriate for a solitary SQL Server instance, the only application running on a server that has 64 GB RAM. Of course, if the server was hosting multiple instances, then we should split the 48 GB appropriately between them, depending on their requirements.

> [!TIP]
>
> The value in listing 10.3 is specified in MB.

Listing 10.3 Configuring maximum memory

```sql
sp_configure 'show advanced options', 1 ;
RECONFIGURE ;
GO

sp_configure 'max server memory', 49152 ;
RECONFIGURE ;
GO
```

We should always configure the maximum memory setting to ensure that we leave enough memory for the operating system. Although we should always avoid other applications being installed on a server hosting a SQL Server instance, when configuring the maximum memory, we should consider the memory requirements of other features within the SQL Server stack, such as SSIS and SSAS. We should also consider the requirements of mandatory tools and agents that must run on the server.

## 10.4 #57 Failing to lock pages in memory

If Windows is running low on memory, it can become unstable, and out-of-memory errors can be thrown. In this case, Windows will try to protect itself by trimming the working set of user processes. In other words, it will cause processes to page data out to disk and reclaim the memory for itself.

Imagine that we have a resource-constrained SQL Server instance and users start complaining of severe performance degradation, including query timeouts. The DBAs investigate the issue and discover the following message in the SQL Server log:

```sql
A significant part of SQL Server process memory has been paged out. This
may result in a performance degradation. Duration: 0 seconds. Working set
(KB): 6081740, committed (KB): 17175674, memory utilization: 35%.
```

This error indicates that part of the buffer cache has been flushed to disk and the memory has been reclaimed by the operating system. So is locking pages in memory a no-brainer? Well, there has actually been a significant amount of debate in the community, which has led to many DBAs not configuring this setting. Why is it debated?

In earlier versions of Windows Server, the operating system was very aggressive when trimming working sets. As time moved on, this behavior became less aggressive while memory management improved in general. This meant that there was less chance of the issue occurring.

At the same time, virtualization platforms became the default environment for hosting SQL Server. One of the main benefits of a virtualization platform, such as VMware or Hyper-V, is the ability to overcommit physical resources. This means that, for example, a virtualization host with 8 CPUs and 64 GB RAM could host VMs with a total requirement of 16 vCPUs and 96 GB RAM.

The hypervisor manages this by allocating resources to VMs when they are required. When they are not being used, it reclaims those resources and assigns them to other VMs that need them more. This process is known as *ballooning* and is illustrated in figure 10.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F03_Carter.png)<br>
**Figure 10.3 Memory ballooning**

If VMs are hosting SQL Server instances that are locking pages in memory, then the memory cannot be reclaimed, and this can cause a performance issue across the whole virtualization estate.

The argument regarding VMs is perfectly valid, and when we run SQL Server in a virtual environment, we should work closely with our VMware administrators to ensure that we can offer a reliable service to our users without impacting the entire estate. The argument regarding the less aggressive trimming of the working set, however, is simply not strong enough to justify not configuring Lock Pages In Memory as a default installation option. The fact that it happens less often does not mean that it never happens, and I have seen occurrences where it has happened. We should try to avoid this, as these are perfectly preventable outages.

As cloud becomes the standard for hosting SQL Server workloads, it is also worth mentioning that major cloud providers, such as AWS, recommend configuring Lock Pages In Memory as a best practice in their environment. This is because, despite it being a virtual environment, the cloud providers do not overcommit resources. Therefore, ballooning is not a consideration.

Lock Pages In Memory is configured by granting the `SeLockMemoryPrivilege` User Rights Assignment to the service account that runs the Database Engine service. It can also be configured during SQL Server installation, or from the Advanced tab of the Database Engine service in SQL Server Configuration Manager.

## 10.5 #58 Working against the optimizer

T-SQL is a descriptive language. This means that when a developer writes a query, they describe the results that they want SQL Server to return, as opposed to giving SQL Server step-by-step instructions on how to perform the task at hand. The Query Optimizer then calculates the most efficient way to return the data that the developer is looking for.

The Query Optimizer is very sophisticated and makes a lot of good decisions, based on indexes, statistics, and a myriad of other factors. It is not infallible, however, and if it chooses a suboptimal plan, it can have a negative impact on query performance.

SQL Server developers and administrators can influence the query plan that is generated by using query hints. These hints will override decisions that the optimizer wants to make. They should only be used in exceptional circumstances but can be invaluable when resolving a performance issue.

WARNING Query hints should be used in exceptional circumstances only. More often than not, the optimizer gets it right.

The mistake that I see people make when working with query hints is being overprescriptive or, as I like to say, working against the optimizer when we should be working with it. The best example of this is a hint that can be used to specify the physical join operation that should happen between tables.

If we perform a join operation, such as an `INNER JOIN or OUTER JOIN` between two tables, then there are four physical operators that the Query Optimizer can use to perform the logical operation. Each is the most efficient choice in different scenarios. Each operator is summarized in table 10.1.

Table 10.1 Physical join operators

| Operator | Description | Most efficient use |
| --- | --- | --- |
| Nested Loops | Choose one table as the outer input table and the other table becomes the inner input table. For each key in the outer input table, it searches every row in the inner input table for matching key values. | Small tables |
| Hash Join | Builds a hash table in memory, with each row of the first table placed in a hash bucket, dependent on the key value. The keys of the second table are then hashed and compared to the first table on a row-by-row basis. If there is not enough memory for every row in the first table, then the operation will be performed in multiple steps. | Large tables not sorted by join key or where inputs have a big difference in row count |
| Merge Join | Checks the equality of the first key in both tables. It then compares the next row in the second table to the first row in the first table. This is similar to nested loops, but because the inputs are sorted by the join key, it can stop as soon as it finds a row in the second table that is not matched and move to the next row in the first table. | Large tables sorted by join key, where each table is a similar size |
| Adaptive Join | The query starts by using a hash join but can change to nested loops during execution if the table that populates the hash table is small enough for nested loops to be more efficient. | Queries where the input rows wildly swing and Batch Mode execution is used |

Imagine a scenario where the query in the following listing is performing poorly.

Listing 10.4 Poorly performing query

```sql
SELECT
    i.ImpressionUID
FROM Marketing.marketing.Impressions i
INNER JOIN MarketingArchive.dbo.ImpressionsArchive ia
    ON i.ImpressionUID = ia.ImpressionUID ;
```

We investigate the issue and on examining the query plan, we discover that the optimizer is choosing to use Nested Loops as the join operator, as shown in figure 10.4. On my test machine, the query takes 19 seconds to run with this operator.

> [!TIP]
>
> If you are following along with the examples in this chapter, then please note that the plan SQL Server chooses may differ from the plan it chose in my environment.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F04_Carter.png)<br>
**Figure 10.4 Plan using Nested Loops**

WARNING Before we go on, it is important to note that using query hints should be the last option. This type of issue can usually be resolved by other means, such as ensuring statistics are up to date. Query hints are a last resort.

The mistake that many DBAs would make in this scenario is forcing the optimizer to use a Hash Join, as that would likely be the most efficient option at this moment in time. There are various ways that this could be achieved, including plan freezing, Query Store hints, a `USE PLAN` query hint, a `HASH MATCH` query hint, or a join hint. For this example, we will use a join hint, as shown in the following listing.

Listing 10.5 Forcing a Hash Join with a join hint

```sql
SELECT
    i.ImpressionUID
FROM Marketing.marketing.Impressions i
INNER HASH JOIN MarketingArchive.dbo.ImpressionsArchive ia
    ON i.ImpressionUID = ia.ImpressionUID ;
```

This query used the plan illustrated in figure 10.5 and took 5 seconds to run on my test rig.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F05_Carter.png)<br>
**Figure 10.5 Plan using Hash Join because of a join hint**

That’s a great result! So why is this a mistake? Well, let’s consider that, after this incident, nonclustered indexes are added to the `ImpressionUID` column of the `Impressions` table in `Marketing` database tables, which can be achieved with the script in listing 10.6.

> [!TIP]
>
> There is no need to create this index for the `MarketingArchive` database because we created a suitable index in chapter 9.

Listing 10.6 Creating indexes on join columns

```sql
USE Marketing ;
GO

CREATE NONCLUSTERED INDEX ImpressionUID
    ON marketing.Impressions(ImpressionUID) ;
GO
```

Now the optimizer would ideally choose a Merge Join, which uses the new index, and on my test rig, takes 3 seconds to execute. Unfortunately, our hint is forcing the optimizer to use a Hash Join instead, which on my test rig takes 4 seconds with the new index. Therefore, a much better approach would have been to use a query hint where we can specify multiple options for the optimizer to choose from. Consider the query in the following listing.

Listing 10.7 Using a query hint instead of a join hint

```sql
SELECT
    i.ImpressionUID
FROM Marketing.marketing.Impressions i
INNER JOIN MarketingArchive.dbo.ImpressionsArchive ia
    ON i.ImpressionUID = ia.ImpressionUID OPTION (MERGE JOIN, HASH JOIN) ;
```

This hint still eliminates the possibility of using Nested Loops, which we know will never be a good idea because of the size of the tables, but it gives the optimizer the choice of using either a Merge Join or a Hash Join, depending on which it determines will be most efficient.

Although an option of last resort, query hints can be a useful tool for resolving performance issues. If you do decide to use query hints, always try to work with the optimizer, rather than against it. Where possible, give it multiple options to choose from rather than forcing a single option, which may become outdated.

## 10.6 #59 Not taking advantage of DOP feedback

In the previous section, we explored ensuring that we work with, rather than against, the optimizer when we think we have no choice but to use query hints. Wouldn’t life be much easier if SQL Server could learn what works best and adapt plans as required?

SQL Server 2022 expands the intelligent query-processing feature set to expand the possibilities of SQL Server adapting plans, based on previous inefficiencies, by bringing query feedback features into the main product. They had already been available in Azure SQL Database, in preview, and memory grant feedback has been slowly introduced to the core product over the last three releases.

These query feedback features utilize the Query Store to allow for comparison between different plans and the associated performance. Therefore, to follow the examples in this section, Query Store will need to be enabled for the `MarketingArchive` database. This can be achieved using the command in listing 10.8.

> [!TIP]
>
> Beginning in SQL Server 2022, the Query Store is enabled by default. It is also enabled by default in Azure SQL Database and Azure Managed Instances.

Listing 10.8 Enabling Query Store for the `MarketingArchive` database

```sql
ALTER DATABASE MarketingArchive
    SET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE) ;
```

> [!TIP]
>
> A full discussion of Query Store is beyond the scope of this book, but Microsoft has a walk-through at <https://mng.bz/YVmA>.

Imagine that we have periods where queries on our instance start to run slowly. We investigate by examining the wait statistics and discover that there are very high values for the `CX_PACKET` and `SOS_SCHEDULER_YEILD` wait types. We also notice that the issue is happening whenever a large reporting query is being executed. We check out the query plan and discover that the query is executing with a maximum degree of parallelism `(MAXDOP)` of 16.

> [!TIP]
>
> Prior to SQL Server 2016 Service Pack 2 and SQL Server 2017 Cumulative Update 4, `CX_PACKET` provided inconsistent results. Since this bug was fixed, it has become very useful.

All of the symptoms suggest that the issue is being caused by the big reporting query, running at a degree of parallelism that is actually detrimental to its own execution time and other queries that run simultaneously.

We have already changed the instance-wide `MAXDOP` setting to 16 to benefit some other queries, so we decide to add a `MAXDOP 8` query hint to the problematic query. This is a mistake, however. Why? Because we are running SQL Server 2022, which means that we could enable DOP feedback and let SQL Server continuously evaluate and change the DOP used in the query plan based on the execution statistics. Why do DBAs make this mistake? There are two reasons: (1) because DOP feedback is the only one of the three query feedback features not to be enabled by default and (2) because in the busy life of a DBA, it’s not always possible to stay up to date with all of the new features of SQL Server when they are released. This is a great example of why keeping up to date with and implementing new features is important, as it can save DBAs a lot of time and free them up to work on higher-value tasks.

SQL Server 2022 provides the following query feedback options, which are part of the intelligent query-processing suite.

* DOP feedback
* Cardinality estimation feedback
* Memory grant feedback

We have already discussed DOP feedback. It allows SQL Server to optimize the DOP of a query based on historical executions.

*Cardinality estimate (CE)* helps SQL Server to select the most appropriate plan based on estimating how many rows will be processed in each step of query execution. The estimates, which are based on column and index statistics and passed through a sophisticated algorithm, can sometimes be incorrect. This is because it is not possible to develop an algorithm to account for every single user’s requirements and usage patterns. Poor estimates can lead to suboptimal query plans and ultimately to poor query performance. CE feedback can help SQL Server identify poor estimates and make better choices.

*Memory grant feedback* adapts a query’s memory grant based on historical information. This can help avoid a memory grant that is too small, resulting in data spooling to disk. It can also help avoid a memory grant that is too high and results in suboptimal parallelization.

Unlike DOP feedback and CE feedback, which are both new in SQL Server 2022, memory grant feedback has been implemented in stages over the last three major releases of SQL Server. Batch-mode memory grant feedback was implemented in SQL Server 2017, with row-mode memory grant feedback implemented in SQL Server 2019. SQL Server 2022 saw the implementation of percentile and persistence modes. This brings the implementation into the Query Store, so that the statistics used to make the decisions are not lost when the instance restarts.

If Query Store is enabled for a database, memory grant feedback and CE feedback will be enabled by default, providing that the compatibility level of the database is 140 or higher for memory grant feedback and 160 or higher for CE feedback. DOP feedback is off by default, however. It can be turned on by using the command in the following listing.

Listing 10.9 Turning on DOP feedback

```sql
ALTER DATABASE SCOPED CONFIGURATION
    SET DOP_FEEDBACK = ON ;
```

The query in the following listing can be used to determine which query feedback features are enabled for the database in scope.

Listing 10.10 Reviewing enabled query feedback options

```sql
SELECT
      name
    , CASE
          WHEN value = 1 THEN 'Enabled'
          ELSE 'Disabled'
      END as Enabled
FROM sys.database_scoped_configurations
WHERE name LIKE '%feedback%' ;
```

It is good practice to enable DOP feedback in databases with a compatibility level of 160 or higher. This removes the need for the DBA to manually assess and optimize `MAXDOP` settings for a query.

## 10.7 #60 Not partitioning large tables

SQL Server is often I/O bound. This means that the disk subsystem is the bottleneck that impacts the performance of a query. This is especially true for very large databases that run reporting-style queries against very large tables.

Let’s consider the `MarketingArchive` database. Imagine that our instance is struggling for IOPS and the query in listing 10.11 is frequently being run. The first command in the script turns on I/O statistics for the session.

Listing 10.11 Query against a large table

```sql
SET STATISTICS IO ON ;
GO

SELECT *
FROM dbo.ImpressionsArchive
WHERE EventTime >= '20210101' AND EventTime <= '20211231' ;
```

If we look at the query plan, shown in figure 10.6, we will notice that a clustered index scan is being performed. This means that every row in the table is being read.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F06_Carter.png)<br>
**Figure 10.6 Query plan from reporting query**

Let’s look at the I/O statistics for this query. They will be displayed in the Messages tab, due to I/O statistics being turned on for the session:

```sql
Table 'ImpressionsArchive'. Scan count 1, logical reads 46979,
physical reads 1, page server reads 0, read-ahead reads 46981,
page server read-ahead reads 0, lob logical reads 0, lob physical reads 0,
lob page server reads 0, lob read-ahead reads 0,
lob page server read-ahead reads 0.
```

The buffer cache was cold, so SQL Server performed a physical read of all required pages (1 physical read and 46,981 read-ahead reads). The read-ahead reads placed the remaining pages required into the buffer cache, which is why the statistics show 46,979 logical reads.

Clearing the cache

For testing purposes, it is sometimes helpful to clear the buffer cache so that pages need to be read from disk. If you fail to do this, the second time you run a query the pages will already be in cache, meaning that the test is not valid.

You can clear the pages from the buffer cache by running the following commands:

```sql
CHECKPOINT
DBCC DROPCLEANBUFFERS
```

The first command causes *dirty pages*, those that have been modified, to be flushed to disk. The second command removes *clean pages*, those that have not been modified, from the cache.

If you plan to run repeated tests, then it is good practice to run these commands at the start of your script. Remember, however, that you should only ever do this on a development or test server. Running these commands on a production server is likely to impact performance.

A mistake that I often see DBAs make is to investigate this far, check for missing indexes, and then stop and either say “Sorry, it is what it is!” or contact the storage team to see if they can coax any more performance out of the SAN. What they should be doing instead is investigating a greatly underutilized feature called *table partitioning*.

Table partitioning is a longstanding feature in SQL Server that splits a table into multiple smaller tables based on a partitioning key. This split is entirely seamless to the developer, as the table still appears as a single table and no code changes are required.

There are two big advantages to partitioning a large table that has reporting-style queries run against it. The first is that it is possible to place each partition on a different filegroup. If the files within those filegroups are stored on different disks, partitions can be read in parallel.

The second, in my opinion more important, advantage is the ability to eliminate partitions. In other words, SQL Server can read data just from the partitions it requires and ignore partitions that do not hold relevant data. This can result in a drastic reduction of reads, easing pressure on the I/O subsystem. It can also improve the performance of the query directly, by reading less data.

So why don’t DBAs take advantage of this functionality? There are three reasons that I have come across. The first is that they are simply unaware of the feature. I always find this surprising, as the feature was introduced in SQL Server 2005—almost 20 years ago! The second reason is political wrangling. I have seen situations where the DBA team insists that it’s a developer’s responsibility to write the partitioning objects, as they don’t have the application knowledge. At the same time, the developers refuse to implement the feature because they are not DBAs and don’t understand partitioning. This is easily solved with collaboration. If both teams can pull together, then implementation will be straightforward.

The final reason is a lack of understanding of how the technology works or how to implement it. Let’s address this reason in the remainder of this section.

To create a partitioned table or partition an existing table, a partition function and partition scheme must be created. To understand these concepts, I like to use the analogy of a farm with multiple fields. The fields are separated by fences, and that allows different crops to be grown in each field.

Using this analogy, the partition function is the fences. It defines the boundary points: where one field ends and the next begins. When we create a partition function, we specify the datatype of the boundary points, the specific boundary point values, and if the range is left or right.

The range left or right is used to determine if values that land on the boundary point are placed to the left of the boundary point or the right of the boundary point. This difference is illustrated in figure 10.7. In this example, a partition function has created boundary points for the integer values 10, 20, and 30.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F07_Carter.png)<br>
**Figure 10.7 Partition function ranges left and right**

The partition scheme defines which filegroup each partition is placed on. We have a choice of specifying a different (precreated) filegroup for each partition or specifying the `ALL` keyword, to denote that we wish to store all partitions on the same filegroup.

Finally, when we create a table, we replace the `ON <FILEGROUP>` clause with an `ON <PARTITION SCHEME>(<PARTITIONING KEY>)` clause. This architecture means that multiple partitions can be created on a single partition scheme and multiple partition schemes can be created on a single partition function.

The script in listing 10.12 creates a partition function called `ImpressionDatesPF`, which uses range right and a `DATETIME` data type. It creates boundary points at the first of January in 2020, 2021, 2022, and 2023. It then creates a partition scheme called `ImpressionDatesPS`, which maps all partitions to the `PRIMARY` filegroup. Next, the script creates a new table, called `ImpressionArchivePartitioned`, on the `ImpressionDatesPS` partition scheme using the `EventTime` column as the primary key, before creating a `PRIMARY KEY` on the `ImpressionID` and `EventTime` columns. It is important to understand that, when dealing with partitioned tables, the partitioning key must be a subset of any unique key of the table. We are creating a new table, rather than partitioning the existing table, for two reasons. First, it allows for comparisons to be run against the two approaches and second, we will be using the `ImpressionArchive` table in later examples.

Listing 10.12 Creating a partitioned table

```sql
CREATE PARTITION FUNCTION ImpressionDatesPF (DATETIME)             ①
AS RANGE RIGHT FOR VALUES ('20200101', '20210101', '20220101', '20230101');
GO

CREATE PARTITION SCHEME ImpressionDatesPS                          ②
AS PARTITION ImpressionDatesPF
ALL TO ([PRIMARY]) ;
GO

CREATE TABLE dbo.ImpressionsArchivePartitioned(
    ImpressionID      BIGINT              NOT NULL IDENTITY(1,1),
    ImpressionUID     UNIQUEIDENTIFIER    NOT NULL,
    ReferralURL       VARCHAR(512)        NOT NULL,
    CookieID          UNIQUEIDENTIFIER    NOT NULL,
    CampaignID        BIGINT              NOT NULL,
    RenderingID       BIGINT              NOT NULL,
    CountryCode       TINYINT             NULL,
    StateID           TINYINT             NULL,
    BrowserVersion    BIGINT              NOT NULL,
    OperatingSystemID BIGINT              NOT NULL,
    BidPrice          MONEY               NOT NULL,
    CostPerMille      MONEY               NOT NULL,
    EventTime         DATETIME            NOT NULL,
) ON ImpressionDatesPS(EventTime) ;                                ③
GO

ALTER TABLE dbo.ImpressionsArchivePartitioned ADD CONSTRAINT       ④
   PK_ImpressionsArchivePartitioned PRIMARY KEY (ImpressionID, EventTime) ;
GO

INSERT INTO dbo.ImpressionsArchivePartitioned (                    ⑤
    ImpressionUID,
    ReferralURL,
    CookieID,
    CampaignID,
    RenderingID,
    CountryCode,
    StateID,
    BrowserVersion,
    OperatingSystemID,
    BidPrice,
    CostPerMille,
    EventTime
)
SELECT
      ImpressionUID
    , ReferralURL
    , CookieID
    , CampaignID
    , RenderingID
    , CountryCode
    , StateID
    , BrowserVersion
    , OperatingSystemID
    , BidPrice
    , CostPerMille
    , EventTime
FROM dbo.ImpressionsArchive ;
```

① Creates partition function

② Creates partition scheme

③ Creates table on partition scheme

④ Creates primary key

⑤ Inserts data into new table

Now let’s see what that has done to the I/O requirements by running the query in listing 10.13.

Listing 10.13 Query against a partitioned table

```sql
SELECT *
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= '20210101' AND EventTime <= '20211231' ;
```

The I/O statistics for this query are

```sql
Table 'ImpressionsArchivePartitioned'. Scan count 1, logical reads 15732,
physical reads 2, page server reads 0, read-ahead reads 15736,
page server read-ahead reads 0, lob logical reads 0, lob physical reads 0,
lob page server reads 0, lob read-ahead reads 0,
lob page server read-ahead reads 0.
```

Despite the query returning exactly the same rows, instead of reading 46,982 pages from disk, it has only read 15,738 pages from disk. If we examine the query plan, shown in figure 10.8, the reason is apparent. Although a clustered index scan has still been performed, all but one of the partitions have been eliminated, meaning that only a third of the data needed to be read.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F08_Carter.png)<br>
**Figure 10.8 Partitioned table query plan**

If you have very large tables with a typical usage pattern for reporting queries, you should consider partitioning the table. This approach allows partitions to be eliminated if they are not required, which can significantly relieve pressure on the I/O subsystem and improve query performance.

## 10.8 #61 Not understanding the limitations of partition elimination

In the previous section, we discussed how helpful partitioning can be when we have very large tables. The benefits come from partition elimination; SQL Server can choose to read the data from only the partitions that it requires. Because each partition is stored as a separate B-tree, however, if our queries are not able to take advantage of partition elimination, then queries can actually be slower than they would be against a nonpartitioned table. Therefore, it is a mistake to implement partitioning without thinking about the queries that will be using it.

I cannot emphasize enough that the partitioning key is vital to achieving partition elimination. One of the most common reasons for partition elimination not working is developers writing queries that do not use the partitioning key. For example, if we filter on the partitioning key `AND` another column, then partition elimination will work. If we filter on the partitioning key `OR` another column, however, then all partitions would need to be read. This is illustrated in listing 10.14. The first query in the script will access only a single partition. The second query, however, uses `OR` logic and will need to read all partitions in the table.

Listing 10.14 `AND` logic versus `OR` logic

```sql
SELECT *
FROM dbo.ImpressionsArchivePartitioned
WHERE CampaignID = 44538
AND EventTime >= '20210101' AND EventTime <= '20211231' ;

SELECT *
FROM dbo.ImpressionsArchivePartitioned
WHERE CampaignID = 44538
OR EventTime >= '20210101' AND EventTime <= '20211231' ;
```

Another common cause of issues is cases in which SQL Server needs to perform a conversion. For example, our `ImpressionsArchivePartitioned` table uses the `DATETIME` data type for the `EventTime` column. In the examples so far, SQL Server has been able to compare our literal value against the column, even though it is formatted as a `DATE,` because `DATETIME` is above `DATE` in the order of precedence. If we explicitly typed our value as `DATE` however, then SQL Server would need to convert it, which would stop partition elimination from working. For example, the first query in the following listing would read all of the table’s partitions, whereas the second query would be able to eliminate all but one partition.

Listing 10.15 Converting data types

```sql
DECLARE @StartDate DATE ;
SET @StartDate = '20210101' ;

DECLARE @EndDate DATE ;
SET @Enddate = '20211231' ;

SELECT *
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= @StartDate AND EventTime <= @EndDate ;
GO

DECLARE @StartDate DATETIME ;
SET @StartDate = '20210101' ;

DECLARE @EndDate DATETIME ;
SET @Enddate = '20211231' ;

SELECT *
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= @StartDate AND EventTime <= @EndDate ;
```

The final common cause of problems is simple parameterization. If SQL Server thinks that it can reuse the plan, it will promote reuse by parameterizing the values in the filter. This will stop partition elimination from working.

We can work around this issue in two different ways. The first workaround is to specify the `RECOMPILE` option. This will prevent simple parameterization, because the plan will not even be cached. This has the obvious disadvantage that the plan will have to be recompiled every time the query is run.

The second workaround is to add a static nonequality operator to the query, such as `1 <> 2`. This will prevent simple parameterization, but the plan will still be cached so that some degree of reuse is possible.

Listing 10.16 has three queries. The first query will read all partitions of the table. The second and third queries will only read a single partition.

Listing 10.16 Simple parameterization

```sql
SELECT COUNT(*)
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= '20210101' AND EventTime <= '20211231' ;
GO

SELECT COUNT(*)
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= '20210101' AND EventTime <= '20211231'
OPTION(RECOMPILE) ;
GO

SELECT COUNT(*)
FROM dbo.ImpressionsArchivePartitioned
WHERE EventTime >= '20210101' AND EventTime <= '20211231'
AND 1<>2 ;
GO
```

Both of these workarounds will result in dynamic partition elimination. This means that SQL Server decides to eliminate partitions at execution time, as opposed to compile time. Figure 10.9 illustrates the seek predicates from the query plan of the third query in listing 10.16. You will notice that a single scalar operator of `3` is used to determine the partition that needs to be accessed.

Partitioning large tables can offer very worthwhile performance benefits. Partitioning rashly and not developing code to ensure partition elimination occurs, however, is a mistake. It is important that we understand the limitations of partition elimination and that performance may degrade if partition elimination does not occur.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F09_Carter.png)<br>
**Figure 10.9 Query plan seek predicates**

We can work around many issues with partition elimination by avoiding simple parameterization, avoiding SQL Server having to convert values, and ensuring that developers write code that filters or joins using the partitioning key. If the code is not compatible with partition elimination, however, then we should avoid using partitioning, as it may actually have a negative impact on performance.

## 10.9 #62 Not compressing large tables

In the previous section, we discussed using partitioning as a way of relieving pressure on the I/O subsystem to improve query performance. Another method of doing this is to consider compression. When most DBAs think of compression, they think of reducing the size of data at the expense of performance, but this is a misconception and leads to underutilization of compression within SQL Server, which can improve query performance for some workloads.

If SQL Server is I/O bound, with spare processor capacity, then row compression and page compression can reduce the number of pages that SQL Server needs to read from disk, alleviating pressure on the I/O subsystem at the expense of additional processor cycles.

To understand how compression works, it is helpful to understand how a normal, uncompressed data page structures data within a row. This is illustrated in figure 10.10. The left of the diagram shows the high-level page structure, where a slot is a physical row container. The right side of the diagram shows how data within a slot is structured.

Let’s think about the `ImpressionArchive` table. We already know from the previous section that to read all rows within this table, we would have to read 46,982 pages. But what about if we implemented row compression? Let’s use the command in listing 10.17 to implement row compression.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F10_Carter.png)<br>
**Figure 10.10 Structure of a data page**

Listing 10.17 Implementing row compression

```sql
ALTER TABLE ImpressionsArchive
    REBUILD WITH (DATA_COMPRESSION = ROW) ;
```

The compressed table now uses 33,687 pages to store the data—an approximate 28% reduction. We can identify the number of pages used by a table either by running a query with no `WHERE` clause and looking at the I/O statistics or by running the query in listing 10.18, which pulls the page count from metadata. Filtering by `index_id = 1` ensures that we are only counting pages from the clustered index.

Listing 10.18 Determining the number of pages in a table

```sql
SELECT
    in_row_used_page_count
FROM sys.dm_db_partition_stats
WHERE object_id = OBJECT_ID('ImpressionsArchive')
    AND index_id = 1 ;
```

Row compression works by implementing a `VARDECIMAL` system. This means that instead of `VARCHAR`, `NVARCHAR`, and `VARBINARY` being the only variable-length data types, decimal values also become variable length. For example, in a `BIGINT` column, the value `10` will only consume 1 byte of space, as it will be stored as a `TINYINT`, and the value `50,000` will only consume 4 bytes, as it will be stored as an `INT`. A `NULL` value would not consume any space at all. Row compression also removes padded white space from fixed-length-character columns and compresses Unicode values to 1 byte per character where possible.

With this in mind, let’s consider how data is stored on a data page that has row compression implemented. Compare this structure, shown in figure 10.11, with the structure of an uncompressed page.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F11_Carter.png)<br>
**Figure 10.11 Structure of a page with row compression**

What if we used page compression? The command in the following listing implements page compression for the `ImpressionArchive` table.

Listing 10.19 Implementing page compression

```sql
ALTER TABLE ImpressionsArchive
    REBUILD WITH (DATA_COMPRESSION = PAGE) ;
```

When you implement page compression, the row-compression techniques described previously are implemented. This is followed by the implementation of prefix compression and dictionary compression.

Prefix compression looks at the value stored in multiple rows of the same column within a page and tries to identify a common prefix for those rows. It selects the longest value that contains the full prefix as an anchor. All other values within that column are stored as the differential of the anchor.

After prefix compression has been implemented, dictionary compression is the final step in the page-compression process. This step looks across all values stored within a page and looks for duplicate values. What is really clever is that these values are assessed using their binary representation, which makes the process data type agnostic and improves compression rates. Matching values are stored in a dictionary at the top of the page, and the rows just store a pointer to the value’s location in the dictionary.

**Figure 10.12 illustrates the structure of a data page with page compression implemented. You will notice that the slot structure is the same as with row compression. You will also notice, however, the addition of a CI record. This is inserted immediately after the page header and acts as an anchor for prefix compression and a dictionary for dictionary compression. The change count is used to track how many times the pages has been updated, as this may impact the effectiveness of the CI record. SQL Server uses this to decide if a page needs to be rebuilt.**

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F12_Carter.png)<br>
**Figure 10.12 Page structure with page compression implemented**

What has page compression done to the size of our `ImpressionArchive` table? Nothing! The table still consumes 33,687 pages. Why would this be? Well, in our case, the table has been built using random values, such as Globally Unique Identifiers and randomly generated character strings, meaning that there is so little commonality that nothing could be compressed.

We can use the `sp_estimate_data_compression_savings` stored procedure to estimate the size of a table with different compression levels before implementing a change. In our case, that would have helped us decide to use row compression instead of page compression. This would have been a good choice, as page compression did not reduce the size of the table further than row compression even though it requires more processor cycles than row compression to decompress a page.

The following listing shows how we can use the `sp_estimate_data_compression_savings` stored procedure to estimate the size of the table if we were to remove all compression compared to the size of the table with the current page compression implemented.

Listing 10.20 Estimating table size with compression

```sql
EXEC sp_estimate_data_compression_savings
    @schema_name = 'dbo',
    @object_name = 'ImpressionsArchive',
    @index_id = 1,
    @partition_number = NULL,
    @data_compression = 'none' ;
```

When we have large tables on I/O-bound SQL Server instances, we should evaluate data compression as a performance optimization technique. For data compression to be viable, the instance must be I/O bound, and there must be spare processor capacity. We should evaluate the impact of both row and page compression before implementing compression. This is because each technique will have varying compression rates based on the nature of the data in the table. Page compression usually achieves a better compression rate, at the expense of additional pressure on the CPU for decompression.

## 10.10 #63 Using Read Uncommitted

In chapter 5 we discussed how using the `NOLOCK` query hint as a performance tweak is a mistake, as the lack of locks can lead to nondeterministic results being returned, but many SQL Server professionals also make mistakes when working with locking at the level of transactions.

The Read Uncommitted transaction isolation level works in a way similar to `NOLOCK` but at the level of a transaction. It does not take out any locks for read operations. This avoids lock contention, but it can lead to *dirty reads*, which refer to reads that return data that is never committed to the database.

This means that Read Uncommitted is only an appropriate option when it’s used against tables that are stored in read-only filegroups, meaning that data cannot be written to them. Unfortunately, it is not uncommon to see SQL Server professionals using this isolation level as a performance optimization when dealing with tables that accept updates.

To understand the dirty read phenomenon, let’s consider the following example. A data steward is busy resolving some data issues in the `MarketingArchive` database. At the same time, a user is running a report.

> [!TIP]
>
> To follow along with this example, you should have two query windows open. The first query window should be used to execute the queries in listings 10.21 and 10.23. The second query window should be used to execute the script in listing 10.22.

The query in the following listing simulates the data steward updating the `BidPrice` within the `ImpressionsArchive` table for the impression with an `ImpressionID` of `100`.

Listing 10.21 Simulating a data steward update

```sql
BEGIN TRANSACTION                                   ①

    UPDATE dbo.ImpressionsArchive
    SET BidPrice = 1.812
    WHERE ImpressionID = 100 ;
```

① Run in the first query window.

The query in the following listing simulates the database user running a report that returns the `SUM` of all `BidPrice` values associated with a campaign.

Listing 10.22 Simulating a reporting query

```sql
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED    ①

BEGIN TRANSACTION

    SELECT SUM(BidPrice)
    FROM dbo.ImpressionsArchive
    WHERE CampaignID = (
        SELECT
            CampaignID
        FROM dbo.ImpressionsArchive
        WHERE ImpressionID = 100
) ;

COMMIT
```

① Run in the second query window.

The command in the following listing rolls back the data steward’s transaction, as they have realized that they have made a mistake.

Listing 10.23 Rolling back the data steward’s transaction

```sql
ROLLBACK ;
```

Unfortunately, this sequence of events means that the query returned by the reporting user is “incorrect” in the sense that it was based on data that was never committed to the database and that the query will return a different result if it is run again.

Do not use the Read Uncommitted transaction isolation level as a performance optimization in normal circumstances. It is only appropriate to use this isolation level when reading data from tables that are stored in a read-only filegroup, or in the very rare scenario that data accuracy does not matter.

## 10.11 #64 Using unnecessarily strong isolation levels

The Read Committed transaction isolation level is the default level of isolation in SQL Server and protects against dirty reads. It does, however, leave users exposed to *nonrepeatable reads*, a phenomenon where a transaction reads the same row twice but receives a different value on each occasion. It also exposes users to *phantom reads*, a phenomenon where a transaction reads a set of rows twice and receives a different number of rows each time.

I have seen occasions where database professionals have decided that their data is so important that they use the Repeatable Read isolation level, which protects against both dirty reads and nonrepeatable reads, or the Serializable isolation level, which protects against all consistency issues, including phantom reads, as a default for all transactions.

Using Repeatable Read and Serializable transaction isolation levels is perfectly valid and necessary in some situations, such as when actuarial calculations are being performed, but setting these isolation levels is often overkill. In many situations with conventional requirements, configuring these stronger isolation levels is a mistake. In the vast majority of cases, leaving the default configuration of Read Committed is the correct course of action.

To help make informed choices about the correct level of isolation to use in specific use cases, let’s explore the consequences of a nonrepeatable read and a phantom read. To follow along with the next two examples, you will need to use two query windows. Each listing will call out which query window in which to run the query.

Let’s first explore a nonrepeatable read. Imagine that, once again, the data steward is updating rows to fix data errors in the `ImpressionsArchive` table. This time, our reporting user will be using the default Read Committed transaction isolation level. The reporting user’s transaction begins with the code in the following listing.

Listing 10.24 Starting the reporting user’s transaction

```sql
BEGIN TRANSACTION                               ①

SELECT SUM(BidPrice)
FROM dbo.ImpressionsArchive
WHERE OperatingSystemID = (
    SELECT OperatingSystemID
    FROM dbo.ImpressionsArchive
    WHERE ImpressionID = 100
) ;
```

① Run this script in the first query window.

The script in listing 10.25 simulates the data steward fixing the `BidPrice` for the impression with an `ImpressionID` of `100`. This time, the data steward commits the change right away.

Listing 10.25 Data steward fixes some data

```sql
BEGIN TRANSACTION                               ①

    UPDATE dbo.ImpressionsArchive
    SET BidPrice = 1.5
    WHERE ImpressionID = 100 ;

COMMIT
```

① Run this script in the second query window.

Finally, the script in the following listing illustrates the end of the reporting user’s transaction.

Listing 10.26 End of the reporting user’s transaction

```sql
SELECT SUM(BidPrice)                            ①
FROM dbo.ImpressionsArchive
WHERE CountryCode = (
    SELECT CountryCode
    FROM dbo.ImpressionsArchive
    WHERE ImpressionID = 100
) ;

COMMIT
```

① Run this script in the first query window.

In this sequence of events, the reporting user has read the value of the `BidPrice` for the impression with an `ImpressionID` of `100` twice in two different queries. The second time the `BidPrice` was read, it had a different value from the first time it was read. Neither value is “wrong”—they were both correct at the time they were read from the table. If the user tries to reconcile the values in the two queries, however, it is likely to be confusing and certainly inconsistent.

In many scenarios, this type of anomaly will not matter. If we are performing regulatory financial calculations, for example, then we may need to set the isolation level to Repeatable Read. If the anomaly is acceptable, however, then we should leave the default Read Committed isolation level.

Now let’s explore a phantom read. Our data steward is back, but this time, incorrect data is being deleted. The script in the following listing begins our reporting user’s transaction.

Listing 10.27 Beginning the reporting user’s transaction

```sql
BEGIN TRANSACTION                              ①

SELECT COUNT(*)
FROM dbo.ImpressionsArchive
WHERE OperatingSystemID = (
    SELECT OperatingSystemID
    FROM dbo.ImpressionsArchive
    WHERE ImpressionID = 100
) ;
```

① Run this script in the first query window.

Next, the script in the following listing simulates the data steward deleting rows from the table.

Listing 10.28 Data steward’s transaction

```sql
BEGIN TRANSACTION                              ①

    DELETE
    FROM dbo.ImpressionsArchive
    WHERE ImpressionID IN (
        SELECT TOP 3 ImpressionID
        FROM dbo.ImpressionsArchive
        WHERE OperatingSystemID = (
            SELECT OperatingSystemID
            FROM dbo.ImpressionsArchive
            WHERE ImpressionID = 100
        )
        AND ImpressionID <> 100
    ) ;

COMMIT
```

① Run this in the second query window.

Finally, the script in the following listing shows the end of the reporting user’s transaction.

Listing 10.29 End of the reporting user’s transaction

```sql
SELECT COUNT(DISTINCT ImpressionID)            ①
FROM dbo.ImpressionsArchive
WHERE OperatingSystemID = (
    SELECT OperatingSystemID
    FROM dbo.ImpressionsArchive
    WHERE ImpressionID = 100
) ;

COMMIT
```

① Run this script in the first query window.

In this scenario, when the reporting user ran the second query, it returned three fewer rows than the first query. This is known as a phantom read, but it would also be known as a phantom read if rows had been inserted into the table, meaning that additional rows were returned by the second query.

A phantom read is not considered a major issue in many situations. If we know that we have a scenario such as an actuarial calculation where a phantom read would be problematic, then we should consider using the Serializable transaction isolation level. Otherwise, we should use a weaker isolation level, ideally, Read Committed.

Stronger isolation levels prevent more data anomalies but can lead to performance issues caused by lock contention. They can even lead to *deadlocks*, which occur when two transactions are waiting for each other to complete and neither can proceed. A deadlock leads to the query that SQL Server deems least expensive being rolled back. Therefore, we should only use stronger isolation levels when we have a legitimate requirement to do so.

## 10.12 #65 Not considering optimistic isolation levels

SQL Server supports two types of transaction isolation levels: pessimistic and optimistic. Pessimistic isolation levels use locks to prevent data from being updated by other transactions. Optimistic isolation levels store old versions of rows in `TempDB`. This avoids data anomalies while preventing the performance issues associated with readers and writers blocking each other.

In the previous two sections, we discussed pessimistic transaction isolation levels. In this section, we will explore the two optimistic isolation levels supported by SQL Server. These isolation levels are called Read Committed Snapshot and Snapshot, equivalent to Read Committed and Serializable, respectively, and they provide protection against data anomalies.

These isolation levels work by storing old row versions in `TempDB` up to the oldest row version within the oldest open transaction, meaning that transactions can reference the correct version of a row without the need for the locking and blocking associated with pessimistic isolation levels.

Unfortunately, these useful isolation levels are overlooked by the majority of DBAs. I think the main reason for this is fear of getting it wrong. That’s understandable, because optimistic concurrency is not a magic bullet by any stretch of the imagination.

No magic bullets

While this section advocates for the consideration of optimistic concurrency, it is important to note that it is not a magic bullet. As with most things in SQL Server, there is a tradeoff.

Row versions are stored inside an area of `TempDB` known as the Version Store. For busy production database systems, this can mean that there is an awful lot of additional I/O associated with the optimistic isolation levels. Given that two sections of this chapter are dedicated to methods for reducing the amount of I/O, you can see that this could potentially pose a challenge. We also need to consider the amount of extra disk space that will be used.

When considering features like optimistic concurrency, we need to assess situations on a case-by-case basis. For example, if we have `TempDB` stored locally on non-volatile memory express (NVMe) drive with loads of free space, then switching to optimistic concurrency is likely to be a great idea. If, on the other hand, `TempDB` is stored on a creaking SAN and we are experiencing I/O bottlenecks, then we are likely to do more harm than good by implementing this feature.

The mistake in this section is the failure to consider optimistic concurrency, given a specific scenario. In the right situations, Read Committed Snapshot and Snapshot isolation can be very useful tools in our arsenal when fighting tricky performance problems.

If we are assessing a situation where there are performance issues caused by locking and blocking but strong isolation is a requirement, we should certainly assess the potential of implementing Read Committed Snapshot and/or Snapshot isolation. We do, however, need to be mindful of the I/O requirements and avoid implementing it in an I/O-bound environment.

We can turn on Read Committed Snapshot isolation by using the command in the following listing.

Listing 10.30 Enabling Read Committed Snapshot

```sql
ALTER DATABASE MarketingArchive
    SET READ_COMMITTED_SNAPSHOT ON WITH NO_WAIT ;
```

Once Read Committed Snapshot is enabled, the ability to use the pessimistic Read Committed isolation level is removed and Read Committed Snapshot becomes the default isolation level for all transactions.

The script in the following listing demonstrates how to enable Snapshot isolation for a database.

Listing 10.31 Enabling Snapshot isolation

```sql
ALTER DATABASE MarketingArchive
    SET ALLOW_SNAPSHOT_ISOLATION ON ;
```

Because Read Committed Snapshot is now the default isolation level, we can examine the rows in the Version Store by starting any transaction and then looking at the system metadata. The script in the following listing begins a transaction and updates a row.

Listing 10.32 Beginning a Read Committed Snapshot transaction

```sql
BEGIN TRANSACTION

    UPDATE dbo.ImpressionsArchive
    SET BidPrice = BidPrice + 0.1
    WHERE ImpressionID = 100 ;

    UPDATE dbo.ImpressionsArchive
    SET CostPerMille = CostPerMille + 0.1
    WHERE ImpressionID = 100 ;
```

We can now view the Version Store by executing the query in the following listing.

Listing 10.33 Examining the Version Store

```sql
    SELECT *
    FROM sys.dm_tran_version_store ;

COMMIT
```

You will notice that the original version of the row is stored as a binary value, with the byte length stored in an additional column.

Every situation is different, and if you are under IO pressure, then optimistic concurrency may not be the right solution. That said, not considering it if you have performance issues caused by pessimistic concurrency is a mistake. Read Committed Snapshot and Snapshot isolation levels can be useful tools for a DBA in the right situation.

## 10.13 #66 Throwing more hardware at the problem

This entire chapter has been dedicated to optimizing SQL Server performance, but a mistake that I see time and time again, often made by managed support partners, is failing to dig into a performance issue and instead trying to throw more hardware at it.

This approach has never been a good idea. The main problem with it is that, if you have a performance issue, scaling the hardware is unlikely to solve the problem; and even if it does solve it, the effects are likely to be temporary, and you will have to upgrade again.

Let’s examine an incredibly badly written query, shown in the following listing, which simply updates the `BidPrice` column in all rows of the `MarketingArchive` table but does so using a cursor.

NOTE This script may take a long time to run.

Listing 10.34 A very slow cursor

```sql
DECLARE @ImpressionID BIGINT

DECLARE Impressions CURSOR FAST_FORWARD FOR
SELECT ImpressionID
FROM dbo.ImpressionsArchive ;

OPEN Impressions ;

FETCH NEXT FROM Impressions INTO @ImpressionID
WHILE @@FETCH_STATUS = 0
BEGIN
    UPDATE dbo.ImpressionsArchive
    SET BidPrice = BidPrice + 0.1
    WHERE ImpressionID = @ImpressionID ;

    FETCH NEXT FROM Impressions INTO @ImpressionID ;
END

CLOSE Impressions ;
DEALLOCATE Impressions ;
```

On my test rig, this query took a whopping 10 minutes 15 seconds to run. Figure 10.13 shows the machine’s resource utilization while the query was running.

While this may be an exaggerated scenario, I frequently have conversations with junior DBAs who will argue that a performance issue like this was caused by processor utilization because there was a spike to 100%, even though it has settled at 8% usage. Instead of investigating the cause of the problem, they simply want to throw more processors at the situation.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH10_F13_Carter.png)<br>
**Figure 10.13 Resource utilization during query execution**

Now you probably realize that it doesn’t matter how much hardware you throw at this issue; the only way to fix it is to rewrite the query, as shown in the following listing, which takes 3 seconds to run on my test rig.

Listing 10.35 Updating `BidPrice` using relational technique

```sql
UPDATE dbo.MarketingArchive
SET BidPrice = BidPrice + 0.1 ;
```

Even though this diagnostic approach was never helpful in the age of the VM, it is now also a cost consideration in the age of cloud. Throwing more hardware at a performance issue means needlessly spending more money by increasing the size of the cloud VM or purchasing faster storage.

Of course, there will be times when a workload outgrows the available capacity of a server and an upgrade is required, but hopefully we will have noticed and planned for this during our capacity planning, which is discussed in chapter 9.

We should always try to resolve a performance issue rather than defaulting to a hardware upgrade. Upgrading hardware doesn’t necessarily resolve the issue or may only alleviate the symptoms for a short time. Instead, we should try to get to the root cause of the issue. We can use performance optimizations discussed in this chapter, ensure coding best practices discussed in chapter 5 are being adhered to, or use any of SQL Server’s vast array of features and diagnostic tools to address the issue.

## Summary

* Trace flags are used to toggle SQL Server functionality on and off.
* Trace flags T1117 and T1118 have been deprecated and no longer have any effect on a SQL Server instance. More granular alternatives such as `AUTOGROW_ALL_FILES` and `UNIFORM_PAGE_ALLOCATION` should be used instead.
* Instance file initialization should be used in the vast majority of situations. It should only be avoided in the most secure of environments.
* Lock Pages In Memory should be used in many situations. There are times when it should be disabled in private clouds, but public cloud providers recommend it.
* Always leave enough RAM for the operating system and any other applications that are running on the server. This is especially important if Lock Pages In Memory is used.
* Use the Max Server Memory setting to configure the maximum amount of RAM that can be allocated to the SQL Server buffer pool.
* Always try to work with the optimizer, rather than against it. In rare situations when query hints are required, try to leave the optimizer with multiple options.
* Remember to take advantage of DOP query feedback.
* DOP feedback is disabled by default, unlike other query feedback mechanisms.
* Consider partitioning large tables to improve performance and reduce load on the I/O subsystem.
* Partitioning tables allows for partition elimination, meaning that partitions are not read if they do not store relevant data.
* Consider using data compression as a performance enhancement for large tables.
* Data compression is best suited to workloads that are I/O bound and have spare processor capacity.
* The compression rate achieved will be dependent on the data stored within the table. Use the `sp_estimate_data_compression_savings` stored procedure to evaluate the impact of row compression and page compression before implementing either option.
* Avoid the Read Uncommitted transaction isolation level unless you are working with tables stored in read-only filegroups.
* The Read Uncommitted isolation level can result in dirty reads just as the use of the `NOLOCK` query hint can.
* Avoid using strong isolation levels such as Repeatable Read or Serializable unless they are absolutely required, as they can lead to lock contention and deadlocks.
* Consider using optimistic isolation levels in situations when you face performance issues caused by lock contention.
* The optimistic isolation levels are Read Committed Snapshot and Snapshot.
* Optimistic isolation levels are best suited to environments where `TempDB` is not I/O bound and there is plenty of free space on the `TempDB` volume.
* Avoid throwing extra hardware at performance issues. Try to diagnose the root cause of the issue and resolve it instead.
