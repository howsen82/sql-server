# 3 Data types

This chapter covers

* Why data types are important
* The consequences of using the wrong standard data type
* Reasons for using advanced data types
* The benefits of working with XML and JSON data

In this chapter, we will explore data types and why it is important to choose the correct data types for our table’s columns. We will start by exploring some simple data types that are commonly used but that many people use incorrectly. We will see the effect that this can have on cost and performance.

We will then explore some of SQL Server’s advanced data types, which are significantly underused. We will investigate the use cases that make them so useful and look at the effects of avoiding them. It is worth noting, however, that while this chapter explores `HIERARCHYID`, XML, and JSON, other specialized data types exist, such as `GEOGRAPHY` and `GEOMETRY` for geospatial data. I strongly encourage you to explore all of SQL Server’s advanced data types.

MagicChoc has decided that it needs a new application, which will support human resources. The script in listing 3.1 creates the `HumanResources` database and then creates the first table: `dbo.employees`. You will notice that this table has been created using the data type `NVARCHAR(MAX)` for every column. This doesn’t look right, does it? But why? All of the data that we want to store can be inserted into this expansive data type. So does it really matter? We will explore this example throughout the chapter.

Listing 3.1 Creating an employees table

```sql
CREATE DATABASE HumanResources ;
GO

USE HumanResources ;
GO

CREATE TABLE dbo.Employees (
    EmployeeID               NVARCHAR(MAX)    NOT NULL,
    FirstName                NVARCHAR(MAX)    NOT NULL,
    LastName                 NVARCHAR(MAX)    NOT NULL,
    DateOfBirth              NVARCHAR(MAX)    NOT NULL,
    EmployeeStartDate        NVARCHAR(MAX)    NOT NULL,
    ManagerID                NVARCHAR(MAX)    NULL,
    Salary                   NVARCHAR(MAX)    NOT NULL,
    Department               NVARCHAR(MAX)    NOT NULL,
    DepartmentCode           NVARCHAR(MAX)    NOT NULL,
    Role                     NVARCHAR(MAX)    NOT NULL,
    WeeklyContractedHours    NVARCHAR(MAX)    NOT NULL,
    StaffOrContract          NVARCHAR(MAX)    NOT NULL,    ①
    ContractEndDate          NVARCHAR(MAX)    NULL
) ;
```

① Designed to store 0 for staff or 1 for contractor

Why is selecting the correct data type so important? Most people who have worked with SQL Server for a while understand the importance of constraints. These constraints come in many forms, such as foreign keys, check constraints, and `NULL` constraints.

Constraints are vital for ensuring data quality within a database. For example, a foreign key will ensure that a value exists in a different table before it is inserted or updated. A `NOT NULL` constraint enforces that a value in the column is mandatory and that the column cannot contain `NULL` values. A check constraint ensures that values in a column meet certain criteria, for example, ensuring that an employee’s start date is not earlier than their 16th birthday.

What a lot of people fail to consider, however, is that a data type is also a constraint—a constraint that applies to every single column in every single table within every single database. It is the foundation of data quality and functionality. Additionally, it can play a part in ensuring our code is self-documenting. Self-documenting code is discussed in more detail in chapter 2.

Using the example of our `Employees` table, there are several immediate problems. First, we cannot create a primary key constraint, as primary keys are not supported on `NVARCHAR(MAX)` columns. Second, calculations of the date columns will be cumbersome and require conversion. Dates could also be inserted in conflicting formats, for example, `13/01/2023` and `01/13/2023`. Third, we can insert any data we like into any column. We could insert a date into the `Salary` column, text into the columns expecting numeric values, or dates into the textual descriptors such as `FirstName`, `LastName`, and `Department`. Finally, as we can store up to 2 GB of data in every column of every row, this table could, in theory, get very large very quickly.

We should fix these problems straight away, and the script in listing 3.2 can address them by dropping and recreating the table. It will change the numeric columns to `INT`, the date columns to `DATE`, and reduce the textual descriptor columns to an appropriate length. The table will also add a primary key to the `EmployeeID` column.

Listing 3.2 Updating the employee table column types

```sql
DROP TABLE dbo.Employees ;
GO

CREATE TABLE dbo.Employees (
    EmployeeID             INT          NOT NULL  PRIMARY KEY,
    FirstName              NVARCHAR(32) NOT NULL,
    LastName               NVARCHAR(32) NOT NULL,
    DateOfBirth            DATE         NOT NULL,
    EmployeeStartDate      DATE         NOT NULL,
    ManagerID              INT          NULL,
    Salary                 MONEY        NOT NULL,
    Department             NVARCHAR(64) NOT NULL,
    DepartmentCode         NVARCHAR(4)  NOT NULL,
    Role                   NVARCHAR(64) NOT NULL,
    WeeklyContractedHours  INT          NOT NULL,
    StaffOrContract        INT          NOT NULL,
    ContractEndDate        DATE         NULL
) ;
```

NOTE While the newly selected data types are far more useful than blanket `NVARCHAR(MAX)`, they are also still far from perfect. We will explore this problem in the following sections of this chapter.

## 3.1 #6 Always storing whole numbers in INT

Imagine that we have a large data warehouse. One of the fact tables has 1 billion rows, and it joins to five dimensions, which all have 30,000 rows. Performance is poor and memory is always maxed out because of the volume of data in the buffer cache. When queries are running, a lot of data is being spooled to `TempDB`. We have optimized the queries. We have also reviewed our indexing strategy and ensured that both indexes and statistics are well maintained. It looks like the only thing to do is to throw more hardware at the problem, but we suspect, based on our trend over the last two years, that if we add more RAM, we are only pushing the problem further down the line. What should we do? A starting point would be to consider reviewing our numeric data types, especially those used in primary/foreign key relationships.

`INT` is the most used but also the most misused data type in SQL Server. We actually have four data types that are designed to store whole numbers specifically. These data types are detailed in table 3.1.

Table 3.1 Integer data type family

| Data type | Range | Size (Bytes) |
| --- | --- | --- |
| `TINYINT` | 0 to 255 | 1 |
| `SMALLINT` | –32,768 to 32,767 | 2 |
| `INT` | –2,147,483,648 to 2,147,483,647 | 4 |
| `BIGINT` | –9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 | 8 |

We can also examine this in code by running the query in listing 3.3. This query uses the `CAST()` function to convert the value `1` into each of the integer data types. This converted value is then passed into the `DATALENGTH()` function, which calculates the size of the input value, in bytes.

Listing 3.3 Examining the size of data types

```sql
SELECT
      DATALENGTH(CAST(1 AS TINYINT)) AS TinyIntSize
    , DATALENGTH(CAST(1 AS SMALLINT)) AS SmallIntSize
    , DATALENGTH(CAST(1 AS INT)) AS IntSize
    , DATALENGTH(CAST(1 AS BIGINT)) AS BigIntSize ;
```

The results of this query are

```sql
TinyIntSize    SmallIntSize    IntSize    BigIntSize
1              2               4          8
```

Imagine that our five dimension tables use `INT` for their primary key column. There are 30,000 rows in each of the dimension tables, and there are more than 32,000 possible positive values in the `SMALLINT` data type. That means if we were not expecting the dimensions to grow dramatically, then we could save 2 bytes in each row.

TIP There are actually more than 64,000 possible values in the `SMALLINT` range, but to use more than 32,000 of them, we would need to start our numbering sequence at –32,000. This may not play nicely with the principle of least surprise.

At this point, you may be thinking, “Why are we bothered about saving 2 bytes?” The answer to that requires some simple math. There are 30,000 rows in each of the five dimension tables. By moving to a `SMALLINT,` we would only be saving 58 KB per table. But our fact table has 1 billion rows. That means we would be saving 1.86 GB per key. Multiply that by the five dimension tables and that means for each query against the fact table that touches all rows and all five keys, we would be saving 9.3 GB. Scale that out against the eight fact tables in the data warehouse. We should also factor in the size of indexes, which are built on those foreign key columns. Now consider parallel sessions running different queries. Suddenly, our choice of data type is having a direct and tangible effect on memory consumption.

How does all of this relate to our `Employees` table? Let’s document our table in the form of an *Entity Relationship Diagram (ERD)*. An ERD is a diagram that lays out data entities (tables). It shows the relationships between these entities (primary/foreign key constraint), and it details the attributes (columns) of an entity. Optionally, it can also detail the data type of each column.

When building C4 diagrams for data-tier applications, an ERD is often used as the code diagram, which documents the table structure. The ERD in figure 3.1 currently shows just our `employees` entity, but we will build on this in chapter 4. The diagram records the currently defined data types but has been annotated to call out the values that we expect in each column.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH03_F01_Carter.png)<br>
**Figure 3.1 `Employees` ERD**

The `EmployeeID` and `ManagerID` columns will require up to 300 unique values. Therefore, the `TINYINT` data type will be too restrictive. We could, however, use the `SMALLINT` data type. This will save us 2 bytes per row over the `INT` data type that we currently have.

The `StaffOrContract` column is an interesting case. It will store integer values but only in the range of 0 to 1. This brings into play an additional data type that we have not yet discussed: `BIT`. The `BIT` data type is technically an integer data type, but it can only store the values `0`, `1,` and `NULL`. It is designed to store Boolean values, such as flags, and has useful extra features.

If a user inserts `TRUE` or `FALSE` into a `BIT` column, SQL Server will automatically convert them to `1` and `0`, respectively. Additionally, if a user inserts any numeric value other than 0 or 1, then SQL Server will automatically convert the value to 1. For example, if we ran the query `SELECT CAST(86.2 AS BIT),` then it would return the value `1`.

Additionally, if a table has multiple `BIT` columns, then SQL Server optimizes its storage. The first eight `BIT` columns only use 1 byte of space. The subsequent eight columns use an additional byte and so on. The `StaffOrContract` column is an ideal use case for the `BIT` data type and will save us 3 bytes per row.

As we have no data in the `employees` table as yet, let’s drop and recreate it using our preferred integer data types. We can do this with the script in the following listing.

Listing 3.4 Changing integer column types

```sql
DROP TABLE dbo.Employees
GO

CREATE TABLE dbo.Employees (
    EmployeeID             SMALLINT      NOT NULL  PRIMARY KEY,
    FirstName              NVARCHAR(32)  NOT NULL,
    LastName               NVARCHAR(32)  NOT NULL,
    DateOfBirth            DATE          NOT NULL,
    EmployeeStartDate      DATE          NOT NULL,
    ManagerID              SMALLINT      NULL,
    Salary                 MONEY         NOT NULL,
    Department             NVARCHAR(64)  NOT NULL,
    DepartmentCode         NVARCHAR(4)   NOT NULL,
    Role                   NVARCHAR(64)  NOT NULL,
    WeeklyContractedHours  INT           NOT NULL,
    StaffOrContract        BIT           NOT NULL,
    ContractEndDate        DATE          NULL
) ;
```

We should always be mindful of the amount of storage space required by our data types. This is especially true if the table is very large or if the column will be used in an index. `SMALLINT` and `TINYINT` should be used instead of `INT` where we do not expect values to overflow these sizes, to reduce wasted space.

## 3.2 #7 Always using variable-length strings

Imagine that we have a table that stores US addresses. We are correctly trying to make sure that our data is as space efficient as possible. Therefore, we use variable-length strings for all of the columns, which include each line of the address and the zip code. We know that Mooselookmeguntic in Maine and Kleinfeltersville in Pennsylvania have the longest city names in the United States, each with 17 characters, so we set the `CityName` column to be a `VARCHAR(17)`. We know that the longest state name in the US is Rhode Island and Providence Plantations, so we set the state column to be a `VARCHAR(48)`. We know that the zip code will be exactly 10 characters, but because we know it will have the same length every time, should we use a `VARCHAR(10)` or a `CHAR(10)`? Does it even matter? Either way, the data is 10 characters long, so will take up 10 bytes of space, right? And if that is correct, why do we even have fixed-length strings at all? The fact is that this assumption is not correct. To understand why, we need to understand a little about how SQL Server stores data.

NOTE Fixed-length strings `CHAR` and `NCHAR` pad out the string with white space, if it has not been filled with data. For example, if we have a `CHAR(8)` and insert the value `Hello!` then it will use 8 bytes of storage because the two spare characters will be filled with white space.

SQL Server stores data in a series of 8 KB pages, with each series of eight pages making up a 64 KB extent, which is usually the smallest amount of data that is read. Each data page has a 96-byte page header, which stores information that applies to the whole page, such as its unique ID and the object ID of the table (or index) that it belongs to.

The data that form rows are then stored in slots on the page. These slots don’t just store the data, however. They also have to store a small amount of *metadata*, to allow the data to be useful. This metadata includes information about the type of record stored in that slot. For example, is it a data record or an index record? Does it include ghost data (data that has been logically deleted but not yet physically removed)?

Other metadata includes the length of fixed-length data (this does not just include fixed-length character data but also data such as integers), a NULL value bitmap that tracks if variable-length columns contain NULL values, and a version tag, which is used by operations such as online index rebuilds or transactions with optimistic transaction isolation levels. We will discuss isolation levels in chapter 10.

The piece of metadata that we are really concerned about here, however, is the *column offset array*. This is used to track where each variable-length column begins within the row. Because variable-length data can be of any length, this offset table is the only way for SQL Server to separate where one piece of data ends and the next one begins.

Each variable-length column requires a 2-byte offset in this table, meaning that every variable-length column uses 2 bytes more space than it would if it was a fixed-length column. This applies even if the column stores a `NULL` value. Therefore, if we used a `CHAR(10)` for our zip code, it would take up 10 bytes of space, but if we used a `VARCHAR(10)`, it would take up 12 bytes of space, despite the actual data length being 10 bytes.

CHAR and VARCHAR vs. NCHAR and NVARCHAR

`NCHAR` and `NVARCHAR` are able to store full UNICODE data, whereas `CHAR` and `VARCHAR` are only able to store an 8-bit codepage. From SQL Server 2019, it has been possible to use UTF-8 enabled collations, and `CHAR/VARCHAR` data types can store the full range of UTF-8 characters. For full UTF-16 support, however, the `NCHAR` and `NVARCHAR` data types are still required.

`CHAR` and `VARCHAR` data types use 1 byte per character. `NCHAR` and `NVARCHAR` use 2 bytes per character. The extra space is required for the full 16-bit UTF-16 codepage. Therefore, not only do the UTF-16 data types use more space, but they limit the number of characters that can be stored on a page. SQL Server imposes a maximum length for fixed-length UTF-16 data of 4,000 characters, as opposed to the maximum length of fixed-length 8-bit codepage data, which is 8,000 characters.

It is possible to store variable-length data up to 2 GB by using the `VARCHAR(MAX)` and `NVARCHAR(MAX)` data types, but any fields that do not fit on the data page are stored in a different type of allocation unit and are referred to as row-overflow data.

The size limitation also applies across multiple columns. No matter how the columns are dispersed, the maximum length of data on a page is 8,060 bytes. Therefore, if we have a table that consists of a `CHAR(5000)` column and a `VARCHAR(5000)` column, and data is inserted into the variable-length column that has a length of 2,000 characters (4,000 bytes), then the variable-length data would dynamically be moved to a different page and stored in the row-overflow allocation unit.

Some people make an argument that because storage and memory are comparatively cheaper than they used to be, we should just use `NCHAR` and `NVARCHAR` data types to avoid any potential problems with codepage incompatibilities between columns. My view of this, which has been strengthened by the addition of UTF-8 collations, is the same as for any other data type. We should use the most restrictive data type that will not overflow.

In other words, if we know, because of the nature of our data, that we will never face any codepage incompatibility problems, then we should use the 8-bit codepage data types. This will ultimately give us the best performance and resource efficiency at the best pricepoint. If, however, this cannot be guaranteed—for example, if we need to pull in data from the internet or other untrusted sources or, indeed, if we know that we may need to mix data from multiple collations—then we should use the UTF-16 data types instead.

In the context of our `Employees` table, most of our columns absolutely need to be variable length. Our `DepartmentCode` column, however, will always contain a two-character value, such as `HR` for Human Resources, `SA` for Sales, and `MA` for Manufacturing. Therefore, we should change the definition of the `DepartmentCode` column to be a `NCHAR(2)`. This will save 2 bytes of information per row. We are using Unicode because, in section 3.4, we will be pulling in data from an external business partner.

The statement in the following listing makes the desired update to the `Employees` table by using the `ALTER TABLE..ALTER COLUMN` command.

Listing 3.5 Updating the `DepartmentCode` column to a fixed-length string

```sql
ALTER TABLE dbo.Employees
    ALTER COLUMN DepartmentCode NCHAR(2) ;
```

Using variable-length strings is the right thing to do if the values in our column are likely to be of differing lengths. This avoids the shorter values being padded out with white space and saves space. If we expect the values of a column to always be the same length, however, then we should use a fixed-length string. Failure to do so will add an additional 2 bytes per row, which will be used for a column offset array, defining the beginning of each variable-length value.

## 3.3 #8 Writing your own hierarchy code

If we look at our `Employees` table, we have probably realized that the `EmployeeID` and `ManagerID` columns are designed to model an employee hierarchy. Therefore, consider the org chart in figure 3.2, which models the organizational structure of the senior management team at MagicChoc.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437401/files/OEBPS/Images/CH03_F02_Carter.png)<br>
**Figure 3.2 MagicChoc management org chart**

To demonstrate how to model this hierarchy using a traditional approach, which a surprising number of developers still use, I invite you to run the script in listing 3.6, which will insert employee records into the `Employees` table. Notice how the `ManagerID` column contains the `EmployeeID` of the person they report to.

Listing 3.6 Inserting employee records into the `Employees` table

```sql
INSERT INTO dbo.Employees (
    EmployeeID
  , FirstName
  , LastName
  , DateOfBirth
  , EmployeeStartDate
  , ManagerID
  , Salary
  , Department
  , DepartmentCode
  , Role
  , WeeklyContractedHours
  , StaffOrContract
  , ContractEndDate
)
VALUES
    (1, 'Simon', 'Gomez', '19691001', '20180101', NULL, 980000, 'C-Suite',
'CS', 'CEO', 40, 1, NULL),
    (2, 'Sanjay', 'Gupta', '19761001', '20180101', 1, 640000, 'C-Suite',
'CS', 'COO', 40, 1, NULL),
    (3, 'Ed', 'Ling', '19690403', '20200801', 1, 320000, 'C-Suite', 'CS',
'CPO', 40, 1, NULL),
    (4, 'Amanda', 'Ballard', '19830401', '20200301', 1, 350000, 'C-Suite',
'CS', 'Sales Director', 40, 1, NULL),

    (5, 'Bob', 'Walford', '19780908', '20191201', 2, 96000, 'Technology',
'TE', 'Head Of Technology', 40, 1, NULL),
    (6, 'Brian', 'Tilly', '19710102', '20181001', 2, 89000,
'Manufacturing', 'MA', 'Head Of Manufacturing', 40, 1, NULL),
    (7, 'Sally', 'Nugent', '19790302', '20220601', 2, 80000, 'Procurement',
'PR', 'Head Of Procurement', 40, 1, NULL),
    (8, 'Jamie', 'Briggs', '19900102', '20190601', 3, 65000, 'Human
Resources', 'HR', 'HR Manager', 40, 1, NULL),
    (9, 'Lance', 'Bernard', '19910707', '20210601', 4, 98000, 'Sales &
Marketing', 'SA', 'Head Of Sales', 40, 1, NULL),
    (10, 'Jo', 'Carver', '19900810', '20191201', 4, 70000, 'Sales &
Marketing', 'SA', 'Head Of Marketing', 40, 1, NULL),

    (11, 'John', 'O''Shea', '19700609', '20180601', 5, 70000, 'Technology',
'TE', 'Development Manager', 40, 1, NULL),
    (12, 'Eric', 'Bristow', '20000109', '20221001', 5, 72000, 'Technology',
'TE', 'Infrastructure Manager', 40, 1, NULL),
    (13, 'Ronald', 'Sanders', '19601209', '20190101', 6, 45000, 'Manufacturing', 'MA', 'Shift Manager', 45, 1, NULL),
    (14, 'Amy', 'Fry', '19921101', '20190101', 6, 45000, 'Manufacturing',
'MA', 'Shift Manager', 45, 1, NULL),
    (15, 'Greg', 'Andrews', '19871212', '20190101', 6, 45000,
'Manufacturing', 'MA', 'Shift Manager', 45, 1, NULL),
    (16, 'Dave', 'Turney', '19760609', '20190101', 6, 52000,
'Manufacturing', 'MA', 'Warehouse Manager', 48, 1, NULL),
    (17, 'Mark', 'Sokolowski', '19960209', '20190901', 16, 42000,
'Manufacturing', 'MA', 'Goods Inn Manager', 40, 1, NULL),

    (18, 'Robin', 'Round', '19940409', '20190601', 3, 60000, 'Human
Resources', 'HR', 'Recruitment Manager', 40, 1, NULL),
    (19, 'Lucy', 'Sykes', '19890201', '20200201', 9, 65000, 'Sales', 'SA',
'US Sales Manager', 40, 1, NULL),
    (20, 'Bruce', 'Bryant', '19860304', '20200301', 9, 70000, 'Sales',
'SA', 'International Sales Manager', 40, 1, NULL),
    (21, 'Ashwin', 'Kumar', '20010212', '20210601', 20, 55000, 'Sales',
'SA', 'Euro Sales Manager', 40, 1, NULL),
    (22, 'Emma', 'Roberts', '20000208', '20210601', 20, 55000, 'Sales',
'SA', 'APAC Sales Manager', 40, 1, NULL) ;
```

Now imagine that we have been asked to write a report that lists all of Sanjay Gupta’s direct reports. This can be achieved using the following simple query:

```sql
SELECT
    FirstName
  , LastName
FROM dbo.Employees
WHERE ManagerID = 2 ;
```

But what if we are asked to write a query that returns all of Sanjay Gupta’s direct and indirect reports? This suddenly becomes more complex. There are several ways of creating this report, including dreaded cursors (which we will discuss in chapter 5), but the best practice method would be for developers to use a recursive *common table expression (CTE)*. A CTE is a temporary result set, which can be referenced multiple times within a query. It can also reference itself, which allows for *recursion*.

NOTE Recursion means that an error in the query could cause an infinite loop. To avoid this, there is an instance-wide setting that controls the maximum level of recursion. By default, this is set to 100. If we want to override this for a specific query, we can use the `MAXRECURSION` query hint.

Recursion is the key to implementing hierarchies in this scenario. For example, the query in listing 3.7 uses a CTE to return all employees who report both directly and indirectly to Sanjay Gupta, who has an `EmployeeID` of `2`. Within the definition of the CTE, the first query returns the employee record for Sanjay himself. A `UNION ALL` clause then appends the results of the second query. This second query is recursive, because it joins the results from the `Employees` table to the results of the first query within the CTE. Because the join is on `EmployeeID` in the first results set to the `ManagerID` in the second result set, the second result set contains the details of subordinate employees. The final `SELECT` statement sits outside of the CTE. It returns all records from the CTE but then joins back to the underlying `Employees` table to populate the managers’ first and last names.

TIP You will notice that the semicolon that terminates the `SET` statement is at the start of the line that begins the `WITH` CTE clause. This is a stylistic choice. A CTE must always be the start of a statement. Therefore, it is a common practice to always start the statement with a semicolon. That will stop a compilation error even if we have forgotten to terminate the previous statement. Alternatively, you can enforce a standard whereby all statements are always terminated with a semicolon.

Listing 3.7 Using a CTE to return all direct and indirect employees

```sql
DECLARE @ManagerID INT ;

SET @ManagerID = 2

;WITH EmployeeCTE AS (
    SELECT
          EmployeeId
        , FirstName
        , LastName
        , ManagerId
    FROM dbo.Employees
    WHERE EmployeeId = @ManagerID
    UNION ALL
    SELECT
          Emp.EmployeeId
        , Emp.FirstName
        , Emp.LastName
        , Emp.ManagerId
    FROM dbo.Employees AS Emp
    INNER JOIN EmployeeCTE AS CTE
        ON CTE.EmployeeId=Emp.ManagerId
)
SELECT
      Emp.EmployeeID
    , Emp.FirstName
    , Emp.LastName
    , Emp.ManagerID
    , Mgr.FirstName AS ManagerFirstName
    , Mgr.LastName AS ManagerLastName
FROM EmployeeCTE Emp
INNER JOIN dbo.Employees Mgr
    ON Emp.ManagerID = Mgr.EmployeeID ;
```

Another typical query that we may be asked to write involves working out who manages an employee’s manager. This is common when constructing escalation paths. The query in listing 3.8 demonstrates how we could establish who manages Emma Robert’s manager. In this query, we add a constant of `0` in the first query within the CTE, with a column name `Level` to the result set, and specify that Emma Roberts is at level 0. The recursive query then increments the level number for each layer of the hierarchy. The final query, outside of the CTE, then filters by level 2 to return the entity two levels above Emma.

Listing 3.8 Using a CTE to return the entity two levels above in a hierarchy

```sql
DECLARE @EmployeeID INT ;

SET @EmployeeID = 22

; WITH EmployeeCTE AS
(
   SELECT
       employeeid
     , firstname
     , lastname
     , managerid
     , Role
     , 0 as Level
   FROM dbo.Employees
   WHERE EmployeeID = @EmployeeID
   UNION ALL
   SELECT
       emp.EmployeeID
     , emp.FirstName
     , emp.LastName
     , emp.ManagerID
     , emp.Role
     , Level + 1
   FROM dbo.employees emp
   INNER JOIN EmployeeCTE cte
      ON emp.EmployeeID = cte.ManagerID
)

SELECT
    firstname
  , lastname
  , Role
FROM EmployeeCTE
WHERE Level = 2 ;
```

The challenge with this traditional approach to implementing a hierarchy is that it requires the developer to write a lot of code. The examples in this section are simple and are for illustrative purposes, but in real-world scenarios, recursive CTEs can become complex and hard to manage. Every time a slightly different request comes from the business, developers need to write the code that will traverse the hierarchy in the appropriate way. For example, we may be asked to return a list of all managers at the third level of the org structure.

What many developers do not realize is that Microsoft has already done most of the hard work for us by implementing the `HIERARCHYID` data type. This is an advanced data type written in .NET that allows us to call methods against it, which will traverse the hierarchy for us without the need to write complex CTEs.

To see how this works, let’s add a new column to the `Employees` table, called `ManagerHierarchyID`, which will be of type `HIERARCHYID`. We can achieve this with the script in the following listing.

Listing 3.9 Adding the `ManagerHierarchyID` column

```sql
ALTER TABLE dbo.Employees ADD
    ManagerHierarchyID HIERARCHYID NULL ;
```

To take advantage of SQL Server’s hierarchy features, we will first need to model the hierarchy. To explain how this modeling works, consider the following section of our org chart. Simon Gomez is at the top of the hierarchy, which we will call the root.

The hierarchy is modeled using a slashed format. Therefore, the root of the hierarchy is represented as `/`. Sanjay Gupta and Ed Ling are at the second level of the hierarchy, and each needs to be uniquely identified. Therefore, Sanjay will be represented as `/1/` and Ed will be represented as `/2/`.

Jamie Briggs and Robin Round both report to Ed Ling. Therefore, they are both at the third level of the hierarchy, but they also need to be uniquely identified. Therefore, they would be represented as `/1/2/1/` and `/1/2/2/`, respectively. The levels continue to build from there. They are then stored in the table as bit strings and displayed, unformatted, as hexadecimal values. For example, the hierarchy ID for Simon Gomez at the root level is stored as `0x`, whereas Sanjay Gupta at the second level is stored as `0x58` and Ed Ling, also at the second level, is stored as `0x68`. Jamie Briggs and Robin Round are stored as `0x6AC0` and `0x6B40`, respectively.

But fear not: there is no need to model the hierarchy manually. Instead, we will take a two-step approach, which takes the hard work out of the modeling process. The first step is to create a temporary table that consists of three columns: `EmployeeID`, `ManagerID`, and a row number, which we will generate by using the `ROW_NUMBER()` function and partitioning the numbers by `ManagerID`. This function will give us the incremental numbers that we need at each level of a hierarchy branch.

The second step is to define a CTE, built on our temporary table, which will define the root of the hierarchy by using the `GetRoot()` global method against the `HIERARCHYID` type for Simon Gomez, who sits at the top of the hierarchy and therefore has a `NULL ManagerID`.

The recursive query will build the subsequent levels of the hierarchy by concatenating the next subsequent hierarchy level with the incremental value that we generated using `ROW_NUMBER()`. We can then use an `UPDATE` statement to pull the modeled hierarchy IDs from the CTE and update the `Employees` table, based on joining the `EmployeeID`. This is demonstrated in the following listing.

Listing 3.10 Generating `HierarchyIDs` for the `Employees` table

```sql
SELECT
    EmployeeID
  , ManagerID
  , ROW_NUMBER()
        OVER (PARTITION BY ManagerID ORDER BY ManagerID) AS Incremental
INTO #Hierachy
FROM dbo.Employees

;WITH HierarchyPathCTE AS (
    SELECT
         hierarchyid::GetRoot() AS ManagerHierarchyID
       , EmployeeID
    FROM #Hierachy AS C
    WHERE ManagerID IS NULL
    UNION ALL
    SELECT
        CAST(hpc.ManagerHierarchyID.ToString() +
            CAST(h.Incremental AS VARCHAR(30)) +
            '/' AS HIERARCHYID)
       ,h.EmployeeID
FROM #Hierachy AS h
JOIN HierarchyPathCTE AS hpc
   ON h.ManagerID = hpc.EmployeeID
)

UPDATE e
    SET ManagerHierarchyID = hp.ManagerHierarchyID
FROM dbo.Employees e
INNER JOIN HierarchyPathCTE hp
    ON e.EmployeeID = hp.EmployeeID ;
```

Case sensitivity

An important note when working with the `HIERARCHYID` data type is that methods are case sensitive. For example, the following query will return an error:

```sql
SELECT ManagerHierarchyID.tostring() FROM dbo.Employees ;
```

whereas the following query would execute successfully:

```sql
SELECT ManagerHierarchyID.ToString() FROM dbo.Employees ;
```

From this point on, working with hierarchies becomes incredibly simple. Think back to the query we wrote to generate a list of all people who report directly and indirectly to Sanjay Gupta. We can replace this whole recursive query with the simple query in the following listing, which uses the `IsDescendantOf()` method to filter the query by identifying anyone who sits within Sanja Gupta’s branch of the hierarchy.

Listing 3.11 Returning all of Sanjay Gupta’s direct and indirect reports

```sql
DECLARE @Manager HIERARCHYID ;

SELECT @Manager = ManagerHierarchyID
FROM dbo.Employees
WHERE EmployeeID = 2 ;

SELECT
      EmployeeID
    , FirstName
    , LastName
    , ManagerHierarchyID.ToString()
FROM dbo.Employees
WHERE ManagerHierarchyID.IsDescendantOf(@Manager) = 1 ;
```

In a similar vein, think back to the recursive query that we wrote to discover who manages Emma Robert’s manager. This can be replaced with the simple script in the following listing, which uses the `GetAncestor()` method to traverse the hierarchy.

Listing 3.12 Discovering who manages Emma Robert’s manager

```sql
DECLARE @EmployeeID INT ;

SET @EmployeeID = 22 ;

SELECT
    FirstName
    , LastName
    , Role
    , ManagerhierarchyID.ToString() AS ManagerHierarchyID
FROM dbo.Employees
WHERE ManagerHierarchyID = (
    SELECT ManagerHierarchyID.GetAncestor(2)
    FROM dbo.Employees
    WHERE EmployeeID = @EmployeeID
) ;
```

You may remember that I also mentioned that we could be asked to traverse the hierarchy in a number of ways and suggested that we may encounter the need to find hierarchy siblings. To explore how simple this is, imagine that MagicChoc is performing a salary benchmarking exercise. Specifically, Amy Fry has requested a pay increase, and the HR team want to know if her salary is on a par with other people at her level of the organization.

In this scenario, we can use the `GetLevel()` method to determine the level of the hierarchy that she is on and then return details of all other employees at the same level. This technique is demonstrated in the following listing.

Listing 3.13 Returning all employees who are organizational siblings of Amy Fry

```sql
DECLARE @EmployeeID INT ;

SET @EmployeeID = 14 ;

SELECT
    FirstName
  , LastName
  , Salary
FROM dbo.Employees
WHERE ManagerHierarchyID.GetLevel() = (
    SELECT
        ManagerHierarchyID.GetLevel()
    FROM dbo.Employees
    WHERE EmployeeID = @EmployeeID
) ;
```

## 3.4 #9 Not storing XML data as native XML

When XML data is passed to a database, developers often feel compelled to store that data in relational data structures. This is sometimes out of a sense of best practice—the suggestion that this is a database, so surely it’s better to store the data in a relational format? Other times the choice is made through a fear of XML and the potential need to write complex XQuery statements to get at the data. Frankly, there are many occasions where storing data in a relational format is a good idea, but this is far from true in some scenarios.

Let’s think back to our `Employees` table. So far, we have only added employee records for the management team, who are all permanent staff. The warehouse, however, often employs temporary staff through a job agency called Total Warehouse Jobs. MagicChoc does a lot of business with this agency, and the companies have therefore decided to integrate their systems.

When a temp leaves MagicChoc, their employee record is deleted from the system. However, Total Warehouse Jobs keeps a record of their past contracts. If a person signs a new contract with MagicChoc, then there is a requirement for Total Warehouse Jobs to provide a list of their previous contracts, with the roles they performed. This helps MagicChoc assign them appropriate work, in which they have experience.

The nature of XML means that it is extensible and human readable, supports schema validation, and can be universally processed. This makes it a popular choice for system integration. In our scenario, it has been agreed that, once a contract is signed, Total Warehouse Jobs will send details of the employee’s previous contracts in an XML document. This information needs to be stored in the `HumanResources` database.

The warehouse application uses this data on a daily basis to calculate which people will be assigned which tasks, based on the daily manufacturing priorities. An ETL process needs to send the data to the warehousing application in XML format.

### 3.4.1 Shredding XML

The way in which I have seen some developers approach this scenario is to shred the XML data that is received into a relational table and then reconstruct an XML document to send it on to the recipient.

> [!TIP]
>
> Shredding XML is the process of removing (or shredding) the markup from the data and organizing the data into relational values.

If we were to take this approach in our MagicChoc example, the first thing we would need to do is create a schema that will hold the historic contract data. The nature of the data means that we will need three tables to avoid duplicating data. This is a key concept of normalization, which we will discuss in chapter 4. An example of what this table might look like can be found in the following listing.

Listing 3.14 Creating table structures for historic contract data

```sql
CREATE TABLE dbo.ContractHistory (
    ContractHistoryID    SMALLINT    NOT NULL    PRIMARY KEY    IDENTITY,
    EmployeeID           SMALLINT    NOT NULL
        REFERENCES dbo.Employees(EmployeeID),
    ContractStartDate    DATE        NOT NULL,
    ContractEndDate      DATE        NOT NULL
) ;

CREATE TABLE dbo.Skills (
    SkillsID    SMALLINT    NOT NULL    PRIMARY KEY    IDENTITY,
    Skill       VARCHAR(30) NOT NULL
) ;

CREATE TABLE dbo.ContractSkills (
    ContractSkillsID    INT         NOT NULL    PRIMARY KEY    IDENTITY,
    ContractHistoryID   SMALLINT    NOT NULL
        REFERENCES dbo.ContractHistory(ContractHistoryID),
    SkillID             SMALLINT    NOT NULL
        REFERENCES dbo.Skills(SkillsID)
) ;
```

Because the `Skills` table we just created is a reference table, before we move on, let’s add some sample data using the script in the following listing.

Listing 3.15 Adding data to the `Skills` table

```sql
INSERT INTO dbo.Skills (Skill)
VALUES
    ('Picker'),
    ('Packer'),
    ('Stock Take'),
    ('Forklift Driver'),
    ('Machine 1 operator'),
    ('Machine 2 operator'),
    ('Machine 3 operator'),
    ('Machine 4 operator') ;
```

The historic contract data consists of an element-centric XML document, with a root element called `<EmployeeContracts>`.

Element-centric vs. attribute-centric mappings

In an element-centric XML document, an element contains child elements, which hold the element’s properties. This is opposed to an attribute-centric XML document, where an element’s properties are stored in attributes of the element. For example, in the fragment

```sql
<Employee ID="23"></Employee>
```

`ID` is an attribute of the `<Employee>` element and has a value of `23`. For simple documents, elements versus attributes is largely a stylistic choice. If you have complex nodes, however, that will repeat or must be in a certain sequence, then elements must be used, as this cannot be achieved with attributes.

Under the root element, there is a complex element called `<Employee>`, which contains the child elements that will be stored in our `Employees` table. It also contains a nested complex element called `<Contracts>`, which contains a repeating element called `<Contract>`, containing the details of each contract that the employee has had. A further complex element is nested under `<Contract>`, called `<Skills>`, containing in turn a repeating element called `<Skill>`. An example of the data that we receive from Total Warehouse Jobs is as follows:

```sql
<EmployeeContracts>
    <Employee>
        <EmployeeID>23</EmployeeID>
        <FirstName>Robert</FirstName>
        <LastName>Blake</LastName>
        <DateOfBirth>19781212</DateOfBirth>
        <Contracts>
            <Contract>
                <StartDate>20200101</StartDate>
                <EndDate>20203006</EndDate>
                <Skills>
                    <Skill>Forklift driver</Skill>
                    <Skill>Picker</Skill>
                    <Skill>Packer</Skill>
                </Skills>
            </Contract>
            <Contract>
                <StartDate>20210101</StartDate>
                <EndDate>20211212</EndDate>
                <Skills>
                    <Skill>Picker</Skill>
                    <Skill>Stock Take</Skill>
                </Skills>
            </Contract>
        </Contracts>
    </Employee>
</EmployeeContracts>
```

So to use the approach of storing this dataset in a traditional relational structure, our first job will be to shred the data we receive. To do this, we could either use the `OPENXML()` function, which will return a rowset from an XML document, or use a combination of the `nodes()` and `value()` XQuery methods. In this example, we will use `OPENXML()` because it is the method I see people use most often.

The first complexity with the `OPENXML()` function is that it does not have a built-in XML parser. Instead, we must parse the XML document using an MSXML parser before passing it to the function. We can perform this document preparation by using the `sp_xml_preparedocument` stored procedure. This procedure will parse the document, and the output object will contain a handle to the in-memory tree representation of the nodes within the document, which can then be passed to the function.

> [!NOTE]
>
> When using `sp_xml_preparedocument`, it is important to use the `sp_xml_removedocument` procedure after the `OPENXML()` function. This will release the memory that is being consumed.

We also need to pass the `OPENXML()` function, an XPath pattern that identifies the rows to be processed. We will point this XPath expression to the lowest level of our hierarchy, and then, in our mappings, we will use the `../` operator to traverse the higher levels.

> [!TIP]
>
> XPath is defined by W3C and is a language used to navigate through nodes in an XML document. An XPath expression is used to select specific nodes in an XML document.

The final parameter is optional and indicates how to populate the spillover column. The possible values are detailed in table 3.2.

Table 3.2 Spillover column population options

| Value | Description |
| --- | --- |
| `0` | Attribute-centric mapping |
| `1` | Applies attribute-centric mapping followed by element-centric mapping |
| `2` | Applies element-centric mapping followed by attribute-centric mapping |
| `8` | Does not copy data to the overflow property |

A `WITH` clause is used with `OPENXML()` to specify the data type and mappings on nodes within the document. If the `WITH` clause is omitted, then SQL Server returns an *XML edge table*, which details the fine-grain document structure, including information such as namespace URI, namespace prefix, and pointers to next and previous sibling elements.

The script in listing 3.16 demonstrates what the process of shredding the data and inserting it into the tables might look like. The first thing we do is to declare variables to store the raw XML document and the handle to the in-memory parse tree of the prepared document. Next come two table variables. The first will store the results of the `OPENXML()` function, while the second simply holds some internal employee data that we are simulating pulling from the HR application and that is required to populate the `Employees` table. Next, we parse the XML data before calling the `OPENXML()` function and insert the results into the table variable. After that, we run queries to duplicate the data and insert it into the tables. These `INSERT` statements are wrapped in a transaction (which will be discussed in chapter 10), which means if one of the inserts fails, all of the inserts will be rolled back. This prevents us from ending up with inconsistent data between the tables that we would have to manually unpick in the event of a failure. Before the transaction begins, `XACT_ABORT` is turned on. This will stop the transaction from continuing, even if a statement has only failed for a minor error, such as a failed foreign key constraint.

> [!TIP]
>
> Yes! You are absolutely right! We should add error handling to this code, especially as the XML data is coming from an external source. We will discuss error handling in chapter 7.

Listing 3.16 Shredding the data with `OPENXML()`

```sql
DECLARE @RawContractDetails XML ;                                   ①
DECLARE @ParsedContractDetails INT ;                                ②
DECLARE @ShreddedData TABLE (
      EmployeeID           INT
    , FirstName            NVARCHAR(32)
    , LastName             NVARCHAR(32)
    , DateOfBirth          DATE
    , ContractStartDate    DATE
    , ContractEndDate      DATE
    , Skill                NVARCHAR(30)
) ;
DECLARE @InternalEmployeeData TABLE (
      EmployeeStartDate        DATE
    , ManagerID                SMALLINT
    , Salary                   MONEY
    , Department               NVARCHAR(64)
    , DepartmentCode           NCHAR(2)
    , Role                     NVARCHAR(64)
    , WeeklyContractedHours    INT
    , StaffOrContract          BIT
    , ContractEndDate          DATE
    , ManagerHierarchyID       HIERARCHYID
) ;

INSERT INTO @InternalEmployeeData
VALUES (
    '20230101',
    14,
    39000,
    'Manufacturing',
    'MA',
    'Warehouse Operative',
    40,
    0,
    '20231231',
    '/1/2/2/1/'
) ;

SET @RawContractDetails = N'<EmployeeContracts>
    <Employee>
        <EmployeeID>23</EmployeeID>
        <FirstName>Robert</FirstName>
        <LastName>Blake</LastName>
        <DateOfBirth>19781212</DateOfBirth>
        <Contracts>
            <Contract>
                <StartDate>20200101</StartDate>
                <EndDate>20200603</EndDate>
                <Skills>
                    <Skill>Forklift driver</Skill>
                    <Skill>Picker</Skill>
                    <Skill>Packer</Skill>
                </Skills>
            </Contract>
            <Contract>
                <StartDate>20210101</StartDate>
                <EndDate>20211231</EndDate>
                <Skills>
                    <Skill>Picker</Skill>
                    <Skill>Stock Take</Skill>
                </Skills>
            </Contract>
        </Contracts>
    </Employee>
</EmployeeContracts>' ;

EXEC sp_xml_preparedocument @ParsedContractDetails OUTPUT,          ③
@RawContractDetails ;                                               ③

INSERT INTO @ShreddedData                                           ④
SELECT *                                                            ④
FROM OPENXML(@ParsedContractDetails,                                ④
'/EmployeeContracts/Employee/Contracts/Contract/Skills/Skill', 2)   ④
WITH (                                                              ④
    EmployeeID           SMALLINT        '../../../../EmployeeID',  ④
    FirstName            NVARCHAR(32)    '../../../../FirstName',   ④
    LastName             NVARCHAR(32)    '../../../../LastName',    ④
    DateOfBirth          DATE            '../../../../DateOfBirth', ④
    ContractStartDate    DATE            '../../StartDate',         ④
    ContractEndDate      DATE            '../../EndDate',           ④
    Skill                NVARCHAR(30)    'text()'                   ④
) ;                                                                 ④

SET XACT_ABORT ON ;                                                 ⑤

BEGIN TRANSACTION                                                   ⑥
    INSERT INTO dbo.Employees
    SELECT
          s.EmployeeID
        , s.FirstName
        , s.LastName
        , s.DateOfBirth
        , i.EmployeeStartDate
        , i.ManagerID
        , i.Salary
        , i.Department
        , i.DepartmentCode
        , i.Role
        , i.WeeklyContractedHours
        , i.StaffOrContract
        , i.ContractEndDate
        , i.ManagerHierarchyID
    FROM @ShreddedData s
    INNER JOIN @InternalEmployeeData i
        ON 1=1
    GROUP BY
          s.EmployeeID
        , s.FirstName
        , s.LastName
        , s.DateOfBirth
        , i.EmployeeStartDate
        , i.ManagerID
        , i.Salary
        , i.Department
        , i.DepartmentCode
        , i.Role
        , i.WeeklyContractedHours
        , i.StaffOrContract
        , i.ContractEndDate
        , i.ManagerHierarchyID ;

    INSERT INTO dbo.ContractHistory(
        EmployeeID,
        ContractStartDate,
        ContractEndDate
    )
    SELECT
          EmployeeID
        , ContractStartDate
        , ContractEndDate
    FROM @ShreddedData
    GROUP BY
          EmployeeID
        , ContractStartDate
        , ContractEndDate ;

    INSERT INTO dbo.ContractSkills(ContractHistoryID, SkillID)
    SELECT
          ch.ContractHistoryID
        , s.SkillsID
    FROM @ShreddedData sd
    INNER JOIN dbo.Skills s
        ON TRIM(s.Skill) = TRIM(sd.Skill)
    INNER JOIN dbo.ContractHistory ch
        ON sd.EmployeeID = ch.EmployeeID
            AND sd.ContractStartDate = ch.ContractStartDate
            AND sd.ContractEndDate = ch.ContractEndDate ;

COMMIT

EXEC sp_xml_removedocument @ParsedContractDetails ;                 ⑦
```

① Declares a variable to store the raw XML

② Declares a variable to store the pointer to the in-memory parse tree

③ Parses the XML document

④ Shreds the XML data and inserts the relational values into a table variable

⑤ Turns on XACT_ABORT, so that if any statements in the transaction fail, the whole transaction will roll back.

⑥ Begins a transaction to perform the table updates, so that if one statement fails, the transaction will roll back

⑦ Removes the parse tree from memory

Okay, so that was hard work, right? It was hard work for SQL Server too! In my lab environment, which is a t2.large EC2 instance running nothing but this script, it took 31ms to execute the script. That’s 31ms to process five rows of data into three tables. So far, all we have done is shred the data. In the next section, we will look at the process required to reconstruct the XML document so that it can be sent to the client. Perhaps you are starting to see why I am not recommending this approach for our particular use case.

### 3.4.2 Reconstructing XML

Now that we have shredded the XML data into tables, we need to write the process that the ETL will use to send the data to the warehouse application. This means reconstructing the XML document from the data stored in the tables. We can achieve this by using a `SELECT` statement that specifies a `FOR XML` clause. `FOR XML` has four possible modes, and these are detailed in table 3.3.

Table 3.3 `FOR XML` modes

| Mode | Description |
| --- | --- |
| `RAW` | The most basic mode for constructing XML. It generates a flat XML document with one element per row. |
| `AUTO` | Generates XML documents with nested elements. The nesting is controlled by the join conditions within the query. The automatic formatting means that we have minimal control over the formatting of the XML. |
| `PATH` | Allows advanced control over the format of the XML document by mapping columns in the query to XML nodes in a given location of the hierarchy. |
| `EXPLICIT` | Offers similar control over formatting as `PATH` mode but is overly complex. There is generally no need to use this mode. |

To create the correct shape for our XML document, we will need to use `FOR XML PATH`, with subqueries, which also use the `FOR XML PATH` clause. The subqueries are required so that we can deal with the repeating elements at the child level of the document.

The query in listing 3.17 demonstrates how we can achieve this by using two layers of subqueries. The innermost query returns the skills associated with a given contract. These results are converted to XML using `FOR XML PATH`. This clause uses the `TYPE` keyword to define that the results will be well-formed XML and uses `ROOT` to specify the name of the root node within the document. The outer subquery uses the same process to pull the details from the repeating `Contracts` element. Finally, the outer query returns the employee details, which will sit at the top of the hierarchy.

Listing 3.17 Reconstructing the XML document

```sql
SELECT
      e.EmployeeID 'EmployeeID'
    , e.FirstName 'FirstName'
    , e.LastName 'LastName'
    , e.DateOfBirth 'DateOfBirth'
    , (
        SELECT
             ch.ContractStartDate 'StartDate'
           , ch.ContractEndDate 'EndDate'
           , (
                SELECT
                    s.Skill 'Skill'
            FROM dbo.Skills s
            INNER JOIN dbo.ContractSkills cs
                ON s.SkillsID = cs.SkillID
                   WHERE cs.ContractHistoryID = ch.ContractHistoryID
                   FOR XML PATH(''), TYPE, ROOT('Skills')
        )
        FROM dbo.ContractHistory ch
        WHERE EmployeeID = e.EmployeeID
        FOR XML PATH('Contract'), TYPE, ROOT('Contracts')
    )
FROM dbo.Employees e
WHERE EmployeeID = 23
FOR XML PATH('Employee'), ROOT('EmployeeContracts') ;
```

In my lab environment, this query took 52 ms to complete. Now think about a production environment where these processes are being run constantly by multiple people. Think too about where such processes need to run against large, complex XML documents. You can see why it can start to become quite costly.

### 3.4.3 Avoiding the overhead by storing data as XML

Converting the XML into relational data and back again took a fair amount of fiddling and also took a combined total of 83 ms. We could have saved development time and compile/execution time if we had stored the data in XML format. So let’s explore the development and processing impact if we were to store the data in its native format.

Because all of the details of skills and contracts for an employee are stored in a single XML document, there is no need to have the `EmployeeContracts` and `EmployeeSkills` tables. Instead, we can simply insert the XML document into the `Employees` table. Before we get started, let’s update the `Employees` table and add a column called `PreviousContracts`, which will store the data. The following listing contains a command that we can run to achieve this.

Listing 3.18 Adding a `PreviousContracts` column to the `Employees` table

```sql
ALTER TABLE dbo.Employees
    ADD PreviousContracts XML NULL ;
```

In our specific scenario, we will not be able to avoid using a small amount of XQuery. This is because the data will be keyed on the `EmployeeID`, and this is supplied by Total Warehouse Jobs inside the XML document. Therefore, we will need to extract this value along with the employee’s name and date of birth.

A script that we can use to add the new user to the `Employees` table is demonstrated in listing 3.19. As in our previous example, we are simulating our HR system to provide some of the values that will populate our `Employees` table with a table variable called `@InternalEmployeeData`. We then use the XQuery `value()` method to extract the `EmployeeID`, `FirstName`, `LastName`, and `DateOfBirth` nodes from the XML document. The `value()` method is passed an XPath to the node we want to extract, followed by the SQL Server data type that the node will be mapped to. `value()` requires a singleton value; therefore the desired node index is mandatory, even if the node does not repeat. > [!NOTE]
>
> that the `EmployeeID` in the XML document has been incremented to avoid the insert failing because of a violation of the primary key constraint.

> [!TIP]
>
> We are extracting the value of an element, but if we were extracting an attribute, then we would need to prefix the attribute name with an `@` symbol.

Listing 3.19 Adding an employee to the `Employees` table

```sql
DECLARE @RawContractDetails XML ;
DECLARE @InternalEmployeeData TABLE (
      EmployeeStartDate        DATE
    , ManagerID                SMALLINT
    , Salary                   MONEY
    , Department               NVARCHAR(64)
    , DepartmentCode           NCHAR(2)
    , Role                     NVARCHAR(64)
    , WeeklyContractedHours    INT
    , StaffOrContract          BIT
    , ContractEndDate          DATE
    , ManagerHierarchyID       HIERARCHYID
) ;

INSERT INTO @InternalEmployeeData
VALUES (
    '20230101',
    14,
    39000,
    'Manufacturing',
    'MA',
    'Warehouse Operative',
    40,
    0,
    '20231231',
    '/1/2/2/1/'
) ;

SET @RawContractDetails = N'<EmployeeContracts>                    ①
    <Employee>
        <EmployeeID>25</EmployeeID>
        <FirstName>Robert</FirstName>
        <LastName>Blake</LastName>
        <DateOfBirth>19781212</DateOfBirth>
        <Contracts>
            <Contract>
                <StartDate>20200101</StartDate>
                <EndDate>20200603</EndDate>
                <Skills>
                    <Skill>Forklift driver</Skill>
                    <Skill>Picker</Skill>
                    <Skill>Packer</Skill>
                </Skills>
            </Contract>
            <Contract>
                <StartDate>20210101</StartDate>
                <EndDate>20211231</EndDate>
                <Skills>
                    <Skill>Picker</Skill>
                    <Skill>Stock Take</Skill>
                </Skills>
            </Contract>
        </Contracts>
    </Employee>
</EmployeeContracts>' ;

INSERT INTO dbo.Employees
SELECT                                                             ②
     @RawContractDetails.value('(/EmployeeContracts/Employee/EmployeeID)[1]', 'SMALLINT') AS EmployeeID
   ,@RawContractDetails.value('(/EmployeeContracts/Employee/FirstName)[1]', 'NVARCHAR(32)') AS FirstName
    ,@RawContractDetails.value('(/EmployeeContracts/Employee/LastName)[1]', 'NVARCHAR(32)') AS LastName
   ,@RawContractDetails.value('(/EmployeeContracts/Employee/DateOfBirth)[1]', 'DATE') AS DateOfBirth
    , EmployeeStartDate
    , ManagerID
    , Salary
    , Department
    , DepartmentCode
    , Role
    , WeeklyContractedHours
    , StaffOrContract
    , ContractEndDate
    , ManagerHierarchyID
    , @RawContractDetails AS PreviousContracts
FROM @InternalEmployeeData ;
```

① Defines the XML document

② The SELECT list consists of calls to the value() method to extract nodes from the XML document.

This script took 33 ms to execute in my lab environment. We can now retrieve the XML document for the warehouse application with the simple `SELECT` statement in the following listing.

Listing 3.20 Retrieving the XML document for the warehouse application

```sql
SELECT
    PreviousContracts
FROM dbo.Employees
WHERE EmployeeID = 25 ;
```

This `SELECT` statement took less than 1 ms to execute in my lab environment. If we round it up to 1 ms, that means the insertion and the return of the XML data took a combined total of 34 ms. This means that, overall, the end-to-end processing was more than twice as fast when we avoided shredding.

Does this mean that we should never shred XML data? No, absolutely not! There are many good reasons for shredding the data. The general rule that I use is that T-SQL is much more efficient than XQuery. Therefore, if our scenario requires us to frequently query the data, then it is a good idea to shred it. Conversely, if we frequently write the data but rarely query it, then storing the data in native XML format is the best approach. The mistake that we should avoid is always shredding the data (or, equally, always storing the data as XML). Instead, make your design decision based on your application’s requirements.

## 3.5 #10 Ignoring JSON

Just as SQL developers have a tendency to avoid XML data, they also have a tendency to avoid JSON data. Just as we saw when talking about XML, this can lead to suboptimal table design and suboptimal performance. Let’s imagine that we have been asked to expand our data schema for the `employees` entity so that it allows us to store their home addresses.

A typical model for modeling addresses in SQL Server would be to have an `Addresses` table with an `AddressID` as a primary key, which then has a column of the same name in the `Employees` table and also potentially other tables where an address is relevant, such as `Offices` or `Sites` in the case of an HR database. The column in the `Employees` table would have a foreign key constraint, so that only applicable values from the `Addresses` table can be stored.

This model is okay, but it will become a wide, sparse table because not all addresses have the same number of lines, and we need to be able to cater to any eventuality. This is made worse in the case of MagicChoc, which is an international company. Think of zip codes. They are known as post codes in the UK, *yóu dì* *q**ū* *hào* in Thailand, and block numbers in Bahrain, to name but a few. Because they have different formats in different countries, we would either store these in a very permissive data type or we could use separate, sparsely populated columns for each. It is also worth noting that many countries, such as Bahamas, Dominica, Fiji, and many African nations, do not have any equivalent to a zip code, further increasing the sparse nature of the table.

Depending on our application requirements, what may be a better data design is to store the data in JSON format. Like XML, JSON has a semistructured format that allows for irrelevant data to be omitted. It is becoming increasingly popular as it is a very lightweight format with significantly less tagging than XML.

Because of the efficiency of T-SQL for querying data, when I’m modeling data, I work to the same rule with JSON as I do with XML. Namely, if the data will be written often and queried infrequently, then JSON is a good choice. If the data is queried far more than it is written, however, then I prefer to store the data relationally.

Other use cases for JSON include modeling data domains that would otherwise need to be split across relational data and NoSQL. This is because, with SQL Server, both types of data can be stored in the same schema, supporting communication with REST APIs or other system integrations that send and receive data in JSON format. It is now also feasible to store and analyze log data in SQL Server, where the logs are JSON based.

So let’s see how we could avoid the mistake of ignoring JSON by modeling our employee address data in JSON format. If our addresses could relate to multiple entities, such as offices or sites, then we may still want to build an `Addresses` table so that the JSON-formatted addresses can be used by multiple entities. We are, however, convinced that `Employees` is the only entity that will need to interact with address data, so we will add an additional column to our `Employees` table to store the address data. The following listing shows a command that will create that column.

Listing 3.21 Adding an `Address` column to the `Employees` table

```sql
ALTER TABLE dbo.Employees
    ADD Address NVARCHAR(MAX) ;
```

> [!NOTE]
>
> But wait a minute! We are going to model data in JSON, so why have we just created a column of type `NVARCHAR(MAX)`? The answer to this is, at the time of writing, a JSON data type is in preview in Azure SQL Database, but for SQL Server in IaaS or on-prem, JSON doesn’t have its own data type. It is stored as `VARCHAR` or `NVARCHAR` and is then indexed as text. This makes JSON fully compatible with any SQL Server functionality that supports text.

First, let’s look at the format of the JSON document we will use to store addresses. The JSON format uses simple `name:value` pairs, encapsulated in `""`. Nesting is designated with `{}`. The use of `[]` designates an array. Our address document has a root of `EmployeeAddress` and contains nodes for `EmployeeID` and `Address`. The `Address` node has further nodes, nested underneath, which store each line of the address. Additional lines can be added or the existing nodes replaced, depending on the requirements of each address, meaning that we remove the problem of sparse data. An example document is as follows:

```sql
{
   "EmployeeAddress":[
      {
         "EmployeeID":1,
         "Address":{
            "Line1":"5331 Rexford Court",
            "City":"Montgomery",
            "State":"AL",
            "ZipCode":"36116"
         }
      }
   ]
}
```

We can generate this document using a `SELECT` statement with a `FOR JSON` clause. There are two `FOR JSON` processing options: `AUTO` and `PATH`. `AUTO` can be used to automatically format the document based on the order of columns and the table `JOIN` sequence order. `PATH` mode gives granular control over the formatting.

> [!TIP]
>
> `FOR JSON AUTO` can only be used against a table, so in theory it cannot be used in the way we plan, but it is easy to work around this limitation by adding a `FROM` clause to an arbitrary table. I tend to use `sys.tables` for this purpose, but any table will suffice. If we use this technique, we will need to add a `TOP 1` to our `SELECT` clause to avoid returning a node for each record in the table.

The query in listing 3.22 will generate our sample JSON document. > [!NOTE]
>
> that we are using `PATH` mode, so each column has an alias that specifies the name of the node and its position within the JSON hierarchy.

Listing 3.22 Generating a JSON document

```sql
SELECT
      1 AS 'EmployeeID'
    , '5331 Rexford Court' AS 'Address.Line1'
    , 'Montgomery'         AS 'Address.City'
    , 'AL'                 AS 'Address.State'
    , '36116'              AS 'Address.ZipCode'
FOR JSON PATH, ROOT ('EmployeeAddress') ;
```

The script in listing 3.23 demonstrates how we could update the `Employees` table to add an employee address if we were given the address data in JSON format. The `WHERE` clause of the `UPDATE` statement uses the `JSON_VALUE()` function to retrieve the `EmployeeID` from inside the document. The first parameter that this function accepts is the JSON document in which to search. The second parameter specifies the path to the node. A JSON path always begins with `$`., which represents the whole document. The root node, `EmployeeAddress`, is an array (denoted by the square brackets), meaning that we need to pass an array index to specify which record we wish to return a value for. If we do not pass this index, then we will return a `NULL` result, even if there is only a single record, as there is in our scenario.

Listing 3.23 Updating the `Employees` table with address data

```sql
DECLARE @EmployeeAddress NVARCHAR(MAX) ;

SET @EmployeeAddress = (
    SELECT
          1 AS 'EmployeeID'
        , '5331 Rexford Court' AS 'Address.Line1'
        , 'Montgomery'         AS 'Address.City'
        , 'AL'                 AS 'Address.State'
        , '36116'              AS 'Address.ZipCode'
    FOR JSON PATH, ROOT ('EmployeeAddress')
) ;

UPDATE dbo.Employees
SET Address = @EmployeeAddress
WHERE EmployeeID =
    JSON_VALUE(@EmployeeAddress, '$.EmployeeAddress[0].EmployeeID') ;
```

If we ever need to shred this JSON document into a relational format, then we can use the `OPENJSON()` function. The format and functionality of `OPENJSON()` will look familiar to those of you who have read about `OPENXML()` earlier in this chapter. The advantage of the `OPENJSON()` function, however, is that there is no need to prepare the document before use. This saves on resource utilization.

In listing 3.24, we use the `OPENJSON()` function to shred our employee address document. The first parameter passed to the function is the JSON document itself. The second parameter specifies the path to the highest level of the hierarchy that we wish to interrogate. Remember that `$` represents the whole document, and just like when we used `JSON_VALUE()`, the path we pass needs to take account of the array by specifying an array index. The `WITH` clause specifies the columns that we want to return from the document, along with the SQL data types that they will map to and the path to the relevant node, where the node is not at the same level as the path expression.

Listing 3.24 Shredding the JSON data into relational values

```sql
DECLARE @EmployeeAddress NVARCHAR(MAX) ;

SET @EmployeeAddress = (
    SELECT
          1 AS 'EmployeeID'
        , '5331 Rexford Court' AS 'Address.Line1'
        , 'Montgomery'         AS 'Address.City'
        , 'AL'                 AS 'Address.State'
        , '36116'              AS 'Address.ZipCode'
    FOR JSON PATH, ROOT ('EmployeeAddress')
) ;

SELECT *
FROM OPENJSON(@EmployeeAddress, '$.EmployeeAddress[0]')
WITH (
    EmployeeID SMALLINT,
    Line1 NVARCHAR(64) '$.Address.Line1',
    City NVARCHAR(64) '$.Address.City',
    [State] NCHAR(2) '$.Address.State',
    ZipCode NVARCHAR(10) '$.Address.ZipCode'
) ;
```

We can see that JSON can play an important role in SQL Server if used appropriately. We can use SQL Server’s JSON support to denormalize data and simplify complex data models, integrate with NoSQL, or avoid processing data that is sent to and from APIs.

## Summary

* Always use the most restrictive data type that will allow you to store all potentially required values. This is especially true for integer values that are used in key constraints or values that are indexed.
* Always use fixed-length strings to reduce storage overhead when the length of strings is consistent within a column. Always use variable-length strings when the length will not be consistent.
* Avoid writing your own code and creating self-referenced tables when you are modeling and creating hierarchies. Instead, use the `HIERARCHYID` data type, as the type includes multiple methods, which take the heavy lifting out of the development.
* When we are faced with XML data, we shouldn’t feel we need to always shred that data into a relational model. Always consider the application requirements and model the data schema accordingly. This may or may not include storing data as native XML.
* Don’t shy away from JSON data. There are use cases where using JSON data is a good choice to simplify our data models.
